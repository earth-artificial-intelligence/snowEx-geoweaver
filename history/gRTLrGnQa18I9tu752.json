[{
  "history_id" : "wb01m2s9w5c",
  "history_input" : "# Write first python in Geoweaver\nimport os\nimport geopandas as gpd\nfrom shapely.geometry import Polygon, mapping\nfrom shapely.geometry.polygon import orient\nimport pandas as pd \nimport requests\nimport json\nimport pprint\nimport getpass\nimport netrc\nfrom platform import system\nfrom getpass import getpass\nfrom urllib import request\nfrom http.cookiejar import CookieJar\nfrom os.path import join, expanduser\nimport requests\nfrom xml.etree import ElementTree as ET\nimport time\nimport zipfile\nimport io\nimport shutil\n\nhomedir = os.path.expanduser('~') \ndatadir = f\"{homedir}/Documents/data/\"\n\npolygon_filepath = str(f'{homedir}/Documents/data/nsidc-polygon.json') # Note: A shapefile or other vector-based spatial data format could be substituted here.\n\ngdf = gpd.read_file(polygon_filepath) #Return a GeoDataFrame object\n\n# Simplify polygon for complex shapes in order to pass a reasonable request length to CMR. The larger the tolerance value, the more simplified the polygon.\n# Orient counter-clockwise: CMR polygon points need to be provided in counter-clockwise order. The last point should match the first point to close the polygon.\npoly = orient(gdf.simplify(0.05, preserve_topology=False).loc[0],sign=1.0)\n\n#Format dictionary to polygon coordinate pairs for CMR polygon filtering\npolygon = ','.join([str(c) for xy in zip(*poly.exterior.coords.xy) for c in xy])\nprint('Polygon coordinates to be used in search:', polygon)\n\ntemporal = '2017-01-01T00:00:00Z,2017-12-31T23:59:59Z' # Set temporal range\n\nparam_dict = {\n    'short_name': 'SNEX17_GPR',\n    'version': '2',\n    'polygon': polygon,\n#     'bounding_box': bounding_box, #optional alternative to polygon search parameter; if using, remove or comment out polygon search parameter\n    'temporal':temporal,\n}\n\ndef search_granules(search_parameters, geojson=None, output_format=\"json\"):\n    \"\"\"\n    Performs a granule search with token authentication for restricted results\n    \n    :search_parameters: dictionary of CMR search parameters\n    :token: CMR token needed for restricted search\n    :geojson: filepath to GeoJSON file for spatial search\n    :output_format: select format for results https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html#supported-result-formats\n    \n    :returns: if hits is greater than 0, search results are returned in chosen output_format, otherwise returns None.\n    \"\"\"\n    search_url = \"https://cmr.earthdata.nasa.gov/search/granules\"\n\n    \n    if geojson:\n        files = {\"shapefile\": (geojson, open(geojson, \"r\"), \"application/geo+json\")}\n    else:\n        files = None\n    \n    \n    parameters = {\n        \"scroll\": \"true\",\n        \"page_size\": 100,\n    }\n    \n    try:\n        response = requests.post(f\"{search_url}.{output_format}\", params=parameters, data=search_parameters, files=files)\n        response.raise_for_status()\n    except HTTPError as http_err:\n        print(f\"HTTP Error: {http_error}\")\n    except Exception as err:\n        print(f\"Error: {err}\")\n    \n    hits = int(response.headers['CMR-Hits'])\n    if hits > 0:\n        print(f\"Found {hits} files\")\n        results = json.loads(response.content)\n        granules = []\n        granules.extend(results['feed']['entry'])\n        granule_sizes = [float(granule['granule_size']) for granule in granules]\n        print(f\"The total size of all files is {sum(granule_sizes):.2f} MB\")\n        return response.json()\n    else:\n        print(\"Found no hits\")\n        return\n\nprint(search_granules(param_dict))\n",
  "history_output" : "Polygon coordinates to be used in search: -108.2352445938561,38.98556907427165,-107.85284607930835,38.978765032966244,-107.85494925720668,39.10596902171742,-108.22772795408136,39.11294532581687,-108.2352445938561,38.98556907427165\nFound 3 files\nThe total size of all files is 209.20 MB\n{'feed': {'updated': '2022-01-19T08:08:09.321Z', 'id': 'https://cmr.earthdata.nasa.gov:443/search/granules.json?short_name=SNEX17_GPR&version=2&polygon=-108.2352445938561%2C38.98556907427165%2C-107.85284607930835%2C38.978765032966244%2C-107.85494925720668%2C39.10596902171742%2C-108.22772795408136%2C39.11294532581687%2C-108.2352445938561%2C38.98556907427165&temporal=2017-01-01T00%3A00%3A00Z%2C2017-12-31T23%3A59%3A59Z', 'title': 'ECHO granule metadata', 'entry': [{'producer_granule_id': 'SnowEx17_GPR_Version2_Week1.csv', 'time_start': '2017-02-08T00:00:00.000Z', 'updated': '2019-11-20T14:19:39.156Z', 'dataset_id': 'SnowEx17 Ground Penetrating Radar V002', 'data_center': 'NSIDC_ECS', 'title': 'SC:SNEX17_GPR.002:167128516', 'coordinate_system': 'GEODETIC', 'day_night_flag': 'UNSPECIFIED', 'time_end': '2017-02-10T23:59:59.000Z', 'id': 'G1657541380-NSIDC_ECS', 'original_format': 'ECHO10', 'granule_size': '57.3195', 'browse_flag': False, 'polygons': [['39.05189 -108.06789 39.04958 -108.07092 39.02644 -108.13422 39.04032 -108.18504 39.0357 -108.2211 39.01719 -108.21534 38.99637 -108.18261 39.00562 -108.11049 39.02413 -108.06225 39.03338 -108.06213 39.02876 -108.08619 39.04264 -108.05301 39.05189 -108.05289 39.0542 -108.06786 39.05189 -108.06789']], 'collection_concept_id': 'C1655875737-NSIDC_ECS', 'online_access_flag': True, 'links': [{'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'type': 'text/plain', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/DP1/SNOWEX/SNEX17_GPR.002/2017.02.08/SnowEx17_GPR_Version2_Week1.csv'}, {'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#', 'type': 'text/xml', 'title': '(METADATA)', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/DP1/SNOWEX/SNEX17_GPR.002/2017.02.08/SnowEx17_GPR_Version2_Week1.csv.xml'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/SNOWEX/SNEX17_GPR.002/'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://search.earthdata.nasa.gov/search/granules?p=C1655875737-NSIDC_ECS&q=SNEX17_GPR&m=29.76382409255042!-108.823974609375!4!1!0!0%2C2&tl=1558474061!4!!'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://nsidc.org/data/data-access-tool/SNEX17_GPR/versions/2/'}, {'inherited': True, 'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#', 'hreflang': 'en-US', 'href': 'https://doi.org/10.5067/G21LGCNLFSC5'}, {'inherited': True, 'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#', 'hreflang': 'en-US', 'href': 'https://doi.org/10.5067/G21LGCNLFSC5'}]}, {'producer_granule_id': 'SnowEx17_GPR_Version2_Week2.csv', 'time_start': '2017-02-14T00:00:00.000Z', 'updated': '2019-11-20T14:19:39.156Z', 'dataset_id': 'SnowEx17 Ground Penetrating Radar V002', 'data_center': 'NSIDC_ECS', 'title': 'SC:SNEX17_GPR.002:167128520', 'coordinate_system': 'GEODETIC', 'day_night_flag': 'UNSPECIFIED', 'time_end': '2017-02-17T23:59:59.000Z', 'id': 'G1657541238-NSIDC_ECS', 'original_format': 'ECHO10', 'granule_size': '85.516', 'browse_flag': False, 'polygons': [['39.10738 -107.88943 39.10738 -107.89539 39.0912 -107.95508 39.07271 -108.02372 39.0542 -108.09234 39.04264 -108.16078 39.0357 -108.2113 39.03338 -108.2113 39.0195 -108.20533 39.00099 -108.18454 39.00099 -108.12811 39.00099 -108.08653 39.02644 -108.02094 39.0357 -107.94938 39.02413 -107.93155 39.04726 -107.89867 39.08195 -107.85677 39.10507 -107.86257 39.10969 -107.88644 39.10738 -107.88943']], 'collection_concept_id': 'C1655875737-NSIDC_ECS', 'online_access_flag': True, 'links': [{'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'type': 'text/plain', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/DP1/SNOWEX/SNEX17_GPR.002/2017.02.14/SnowEx17_GPR_Version2_Week2.csv'}, {'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#', 'type': 'text/xml', 'title': '(METADATA)', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/DP1/SNOWEX/SNEX17_GPR.002/2017.02.14/SnowEx17_GPR_Version2_Week2.csv.xml'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/SNOWEX/SNEX17_GPR.002/'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://search.earthdata.nasa.gov/search/granules?p=C1655875737-NSIDC_ECS&q=SNEX17_GPR&m=29.76382409255042!-108.823974609375!4!1!0!0%2C2&tl=1558474061!4!!'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://nsidc.org/data/data-access-tool/SNEX17_GPR/versions/2/'}, {'inherited': True, 'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#', 'hreflang': 'en-US', 'href': 'https://doi.org/10.5067/G21LGCNLFSC5'}, {'inherited': True, 'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#', 'hreflang': 'en-US', 'href': 'https://doi.org/10.5067/G21LGCNLFSC5'}]}, {'producer_granule_id': 'SnowEx17_GPR_Version2_Week3.csv', 'time_start': '2017-02-21T00:00:00.000Z', 'updated': '2020-02-18T12:25:16.785Z', 'dataset_id': 'SnowEx17 Ground Penetrating Radar V002', 'data_center': 'NSIDC_ECS', 'title': 'SC:SNEX17_GPR.002:173252482', 'coordinate_system': 'GEODETIC', 'day_night_flag': 'UNSPECIFIED', 'time_end': '2017-02-25T23:59:59.000Z', 'id': 'G1694922459-NSIDC_ECS', 'original_format': 'ECHO10', 'granule_size': '66.3598', 'browse_flag': False, 'polygons': [['39.05189 -108.06789 39.04958 -108.06792 39.03107 -108.08616 39.0195 -108.15531 39.00331 -108.14352 39.00562 -108.11049 39.00562 -108.05349 39.01719 -108.05334 39.02876 -108.02919 39.0542 -108.05586 39.0542 -108.06786 39.05189 -108.06789']], 'collection_concept_id': 'C1655875737-NSIDC_ECS', 'online_access_flag': True, 'links': [{'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'type': 'text/plain', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/DP1/SNOWEX/SNEX17_GPR.002/2017.02.21/SnowEx17_GPR_Version2_Week3.csv'}, {'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#', 'type': 'text/xml', 'title': '(METADATA)', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/DP1/SNOWEX/SNEX17_GPR.002/2017.02.21/SnowEx17_GPR_Version2_Week3.csv.xml'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/SNOWEX/SNEX17_GPR.002/'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://search.earthdata.nasa.gov/search/granules?p=C1655875737-NSIDC_ECS&q=SNEX17_GPR&m=29.76382409255042!-108.823974609375!4!1!0!0%2C2&tl=1558474061!4!!'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://nsidc.org/data/data-access-tool/SNEX17_GPR/versions/2/'}, {'inherited': True, 'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#', 'hreflang': 'en-US', 'href': 'https://doi.org/10.5067/G21LGCNLFSC5'}, {'inherited': True, 'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#', 'hreflang': 'en-US', 'href': 'https://doi.org/10.5067/G21LGCNLFSC5'}]}]}}\n",
  "history_begin_time" : 1642579687498,
  "history_end_time" : 1642579689465,
  "history_notes" : null,
  "history_process" : "4fh28s",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "bdubkory441",
  "history_input" : "#!/usr/bin/env python\n# ----------------------------------------------------------------------------\n# NSIDC Data Download Script\n#\n# Copyright (c) 2021 Regents of the University of Colorado\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# Tested in Python 2.7 and Python 3.4, 3.6, 3.7\n#\n# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n#   $ python nsidc-data-download.py\n#\n# On Windows, open Start menu -> Run and type cmd. Then type:\n#     python nsidc-data-download.py\n#\n# The script will first search Earthdata for all matching files.\n# You will then be prompted for your Earthdata username/password\n# and the script will download the matching files.\n#\n# If you wish, you may store your Earthdata username/password in a .netrc\n# file in your $HOME directory and the script will automatically attempt to\n# read this file. The .netrc file should have the following format:\n#    machine urs.earthdata.nasa.gov login myusername password mypassword\n# where 'myusername' and 'mypassword' are your Earthdata credentials.\n#\nfrom __future__ import print_function\n\nimport base64\nimport getopt\nimport itertools\nimport json\nimport math\nimport netrc\nimport os.path\nimport ssl\nimport sys\nimport time\nfrom getpass import getpass\nimport os\nimport re\nimport shutil\n\ntry:\n    from urllib.parse import urlparse\n    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n    from urllib.error import HTTPError, URLError\nexcept ImportError:\n    from urlparse import urlparse\n    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n\nshort_name = 'SNEX20_SD'\nversion = '1'\ntime_start = '2020-01-28T00:00:00Z'\ntime_end = '2020-02-12T23:59:59Z'\nbounding_box = ''\npolygon = ''\nfilename_filter = ''\nurl_list = []\n\nCMR_URL = 'https://cmr.earthdata.nasa.gov'\nURS_URL = 'https://urs.earthdata.nasa.gov'\nCMR_PAGE_SIZE = 2000\nCMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n\nhomedir = os.path.expanduser('~') \ndatadir = f\"{homedir}/Documents/data/\"\n\n\ndef get_username():\n    username = ''\n\n    # For Python 2/3 compatibility:\n    try:\n        do_input = raw_input  # noqa\n    except NameError:\n        do_input = input\n\n    while not username:\n        username = do_input('Earthdata username: ')\n    return username\n\n\ndef get_password():\n    password = ''\n    while not password:\n        password = getpass('password: ')\n    return password\n\n\ndef get_credentials(url):\n    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n    credentials = None\n    errprefix = ''\n    try:\n        info = netrc.netrc()\n        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n        errprefix = 'netrc error: '\n    except Exception as e:\n        if (not ('No such file' in str(e))):\n            print('netrc error: {0}'.format(str(e)))\n        username = None\n        password = None\n\n    while not credentials:\n        if not username:\n            username = get_username()\n            password = get_password()\n        credentials = '{0}:{1}'.format(username, password)\n        credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n\n        if url:\n            try:\n                req = Request(url)\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n                opener = build_opener(HTTPCookieProcessor())\n                opener.open(req)\n            except HTTPError:\n                print(errprefix + 'Incorrect username or password')\n                errprefix = ''\n                credentials = None\n                username = None\n                password = None\n\n    return credentials\n\n\ndef build_version_query_params(version):\n    desired_pad_length = 3\n    if len(version) > desired_pad_length:\n        print('Version string too long: \"{0}\"'.format(version))\n        quit()\n\n    version = str(int(version))  # Strip off any leading zeros\n    query_params = ''\n\n    while len(version) <= desired_pad_length:\n        padded_version = version.zfill(desired_pad_length)\n        query_params += '&version={0}'.format(padded_version)\n        desired_pad_length -= 1\n    return query_params\n\n\ndef filter_add_wildcards(filter):\n    if not filter.startswith('*'):\n        filter = '*' + filter\n    if not filter.endswith('*'):\n        filter = filter + '*'\n    return filter\n\n\ndef build_filename_filter(filename_filter):\n    filters = filename_filter.split(',')\n    result = '&options[producer_granule_id][pattern]=true'\n    for filter in filters:\n        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n    return result\n\n\ndef build_cmr_query_url(short_name, version, time_start, time_end,\n                        bounding_box=None, polygon=None,\n                        filename_filter=None):\n    params = '&short_name={0}'.format(short_name)\n    params += build_version_query_params(version)\n    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n    if polygon:\n        params += '&polygon={0}'.format(polygon)\n    elif bounding_box:\n        params += '&bounding_box={0}'.format(bounding_box)\n    if filename_filter:\n        params += build_filename_filter(filename_filter)\n    return CMR_FILE_URL + params\n\n\ndef get_speed(time_elapsed, chunk_size):\n    if time_elapsed <= 0:\n        return ''\n    speed = chunk_size / time_elapsed\n    if speed <= 0:\n        speed = 1\n    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    i = int(math.floor(math.log(speed, 1000)))\n    p = math.pow(1000, i)\n    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n\n\ndef output_progress(count, total, status='', bar_len=60):\n    if total <= 0:\n        return\n    fraction = min(max(count / float(total), 0), 1)\n    filled_len = int(round(bar_len * fraction))\n    percents = int(round(100.0 * fraction))\n    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n    sys.stdout.write(fmt)\n    sys.stdout.flush()\n\n\ndef cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n    while True:\n        data = file_object.read(chunk_size)\n        if not data:\n            break\n        yield data\n\n\ndef cmr_download(urls, force=False, quiet=False):\n    \"\"\"Download files from list of urls.\"\"\"\n    if not urls:\n        return\n\n    url_count = len(urls)\n    if not quiet:\n        print('Downloading {0} files...'.format(url_count))\n    credentials = None\n\n    for index, url in enumerate(urls, start=1):\n        if not credentials and urlparse(url).scheme == 'https':\n            credentials = get_credentials(url)\n\n        filename = url.split('/')[-1]\n        if not quiet:\n            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n                                        url_count, filename))\n\n        try:\n            req = Request(url)\n            if credentials:\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n            opener = build_opener(HTTPCookieProcessor())\n            response = opener.open(req)\n            length = int(response.headers['content-length'])\n            try:\n                if not force and length == os.path.getsize(filename):\n                    if not quiet:\n                        print('  File exists, skipping')\n                    continue\n            except OSError:\n                pass\n            count = 0\n            chunk_size = min(max(length, 1), 1024 * 1024)\n            max_chunks = int(math.ceil(length / chunk_size))\n            time_initial = time.time()\n            with open(filename, 'wb') as out_file:\n                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n                    out_file.write(data)\n                    if not quiet:\n                        count = count + 1\n                        time_elapsed = time.time() - time_initial\n                        download_speed = get_speed(time_elapsed, count * chunk_size)\n                        output_progress(count, max_chunks, status=download_speed)\n            if not quiet:\n                print()\n        except HTTPError as e:\n            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n        except URLError as e:\n            print('URL error: {0}'.format(e.reason))\n        except IOError:\n            raise\n\n\ndef cmr_filter_urls(search_results):\n    \"\"\"Select only the desired data files from CMR response.\"\"\"\n    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n        return []\n\n    entries = [e['links']\n               for e in search_results['feed']['entry']\n               if 'links' in e]\n    # Flatten \"entries\" to a simple list of links\n    links = list(itertools.chain(*entries))\n\n    urls = []\n    unique_filenames = set()\n    for link in links:\n        if 'href' not in link:\n            # Exclude links with nothing to download\n            continue\n        if 'inherited' in link and link['inherited'] is True:\n            # Why are we excluding these links?\n            continue\n        if 'rel' in link and 'data#' not in link['rel']:\n            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n            continue\n\n        if 'title' in link and 'opendap' in link['title'].lower():\n            # Exclude OPeNDAP links--they are responsible for many duplicates\n            # This is a hack; when the metadata is updated to properly identify\n            # non-datapool links, we should be able to do this in a non-hack way\n            continue\n\n        filename = link['href'].split('/')[-1]\n        if filename in unique_filenames:\n            # Exclude links with duplicate filenames (they would overwrite)\n            continue\n        unique_filenames.add(filename)\n\n        urls.append(link['href'])\n\n    return urls\n\n\ndef cmr_search(short_name, version, time_start, time_end,\n               bounding_box='', polygon='', filename_filter='', quiet=False):\n    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n                                        time_start=time_start, time_end=time_end,\n                                        bounding_box=bounding_box,\n                                        polygon=polygon, filename_filter=filename_filter)\n    if not quiet:\n        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n\n    cmr_scroll_id = None\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n\n    urls = []\n    hits = 0\n    while True:\n        req = Request(cmr_query_url)\n        if cmr_scroll_id:\n            req.add_header('cmr-scroll-id', cmr_scroll_id)\n        response = urlopen(req, context=ctx)\n        if not cmr_scroll_id:\n            # Python 2 and 3 have different case for the http headers\n            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n            cmr_scroll_id = headers['cmr-scroll-id']\n            hits = int(headers['cmr-hits'])\n            if not quiet:\n                if hits > 0:\n                    print('Found {0} matches.'.format(hits))\n                else:\n                    print('Found no matches.')\n        search_page = response.read()\n        search_page = json.loads(search_page.decode('utf-8'))\n        url_scroll_results = cmr_filter_urls(search_page)\n        if not url_scroll_results:\n            break\n        if not quiet and hits > CMR_PAGE_SIZE:\n            print('.', end='')\n            sys.stdout.flush()\n        urls += url_scroll_results\n\n    if not quiet and hits > CMR_PAGE_SIZE:\n        print()\n    return urls\n\n\ndef main(argv=None):\n    global short_name, version, time_start, time_end, bounding_box, \\\n        polygon, filename_filter, url_list\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    force = False\n    quiet = False\n    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n\n    try:\n        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n        for opt, _arg in opts:\n            if opt in ('-f', '--force'):\n                force = True\n            elif opt in ('-q', '--quiet'):\n                quiet = True\n            elif opt in ('-h', '--help'):\n                print(usage)\n                sys.exit(0)\n    except getopt.GetoptError as e:\n        print(e.args[0])\n        print(usage)\n        sys.exit(1)\n\n    # Supply some default search parameters, just for testing purposes.\n    # These are only used if the parameters aren't filled in up above.\n    if 'short_name' in short_name:\n        short_name = 'ATL06'\n        version = '003'\n        time_start = '2018-10-14T00:00:00Z'\n        time_end = '2021-01-08T21:48:13Z'\n        bounding_box = ''\n        polygon = ''\n        filename_filter = '*ATL06_2020111121*'\n        url_list = []\n\n    try:\n        if not url_list:\n            url_list = cmr_search(short_name, version, time_start, time_end,\n                                  bounding_box=bounding_box, polygon=polygon,\n                                  filename_filter=filename_filter, quiet=quiet)\n\n        cmr_download(url_list, force=force, quiet=quiet)\n    except KeyboardInterrupt:\n        quit()\n        \n    try:\n        # move all the downloaded csv files to $datadir\n        for filename in os.listdir('./'):\n            if re.match(r'.*csv', filename) or re.match(r'.*csv', filename):\n                shutil.move(os.path.join('./', filename), f'{datadir}')\n    except ValueError:\n        quit()\n            \n    # and so forth\n\n\nif __name__ == '__main__':\n    main()",
  "history_output" : "Querying for data:\n\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=SNEX20_SD&version=001&version=01&version=1&temporal[]=2020-01-28T00:00:00Z,2020-02-12T23:59:59Z\nFound 1 matches.\nDownloading 2 files...\n1/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [===============                                             ]  25%  6.6MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [==============================                              ]  50%  7.2MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [=============================================               ]  75%  7.8MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [============================================================] 100%  9.4MB/s   \n2/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv.xml\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [============================================================] 100%  741.9kB/s   \nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/iBclPGbEFuG34bajrTMSS68l44/snowex_nsidc_download_script.py\", line 420, in <module>\n    main()\n  File \"/Users/joe/gw-workspace/iBclPGbEFuG34bajrTMSS68l44/snowex_nsidc_download_script.py\", line 412, in main\n    shutil.move(os.path.join('./', filename), f'{datadir}')\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/shutil.py\", line 812, in move\n    raise Error(\"Destination path '%s' already exists\" % real_dst)\nshutil.Error: Destination path '/Users/joe/Documents/data/SnowEx2020_SnowDepths_COGM_alldepths_v01.csv.xml' already exists\n",
  "history_begin_time" : 1642579691454,
  "history_end_time" : 1642579698222,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "mql01z4iwg3",
  "history_input" : "from snowex_nsidc_search import * \n\n\n\n#Set NSIDC data access base URL\nbase_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n\n#Set the request mode to asynchronous, \"no\" processing agent (no subsetting or reformatting services available), and optionally removing metadata delivery\n\nparam_dict['request_mode'] = 'async'\nparam_dict['agent'] = 'NO'\nparam_dict['INCLUDE_META'] ='N' #optional if you do not wish to receive the associated metadata files with each science file. \n\nparam_string = '&'.join(\"{!s}={!r}\".format(k,v) for (k,v) in param_dict.items()) # Convert param_dict to string\nparam_string = param_string.replace(\"'\",\"\") # Remove quotes\n\napi_list = [f'{base_url}?{param_string}']\napi_request = api_list[0]\nprint(api_request) # Print API base URL + request parameters\n\n# Start authenticated session with Earthdata Login to allow for data downloads:\ndef setup_earthdata_login_auth(endpoint: str='urs.earthdata.nasa.gov'):\n    netrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n    try:\n        username, _, password = netrc.netrc(file=join(expanduser('~'), netrc_name)).authenticators(endpoint)\n    except (FileNotFoundError, TypeError):\n        print('Please provide your Earthdata Login credentials for access.')\n        print('Your info will only be passed to %s and will not be exposed in Jupyter.' % (endpoint))\n        username = input('Username: ')\n        password = getpass('Password: ')\n    manager = request.HTTPPasswordMgrWithDefaultRealm()\n    manager.add_password(None, endpoint, username, password)\n    auth = request.HTTPBasicAuthHandler(manager)\n    jar = CookieJar()\n    processor = request.HTTPCookieProcessor(jar)\n    opener = request.build_opener(auth, processor)\n    request.install_opener(opener)\n\nsetup_earthdata_login_auth(endpoint=\"urs.earthdata.nasa.gov\")\n\n\ndef request_nsidc_data(API_request):\n    \"\"\"\n    Performs a data customization and access request from NSIDC's API/\n    Creates an output folder in the working directory if one does not already exist.\n    \n    :API_request: NSIDC API endpoint; see https://nsidc.org/support/how/how-do-i-programmatically-request-data-services for more info\n    on how to configure the API request.\n    \n    \"\"\"\n\n    path = str(f'{datadir}') # Create an output folder if the folder does not already exist.\n    if not os.path.exists(path):\n        os.mkdir(path)\n        \n    base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n\n    \n    r = request.urlopen(API_request)\n    esir_root = ET.fromstring(r.read())\n    orderlist = []   # Look up order ID\n    for order in esir_root.findall(\"./order/\"):\n        orderlist.append(order.text)\n    orderID = orderlist[0]\n    statusURL = base_url + '/' + orderID # Create status URL\n    print('Order status URL: ', statusURL)\n    request_response = request.urlopen(statusURL) # Find order status  \n    request_root = ET.fromstring(request_response.read())\n    statuslist = []\n    for status in request_root.findall(\"./requestStatus/\"):\n        statuslist.append(status.text)\n    status = statuslist[0]\n    while status == 'pending' or status == 'processing': #Continue loop while request is still processing\n        print('Job status is ', status,'. Trying again.')\n        time.sleep(10)\n        loop_response = request.urlopen(statusURL)\n        loop_root = ET.fromstring(loop_response.read())\n        statuslist = [] #find status\n        for status in loop_root.findall(\"./requestStatus/\"):\n            statuslist.append(status.text)\n        status = statuslist[0]\n        if status == 'pending' or status == 'processing':\n            continue\n    if status == 'complete_with_errors' or status == 'failed': # Provide complete_with_errors error message:\n        messagelist = []\n        for message in loop_root.findall(\"./processInfo/\"):\n            messagelist.append(message.text)\n        print('Job status is ', status)\n        print('error messages:')\n        pprint(messagelist)\n    if status == 'complete' or status == 'complete_with_errors':# Download zipped order if status is complete or complete_with_errors\n        downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n        print('Job status is ', status)\n        print('Zip download URL: ', downloadURL)\n        print('Beginning download of zipped output...')\n        zip_response = request.urlopen(downloadURL)\n        with zipfile.ZipFile(io.BytesIO(zip_response.read())) as z:\n            z.extractall(path)\n        print('Download is complete.')\n    else: print('Request failed.')\n    \n    # Clean up Outputs folder by removing individual granule folders \n    for root, dirs, files in os.walk(path, topdown=False):\n        for file in files:\n            try:\n                shutil.move(os.path.join(root, file), path)\n            except OSError:\n                pass\n        for name in dirs:\n            os.rmdir(os.path.join(root, name))\n    return  \n\n\n# NOTE: downloads ~ 200MB of CSV files\nrequest_nsidc_data(api_request)",
  "history_output" : "Polygon coordinates to be used in search: -108.2352445938561,38.98556907427165,-107.85284607930835,38.978765032966244,-107.85494925720668,39.10596902171742,-108.22772795408136,39.11294532581687,-108.2352445938561,38.98556907427165\nFound 3 files\nThe total size of all files is 209.20 MB\n{'feed': {'updated': '2022-01-19T08:08:20.457Z', 'id': 'https://cmr.earthdata.nasa.gov:443/search/granules.json?short_name=SNEX17_GPR&version=2&polygon=-108.2352445938561%2C38.98556907427165%2C-107.85284607930835%2C38.978765032966244%2C-107.85494925720668%2C39.10596902171742%2C-108.22772795408136%2C39.11294532581687%2C-108.2352445938561%2C38.98556907427165&temporal=2017-01-01T00%3A00%3A00Z%2C2017-12-31T23%3A59%3A59Z', 'title': 'ECHO granule metadata', 'entry': [{'producer_granule_id': 'SnowEx17_GPR_Version2_Week1.csv', 'time_start': '2017-02-08T00:00:00.000Z', 'updated': '2019-11-20T14:19:39.156Z', 'dataset_id': 'SnowEx17 Ground Penetrating Radar V002', 'data_center': 'NSIDC_ECS', 'title': 'SC:SNEX17_GPR.002:167128516', 'coordinate_system': 'GEODETIC', 'day_night_flag': 'UNSPECIFIED', 'time_end': '2017-02-10T23:59:59.000Z', 'id': 'G1657541380-NSIDC_ECS', 'original_format': 'ECHO10', 'granule_size': '57.3195', 'browse_flag': False, 'polygons': [['39.05189 -108.06789 39.04958 -108.07092 39.02644 -108.13422 39.04032 -108.18504 39.0357 -108.2211 39.01719 -108.21534 38.99637 -108.18261 39.00562 -108.11049 39.02413 -108.06225 39.03338 -108.06213 39.02876 -108.08619 39.04264 -108.05301 39.05189 -108.05289 39.0542 -108.06786 39.05189 -108.06789']], 'collection_concept_id': 'C1655875737-NSIDC_ECS', 'online_access_flag': True, 'links': [{'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'type': 'text/plain', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/DP1/SNOWEX/SNEX17_GPR.002/2017.02.08/SnowEx17_GPR_Version2_Week1.csv'}, {'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#', 'type': 'text/xml', 'title': '(METADATA)', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/DP1/SNOWEX/SNEX17_GPR.002/2017.02.08/SnowEx17_GPR_Version2_Week1.csv.xml'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/SNOWEX/SNEX17_GPR.002/'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://search.earthdata.nasa.gov/search/granules?p=C1655875737-NSIDC_ECS&q=SNEX17_GPR&m=29.76382409255042!-108.823974609375!4!1!0!0%2C2&tl=1558474061!4!!'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://nsidc.org/data/data-access-tool/SNEX17_GPR/versions/2/'}, {'inherited': True, 'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#', 'hreflang': 'en-US', 'href': 'https://doi.org/10.5067/G21LGCNLFSC5'}, {'inherited': True, 'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#', 'hreflang': 'en-US', 'href': 'https://doi.org/10.5067/G21LGCNLFSC5'}]}, {'producer_granule_id': 'SnowEx17_GPR_Version2_Week2.csv', 'time_start': '2017-02-14T00:00:00.000Z', 'updated': '2019-11-20T14:19:39.156Z', 'dataset_id': 'SnowEx17 Ground Penetrating Radar V002', 'data_center': 'NSIDC_ECS', 'title': 'SC:SNEX17_GPR.002:167128520', 'coordinate_system': 'GEODETIC', 'day_night_flag': 'UNSPECIFIED', 'time_end': '2017-02-17T23:59:59.000Z', 'id': 'G1657541238-NSIDC_ECS', 'original_format': 'ECHO10', 'granule_size': '85.516', 'browse_flag': False, 'polygons': [['39.10738 -107.88943 39.10738 -107.89539 39.0912 -107.95508 39.07271 -108.02372 39.0542 -108.09234 39.04264 -108.16078 39.0357 -108.2113 39.03338 -108.2113 39.0195 -108.20533 39.00099 -108.18454 39.00099 -108.12811 39.00099 -108.08653 39.02644 -108.02094 39.0357 -107.94938 39.02413 -107.93155 39.04726 -107.89867 39.08195 -107.85677 39.10507 -107.86257 39.10969 -107.88644 39.10738 -107.88943']], 'collection_concept_id': 'C1655875737-NSIDC_ECS', 'online_access_flag': True, 'links': [{'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'type': 'text/plain', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/DP1/SNOWEX/SNEX17_GPR.002/2017.02.14/SnowEx17_GPR_Version2_Week2.csv'}, {'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#', 'type': 'text/xml', 'title': '(METADATA)', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/DP1/SNOWEX/SNEX17_GPR.002/2017.02.14/SnowEx17_GPR_Version2_Week2.csv.xml'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/SNOWEX/SNEX17_GPR.002/'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://search.earthdata.nasa.gov/search/granules?p=C1655875737-NSIDC_ECS&q=SNEX17_GPR&m=29.76382409255042!-108.823974609375!4!1!0!0%2C2&tl=1558474061!4!!'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://nsidc.org/data/data-access-tool/SNEX17_GPR/versions/2/'}, {'inherited': True, 'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#', 'hreflang': 'en-US', 'href': 'https://doi.org/10.5067/G21LGCNLFSC5'}, {'inherited': True, 'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#', 'hreflang': 'en-US', 'href': 'https://doi.org/10.5067/G21LGCNLFSC5'}]}, {'producer_granule_id': 'SnowEx17_GPR_Version2_Week3.csv', 'time_start': '2017-02-21T00:00:00.000Z', 'updated': '2020-02-18T12:25:16.785Z', 'dataset_id': 'SnowEx17 Ground Penetrating Radar V002', 'data_center': 'NSIDC_ECS', 'title': 'SC:SNEX17_GPR.002:173252482', 'coordinate_system': 'GEODETIC', 'day_night_flag': 'UNSPECIFIED', 'time_end': '2017-02-25T23:59:59.000Z', 'id': 'G1694922459-NSIDC_ECS', 'original_format': 'ECHO10', 'granule_size': '66.3598', 'browse_flag': False, 'polygons': [['39.05189 -108.06789 39.04958 -108.06792 39.03107 -108.08616 39.0195 -108.15531 39.00331 -108.14352 39.00562 -108.11049 39.00562 -108.05349 39.01719 -108.05334 39.02876 -108.02919 39.0542 -108.05586 39.0542 -108.06786 39.05189 -108.06789']], 'collection_concept_id': 'C1655875737-NSIDC_ECS', 'online_access_flag': True, 'links': [{'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'type': 'text/plain', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/DP1/SNOWEX/SNEX17_GPR.002/2017.02.21/SnowEx17_GPR_Version2_Week3.csv'}, {'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#', 'type': 'text/xml', 'title': '(METADATA)', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/DP1/SNOWEX/SNEX17_GPR.002/2017.02.21/SnowEx17_GPR_Version2_Week3.csv.xml'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://n5eil01u.ecs.nsidc.org/SNOWEX/SNEX17_GPR.002/'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://search.earthdata.nasa.gov/search/granules?p=C1655875737-NSIDC_ECS&q=SNEX17_GPR&m=29.76382409255042!-108.823974609375!4!1!0!0%2C2&tl=1558474061!4!!'}, {'inherited': True, 'length': '0.0KB', 'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', 'hreflang': 'en-US', 'href': 'https://nsidc.org/data/data-access-tool/SNEX17_GPR/versions/2/'}, {'inherited': True, 'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#', 'hreflang': 'en-US', 'href': 'https://doi.org/10.5067/G21LGCNLFSC5'}, {'inherited': True, 'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#', 'hreflang': 'en-US', 'href': 'https://doi.org/10.5067/G21LGCNLFSC5'}]}]}}\nhttps://n5eil02u.ecs.nsidc.org/egi/request?short_name=SNEX17_GPR&version=2&polygon=-108.2352445938561,38.98556907427165,-107.85284607930835,38.978765032966244,-107.85494925720668,39.10596902171742,-108.22772795408136,39.11294532581687,-108.2352445938561,38.98556907427165&temporal=2017-01-01T00:00:00Z,2017-12-31T23:59:59Z&request_mode=async&agent=NO&INCLUDE_META=N\nOrder status URL:  https://n5eil02u.ecs.nsidc.org/egi/request/5000002721510\nJob status is  complete\nZip download URL:  https://n5eil02u.ecs.nsidc.org/esir/5000002721510.zip\nBeginning download of zipped output...\nDownload is complete.\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/iBclPGbEFuG34bajrTMSS68l44/snowex_nsidc_download_api.py\", line 115, in <module>\n    request_nsidc_data(api_request)\n  File \"/Users/joe/gw-workspace/iBclPGbEFuG34bajrTMSS68l44/snowex_nsidc_download_api.py\", line 110, in request_nsidc_data\n    os.rmdir(os.path.join(root, name))\nOSError: [Errno 66] Directory not empty: '/Users/joe/Documents/data/173252482'\n",
  "history_begin_time" : 1642579699566,
  "history_end_time" : 1642579714433,
  "history_notes" : null,
  "history_process" : "li3z29",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "yvyh1xs5hu0",
  "history_input" : "import os\nimport geopandas as gpd\nfrom shapely.geometry import Polygon, mapping\nfrom shapely.geometry.polygon import orient\nimport pandas as pd \nimport requests\nimport json\nimport pprint\nimport getpass\nimport netrc\nfrom platform import system\nfrom getpass import getpass\nfrom urllib import request\nfrom http.cookiejar import CookieJar\nfrom os.path import join, expanduser\nimport requests\nfrom xml.etree import ElementTree as ET\nimport time\nimport zipfile\nimport io\nimport shutil\n\nhomedir = os.path.expanduser('~')\ndatadir = f\"{homedir}/Documents/data/\"\n\nsnowex_path = f'{datadir}/SnowEx17_GPR_Version2_Week1.csv' # Define local filepath\ndf = pd.read_csv(snowex_path, sep='\\t') \nprint(df.head())\n\n# extract date columns\n\ndf['date'] = df.collection.str.rsplit('_').str[-1].astype(str)\ndf.date = pd.to_datetime(df.date, format=\"%m%d%y\")\ndf = df.sort_values(['date'])\nprint(df.head())\n\n# Convert to Geopandas dataframe to provide point geometry\n\ngdf_utm= gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs='EPSG:32612')\nprint(gdf_utm.head())\n\n",
  "history_output" : "        collection  trace        long  ...              x             y  UTM_Zone\n0  GPR_0042_020817   2581 -108.066856  ...  753854.880092  4.325659e+06      12 S\n1  GPR_0042_020817   2582 -108.066856  ...  753854.899385  4.325660e+06      12 S\n2  GPR_0042_020817   2583 -108.066856  ...  753854.918686  4.325660e+06      12 S\n3  GPR_0042_020817   2584 -108.066855  ...  753854.937987  4.325660e+06      12 S\n4  GPR_0042_020817   2585 -108.066855  ...  753854.957280  4.325660e+06      12 S\n[5 rows x 11 columns]\n             collection  trace        long  ...             y  UTM_Zone       date\n0       GPR_0042_020817   2581 -108.066856  ...  4.325659e+06      12 S 2017-02-08\n109172  GPR_0043_020817   6360 -108.063209  ...  4.326342e+06      12 S 2017-02-08\n109173  GPR_0043_020817   6361 -108.063209  ...  4.326342e+06      12 S 2017-02-08\n109174  GPR_0043_020817   6362 -108.063208  ...  4.326342e+06      12 S 2017-02-08\n109175  GPR_0043_020817   6363 -108.063208  ...  4.326342e+06      12 S 2017-02-08\n[5 rows x 12 columns]\n             collection  trace  ...       date                        geometry\n0       GPR_0042_020817   2581  ... 2017-02-08  POINT (753854.880 4325659.484)\n109172  GPR_0043_020817   6360  ... 2017-02-08  POINT (754148.854 4326341.915)\n109173  GPR_0043_020817   6361  ... 2017-02-08  POINT (754148.883 4326341.916)\n109174  GPR_0043_020817   6362  ... 2017-02-08  POINT (754148.911 4326341.917)\n109175  GPR_0043_020817   6363  ... 2017-02-08  POINT (754148.947 4326341.918)\n[5 rows x 13 columns]\n",
  "history_begin_time" : 1642579716324,
  "history_end_time" : 1642579723012,
  "history_notes" : null,
  "history_process" : "pvrvwa",
  "host_id" : "100001",
  "indicator" : "Done"
}]
