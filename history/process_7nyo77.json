[{
  "history_id" : "BtWZlSYjD7L8",
  "history_input" : "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\nimport requests\nimport os\nimport tensorflow as tf\n\nhomedir = os.path.expanduser('~')\ndatadir = f\"{homedir}/snowx/\"\n\nexampledataurl = \"https://raw.githubusercontent.com/snowex-hackweek/website/main/book/tutorials/machine-learning/data/snow_depth_data.csv\"\n\nr = requests.get(exampledataurl, allow_redirects=True)\n\nopen(f\"{datadir}snow_depth_data.csv\", 'wb').write(r.content)\n\n# retrieve the example training data first\ndataset = pd.read_csv(f\"{datadir}snow_depth_data.csv\")\n\nprint(dataset.info())\n\nprint(dataset.head())\n\ntrain_dataset = dataset.sample(frac=0.8, random_state=0)\ntest_dataset = dataset.drop(train_dataset.index)\n\ntrain_dataset.describe().transpose()\n\nscaler = MinMaxScaler()\nscaled_train = scaler.fit_transform(train_dataset)\nscaled_test = scaler.transform(test_dataset) ## fit_transform != transform. \n## transform uses the parameters of fit_transform\n\n  \n# Sepatare Features from Labels\ntrain_X, train_y = scaled_train[:, :-1], scaled_train[:, -1]\ntest_X, test_y = scaled_test[:, :-1], scaled_test[:, -1]\n\nprint(\"TensorFlow version ==>\", tf.__version__) \nprint(\"Keras version ==>\",tf.keras.__version__)\n\ntf.random.set_seed(0) ## For reproducible results\nlinear_regression = tf.keras.models.Sequential() # Specify layers in their sequential order\n# inputs are 4 dimensions (4 dimensions = 4 features)\n# Dense = Fully Connected.  \nlinear_regression.add(tf.keras.layers.Dense(1, activation=None ,input_shape=(train_X.shape[1],)))\n# Output layer has no activation with just 1 node\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nlinear_regression.compile(optimizer = opt, loss='mean_squared_error')\n\nprint(linear_regression.summary())\n\n# NOTE: can changed from epochs=150 to run faster, change to verbose=1 for per-epoch output\nhistory =linear_regression.fit(train_X, train_y, epochs=100, validation_split = 0.2, verbose=0)\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.grid(True)\n\n# linear regression\n\nlinear_regression.get_weights()\n\ndef Sigmoid(z):\n    \"\"\"\n    A function that performs the sigmoid transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* logistic: the transformed/activated version of the array\n    \"\"\"\n    \n    logistic = 1/(1+ np.exp(-z))\n    return logistic\n    \n\ndef Tanh(z):\n    \"\"\"\n    A function that performs the hyperbolic tangent transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* hyp: the transformed/activated version of the array\n    \"\"\"\n    \n    hyp = np.tanh(z)\n    return hyp\n\n\ndef ReLu(z):\n    \"\"\"\n    A function that performs the hyperbolic tangent transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* points: the transformed/activated version of the array\n    \"\"\"\n    \n    points = np.where(z < 0, 0, z)\n    return points\n\nz = np.linspace(-10,10)\nfa = plt.figure(figsize=(16,5))\n\nplt.subplot(1,3,1)\nplt.plot(z,Sigmoid(z),color=\"red\", label=r'$\\frac{1}{1 + e^{-z}}$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Sigmoid Activation Function\")\nplt.legend(loc='best',fontsize = 22)\n\n\nplt.subplot(1,3,2)\nplt.plot(z,Tanh(z),color=\"red\", label=r'$\\tanh (z)$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Hyperbolic Tangent Activation Function\")\nplt.legend(loc='best',fontsize = 18)\n\nplt.subplot(1,3,3)\nplt.plot(z,ReLu(z),color=\"red\", label=r'$\\max(0,z)$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Rectified Linear Unit Activation Function\")\nplt.legend(loc='best', fontsize = 18)\n\ntf.random.set_seed(1000)  ## For reproducible results\nnetwork = tf.keras.models.Sequential() # Specify layers in their sequential order\n# inputs are 4 dimensions (4 dimensions = 4 features)\n# Dense = Fully Connected.   \n# First hidden layer has 1000 neurons with relu activations.\n# Second hidden layer has 512 neurons with relu activations\n# Third hidden layer has 256 neurons with Sigmoid activations\nnetwork.add(tf.keras.layers.Dense(1000, activation='relu' ,input_shape=(train_X.shape[1],)))\nnetwork.add(tf.keras.layers.Dense(512, activation='relu')) # sigmoid, tanh\nnetwork.add(tf.keras.layers.Dense(256, activation='sigmoid'))\n# Output layer uses no activation with 1 output neurons\nnetwork.add(tf.keras.layers.Dense(1)) # Output layer\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nnetwork.compile(optimizer = opt, loss='mean_squared_error')\n\nnetwork.summary()\n\n# NOTE: if you have time, consider upping epochs -> 150\nhistory =network.fit(train_X, train_y, epochs=20, validation_split = 0.2, verbose=0)\n\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('Error')\nplt.legend()\nplt.grid(True)\n\n# Prediction\n\n## Linear Regression\n\nyhat_linReg = linear_regression.predict(test_X)\ninv_yhat_linReg = np.concatenate((test_X, yhat_linReg), axis=1)\ninv_yhat_linReg = scaler.inverse_transform(inv_yhat_linReg)\ninv_yhat_linReg = inv_yhat_linReg[:,-1]\n\n## DNN\nyhat_dnn = network.predict(test_X) \ninv_yhat_dnn = np.concatenate((test_X, yhat_dnn), axis=1)\ninv_yhat_dnn = scaler.inverse_transform(inv_yhat_dnn)\ninv_yhat_dnn = inv_yhat_dnn[:,-1]\n\n## True Snow Depth (Test Set)\ninv_y = test_dataset[\"snow_depth\"]\n\n\n## Put Observed and Predicted (Linear Regression and DNN) in a Dataframe\nprediction_df = pd.DataFrame({\"Observed\": inv_y,\n                    \"LR\":inv_yhat_linReg, \"DNN\":inv_yhat_dnn})\n\n# check performance\n\ndef metrics_print(test_data,test_predict):\n    print('Test RMSE: ', round(np.sqrt(mean_squared_error(test_data, test_predict)), 2))\n    print('Test R^2 : ', round((r2_score(test_data, test_predict)*100), 2) ,\"%\")\n    print('Test MAPE: ', round(mean_absolute_percentage_error(test_data, test_predict)*100,2), '%')\n    \nprint(\"##************** Linear Regression Results **************##\")\nmetrics_print(prediction_df['Observed'], prediction_df['LR'])\nprint(\" \")\nprint(\" \")\n\nprint(\"##************** Deep Learning Results **************##\")\nmetrics_print(prediction_df['Observed'], prediction_df['DNN'])\nprint(\" \")\nprint(\" \")\n\n# visualize performance\n\nfa = plt.figure(figsize=(16,5))\nplt.subplot(1,2,1)\nplt.scatter(prediction_df['Observed'],prediction_df['LR'])\nplt.xlabel('True Values [snow_depth]', fontsize=15)\nplt.ylabel('Predictions [snow_depth]', fontsize=15)\nplt.title(\"Linear Regression\")\n\n\nplt.subplot(1,2,2)\nplt.scatter(prediction_df['Observed'],prediction_df['DNN'])\nplt.xlabel('True Values [snow_depth]', fontsize=15)\nplt.ylabel('Predictions [snow_depth]', fontsize=15)\nplt.title(\"Deep Neural Network\")\n\n# visualize error\n\nLR_error = prediction_df['Observed'] - prediction_df['LR']\nDNN_error = prediction_df['Observed'] - prediction_df['DNN']\n\nfa = plt.figure(figsize=(16,5))\n\nplt.subplot(1,2,1)\nLR_error.hist()\nplt.xlabel('Error', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.title(\"Linear Regression\")\n\nplt.subplot(1,2,2)\nDNN_error.hist()\nplt.xlabel('Error', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.title(\"Deep Neural Network\")\n\n# save best model\n\nnetwork.save('DNN')\n\n## To load model, use;\nmodel = tf.keras.models.load_model('DNN')\n\n\n\n\n",
  "history_output" : "2022-07-10 18:47:06.499099: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-07-10 18:47:06.572244: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-07-10 18:47:21.165224: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3000 entries, 0 to 2999\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   amplitude   3000 non-null   float64\n 1   coherence   3000 non-null   float64\n 2   phase       3000 non-null   float64\n 3   inc_ang     3000 non-null   float64\n 4   snow_depth  3000 non-null   float64\ndtypes: float64(5)\nmemory usage: 117.3 KB\nNone\n   amplitude  coherence     phase   inc_ang  snow_depth\n0   0.198821   0.866390 -8.844648  0.797111    0.969895\n1   0.198821   0.866390 -8.844648  0.797111    1.006760\n2   0.212010   0.785662 -8.843649  0.799110    1.033615\n3   0.212010   0.785662 -8.843649  0.799110    1.043625\n4   0.173967   0.714862 -8.380865  0.792508    1.593430\nTensorFlow version ==> 2.5.0\nKeras version ==> 2.5.0\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 1)                 5         \n=================================================================\nTotal params: 5\nTrainable params: 5\nNon-trainable params: 0\n_________________________________________________________________\nNone\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 1000)              5000      \n_________________________________________________________________\ndense_2 (Dense)              (None, 512)               512512    \n_________________________________________________________________\ndense_3 (Dense)              (None, 256)               131328    \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 257       \n=================================================================\nTotal params: 649,097\nTrainable params: 649,097\nNon-trainable params: 0\n_________________________________________________________________\n##************** Linear Regression Results **************##\nTest RMSE:  0.47\nTest R^2 :  47.74 %\nTest MAPE:  41.44 %\n \n \n##************** Deep Learning Results **************##\nTest RMSE:  0.31\nTest R^2 :  76.22 %\nTest MAPE:  27.64 %\n \n \n",
  "history_begin_time" : 1657493203266,
  "history_end_time" : 1657493242571,
  "history_notes" : null,
  "history_process" : "7nyo77",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jo07kqx2uyv",
  "history_input" : "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\nimport requests\nimport os\nimport tensorflow as tf\n\nhomedir = os.path.expanduser('~')\ndatadir = f\"{homedir}/Documents/data/\"\n\nexampledataurl = \"https://raw.githubusercontent.com/snowex-hackweek/website/main/book/tutorials/machine-learning/data/snow_depth_data.csv\"\n\nr = requests.get(exampledataurl, allow_redirects=True)\n\nopen(f\"{datadir}snow_depth_data.csv\", 'wb').write(r.content)\n\n# retrieve the example training data first\ndataset = pd.read_csv(\"data/snow_depth_data.csv\")\n\nprint(dataset.info())\n\nprint(dataset.head())\n\ntrain_dataset = dataset.sample(frac=0.8, random_state=0)\ntest_dataset = dataset.drop(train_dataset.index)\n\ntrain_dataset.describe().transpose()\n\nscaler = MinMaxScaler()\nscaled_train = scaler.fit_transform(train_dataset)\nscaled_test = scaler.transform(test_dataset) ## fit_transform != transform. \n## transform uses the parameters of fit_transform\n\n  \n# Sepatare Features from Labels\ntrain_X, train_y = scaled_train[:, :-1], scaled_train[:, -1]\ntest_X, test_y = scaled_test[:, :-1], scaled_test[:, -1]\n\nprint(\"TensorFlow version ==>\", tf.__version__) \nprint(\"Keras version ==>\",tf.keras.__version__)\n\ntf.random.set_seed(0) ## For reproducible results\nlinear_regression = tf.keras.models.Sequential() # Specify layers in their sequential order\n# inputs are 4 dimensions (4 dimensions = 4 features)\n# Dense = Fully Connected.  \nlinear_regression.add(tf.keras.layers.Dense(1, activation=None ,input_shape=(train_X.shape[1],)))\n# Output layer has no activation with just 1 node\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nlinear_regression.compile(optimizer = opt, loss='mean_squared_error')\n\nprint(linear_regression.summary())\n\n# NOTE: can changed from epochs=150 to run faster, change to verbose=1 for per-epoch output\nhistory =linear_regression.fit(train_X, train_y, epochs=100, validation_split = 0.2, verbose=0)\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.grid(True)\n\n# linear regression\n\nlinear_regression.get_weights()\n\ndef Sigmoid(z):\n    \"\"\"\n    A function that performs the sigmoid transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* logistic: the transformed/activated version of the array\n    \"\"\"\n    \n    logistic = 1/(1+ np.exp(-z))\n    return logistic\n    \n\ndef Tanh(z):\n    \"\"\"\n    A function that performs the hyperbolic tangent transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* hyp: the transformed/activated version of the array\n    \"\"\"\n    \n    hyp = np.tanh(z)\n    return hyp\n\n\ndef ReLu(z):\n    \"\"\"\n    A function that performs the hyperbolic tangent transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* points: the transformed/activated version of the array\n    \"\"\"\n    \n    points = np.where(z < 0, 0, z)\n    return points\n\nz = np.linspace(-10,10)\nfa = plt.figure(figsize=(16,5))\n\nplt.subplot(1,3,1)\nplt.plot(z,Sigmoid(z),color=\"red\", label=r'$\\frac{1}{1 + e^{-z}}$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Sigmoid Activation Function\")\nplt.legend(loc='best',fontsize = 22)\n\n\nplt.subplot(1,3,2)\nplt.plot(z,Tanh(z),color=\"red\", label=r'$\\tanh (z)$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Hyperbolic Tangent Activation Function\")\nplt.legend(loc='best',fontsize = 18)\n\nplt.subplot(1,3,3)\nplt.plot(z,ReLu(z),color=\"red\", label=r'$\\max(0,z)$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Rectified Linear Unit Activation Function\")\nplt.legend(loc='best', fontsize = 18)\n\ntf.random.set_seed(1000)  ## For reproducible results\nnetwork = tf.keras.models.Sequential() # Specify layers in their sequential order\n# inputs are 4 dimensions (4 dimensions = 4 features)\n# Dense = Fully Connected.   \n# First hidden layer has 1000 neurons with relu activations.\n# Second hidden layer has 512 neurons with relu activations\n# Third hidden layer has 256 neurons with Sigmoid activations\nnetwork.add(tf.keras.layers.Dense(1000, activation='relu' ,input_shape=(train_X.shape[1],)))\nnetwork.add(tf.keras.layers.Dense(512, activation='relu')) # sigmoid, tanh\nnetwork.add(tf.keras.layers.Dense(256, activation='sigmoid'))\n# Output layer uses no activation with 1 output neurons\nnetwork.add(tf.keras.layers.Dense(1)) # Output layer\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nnetwork.compile(optimizer = opt, loss='mean_squared_error')\n\nnetwork.summary()\n\n# NOTE: if you have time, consider upping epochs -> 150\nhistory =network.fit(train_X, train_y, epochs=20, validation_split = 0.2, verbose=0)\n\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('Error')\nplt.legend()\nplt.grid(True)\n\n# Prediction\n\n## Linear Regression\n\nyhat_linReg = linear_regression.predict(test_X)\ninv_yhat_linReg = np.concatenate((test_X, yhat_linReg), axis=1)\ninv_yhat_linReg = scaler.inverse_transform(inv_yhat_linReg)\ninv_yhat_linReg = inv_yhat_linReg[:,-1]\n\n## DNN\nyhat_dnn = network.predict(test_X) \ninv_yhat_dnn = np.concatenate((test_X, yhat_dnn), axis=1)\ninv_yhat_dnn = scaler.inverse_transform(inv_yhat_dnn)\ninv_yhat_dnn = inv_yhat_dnn[:,-1]\n\n## True Snow Depth (Test Set)\ninv_y = test_dataset[\"snow_depth\"]\n\n\n## Put Observed and Predicted (Linear Regression and DNN) in a Dataframe\nprediction_df = pd.DataFrame({\"Observed\": inv_y,\n                    \"LR\":inv_yhat_linReg, \"DNN\":inv_yhat_dnn})\n\n# check performance\n\ndef metrics_print(test_data,test_predict):\n    print('Test RMSE: ', round(np.sqrt(mean_squared_error(test_data, test_predict)), 2))\n    print('Test R^2 : ', round((r2_score(test_data, test_predict)*100), 2) ,\"%\")\n    print('Test MAPE: ', round(mean_absolute_percentage_error(test_data, test_predict)*100,2), '%')\n    \nprint(\"##************** Linear Regression Results **************##\")\nmetrics_print(prediction_df['Observed'], prediction_df['LR'])\nprint(\" \")\nprint(\" \")\n\nprint(\"##************** Deep Learning Results **************##\")\nmetrics_print(prediction_df['Observed'], prediction_df['DNN'])\nprint(\" \")\nprint(\" \")\n\n# visualize performance\n\nfa = plt.figure(figsize=(16,5))\nplt.subplot(1,2,1)\nplt.scatter(prediction_df['Observed'],prediction_df['LR'])\nplt.xlabel('True Values [snow_depth]', fontsize=15)\nplt.ylabel('Predictions [snow_depth]', fontsize=15)\nplt.title(\"Linear Regression\")\n\n\nplt.subplot(1,2,2)\nplt.scatter(prediction_df['Observed'],prediction_df['DNN'])\nplt.xlabel('True Values [snow_depth]', fontsize=15)\nplt.ylabel('Predictions [snow_depth]', fontsize=15)\nplt.title(\"Deep Neural Network\")\n\n# visualize error\n\nLR_error = prediction_df['Observed'] - prediction_df['LR']\nDNN_error = prediction_df['Observed'] - prediction_df['DNN']\n\nfa = plt.figure(figsize=(16,5))\n\nplt.subplot(1,2,1)\nLR_error.hist()\nplt.xlabel('Error', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.title(\"Linear Regression\")\n\nplt.subplot(1,2,2)\nDNN_error.hist()\nplt.xlabel('Error', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.title(\"Deep Neural Network\")\n\n# save best model\n\nnetwork.save('DNN')\n\n## To load model, use;\nmodel = tf.keras.models.load_model('DNN')\n\n\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1642609039209,
  "history_end_time" : 1642609040339,
  "history_notes" : null,
  "history_process" : "7nyo77",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ujbcj1fjz52",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642608943452,
  "history_end_time" : 1642608943468,
  "history_notes" : null,
  "history_process" : "7nyo77",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "rusmwbrcc8e",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642608919231,
  "history_end_time" : 1642608919254,
  "history_notes" : null,
  "history_process" : "7nyo77",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "m5usxrww5tl",
  "history_input" : "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\nimport requests\nimport os\nimport tensorflow as tf\n\nhomedir = os.path.expanduser('~')\ndatadir = f\"{homedir}/Documents/data/\"\n\nexampledataurl = \"https://raw.githubusercontent.com/snowex-hackweek/website/main/book/tutorials/machine-learning/data/snow_depth_data.csv\"\n\nr = requests.get(exampledataurl, allow_redirects=True)\n\nopen(f\"{datadir}snow_depth_data.csv\", 'wb').write(r.content)\n\n# retrieve the example training data first\ndataset = pd.read_csv(\"data/snow_depth_data.csv\")\n\nprint(dataset.info())\n\nprint(dataset.head())\n\ntrain_dataset = dataset.sample(frac=0.8, random_state=0)\ntest_dataset = dataset.drop(train_dataset.index)\n\ntrain_dataset.describe().transpose()\n\nscaler = MinMaxScaler()\nscaled_train = scaler.fit_transform(train_dataset)\nscaled_test = scaler.transform(test_dataset) ## fit_transform != transform. \n## transform uses the parameters of fit_transform\n\n  \n# Sepatare Features from Labels\ntrain_X, train_y = scaled_train[:, :-1], scaled_train[:, -1]\ntest_X, test_y = scaled_test[:, :-1], scaled_test[:, -1]\n\nprint(\"TensorFlow version ==>\", tf.__version__) \nprint(\"Keras version ==>\",tf.keras.__version__)\n\ntf.random.set_seed(0) ## For reproducible results\nlinear_regression = tf.keras.models.Sequential() # Specify layers in their sequential order\n# inputs are 4 dimensions (4 dimensions = 4 features)\n# Dense = Fully Connected.  \nlinear_regression.add(tf.keras.layers.Dense(1, activation=None ,input_shape=(train_X.shape[1],)))\n# Output layer has no activation with just 1 node\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nlinear_regression.compile(optimizer = opt, loss='mean_squared_error')\n\nprint(linear_regression.summary())\n\n# NOTE: can changed from epochs=150 to run faster, change to verbose=1 for per-epoch output\nhistory =linear_regression.fit(train_X, train_y, epochs=100, validation_split = 0.2, verbose=0)\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.grid(True)\n\n# linear regression\n\nlinear_regression.get_weights()\n\ndef Sigmoid(z):\n    \"\"\"\n    A function that performs the sigmoid transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* logistic: the transformed/activated version of the array\n    \"\"\"\n    \n    logistic = 1/(1+ np.exp(-z))\n    return logistic\n    \n\ndef Tanh(z):\n    \"\"\"\n    A function that performs the hyperbolic tangent transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* hyp: the transformed/activated version of the array\n    \"\"\"\n    \n    hyp = np.tanh(z)\n    return hyp\n\n\ndef ReLu(z):\n    \"\"\"\n    A function that performs the hyperbolic tangent transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* points: the transformed/activated version of the array\n    \"\"\"\n    \n    points = np.where(z < 0, 0, z)\n    return points\n\nz = np.linspace(-10,10)\nfa = plt.figure(figsize=(16,5))\n\nplt.subplot(1,3,1)\nplt.plot(z,Sigmoid(z),color=\"red\", label=r'$\\frac{1}{1 + e^{-z}}$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Sigmoid Activation Function\")\nplt.legend(loc='best',fontsize = 22)\n\n\nplt.subplot(1,3,2)\nplt.plot(z,Tanh(z),color=\"red\", label=r'$\\tanh (z)$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Hyperbolic Tangent Activation Function\")\nplt.legend(loc='best',fontsize = 18)\n\nplt.subplot(1,3,3)\nplt.plot(z,ReLu(z),color=\"red\", label=r'$\\max(0,z)$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Rectified Linear Unit Activation Function\")\nplt.legend(loc='best', fontsize = 18)\n\ntf.random.set_seed(1000)  ## For reproducible results\nnetwork = tf.keras.models.Sequential() # Specify layers in their sequential order\n# inputs are 4 dimensions (4 dimensions = 4 features)\n# Dense = Fully Connected.   \n# First hidden layer has 1000 neurons with relu activations.\n# Second hidden layer has 512 neurons with relu activations\n# Third hidden layer has 256 neurons with Sigmoid activations\nnetwork.add(tf.keras.layers.Dense(1000, activation='relu' ,input_shape=(train_X.shape[1],)))\nnetwork.add(tf.keras.layers.Dense(512, activation='relu')) # sigmoid, tanh\nnetwork.add(tf.keras.layers.Dense(256, activation='sigmoid'))\n# Output layer uses no activation with 1 output neurons\nnetwork.add(tf.keras.layers.Dense(1)) # Output layer\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nnetwork.compile(optimizer = opt, loss='mean_squared_error')\n\nnetwork.summary()\n\n# NOTE: if you have time, consider upping epochs -> 150\nhistory =network.fit(train_X, train_y, epochs=20, validation_split = 0.2, verbose=0)\n\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('Error')\nplt.legend()\nplt.grid(True)\n\n# Prediction\n\n## Linear Regression\n\nyhat_linReg = linear_regression.predict(test_X)\ninv_yhat_linReg = np.concatenate((test_X, yhat_linReg), axis=1)\ninv_yhat_linReg = scaler.inverse_transform(inv_yhat_linReg)\ninv_yhat_linReg = inv_yhat_linReg[:,-1]\n\n## DNN\nyhat_dnn = network.predict(test_X) \ninv_yhat_dnn = np.concatenate((test_X, yhat_dnn), axis=1)\ninv_yhat_dnn = scaler.inverse_transform(inv_yhat_dnn)\ninv_yhat_dnn = inv_yhat_dnn[:,-1]\n\n## True Snow Depth (Test Set)\ninv_y = test_dataset[\"snow_depth\"]\n\n\n## Put Observed and Predicted (Linear Regression and DNN) in a Dataframe\nprediction_df = pd.DataFrame({\"Observed\": inv_y,\n                    \"LR\":inv_yhat_linReg, \"DNN\":inv_yhat_dnn})\n\n# check performance\n\ndef metrics_print(test_data,test_predict):\n    print('Test RMSE: ', round(np.sqrt(mean_squared_error(test_data, test_predict)), 2))\n    print('Test R^2 : ', round((r2_score(test_data, test_predict)*100), 2) ,\"%\")\n    print('Test MAPE: ', round(mean_absolute_percentage_error(test_data, test_predict)*100,2), '%')\n    \nprint(\"##************** Linear Regression Results **************##\")\nmetrics_print(prediction_df['Observed'], prediction_df['LR'])\nprint(\" \")\nprint(\" \")\n\nprint(\"##************** Deep Learning Results **************##\")\nmetrics_print(prediction_df['Observed'], prediction_df['DNN'])\nprint(\" \")\nprint(\" \")\n\n# visualize performance\n\nfa = plt.figure(figsize=(16,5))\nplt.subplot(1,2,1)\nplt.scatter(prediction_df['Observed'],prediction_df['LR'])\nplt.xlabel('True Values [snow_depth]', fontsize=15)\nplt.ylabel('Predictions [snow_depth]', fontsize=15)\nplt.title(\"Linear Regression\")\n\n\nplt.subplot(1,2,2)\nplt.scatter(prediction_df['Observed'],prediction_df['DNN'])\nplt.xlabel('True Values [snow_depth]', fontsize=15)\nplt.ylabel('Predictions [snow_depth]', fontsize=15)\nplt.title(\"Deep Neural Network\")\n\n# visualize error\n\nLR_error = prediction_df['Observed'] - prediction_df['LR']\nDNN_error = prediction_df['Observed'] - prediction_df['DNN']\n\nfa = plt.figure(figsize=(16,5))\n\nplt.subplot(1,2,1)\nLR_error.hist()\nplt.xlabel('Error', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.title(\"Linear Regression\")\n\nplt.subplot(1,2,2)\nDNN_error.hist()\nplt.xlabel('Error', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.title(\"Deep Neural Network\")\n\n# save best model\n\nnetwork.save('DNN')\n\n## To load model, use;\nmodel = tf.keras.models.load_model('DNN')\n\n\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1642608631628,
  "history_end_time" : 1642608632750,
  "history_notes" : null,
  "history_process" : "7nyo77",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "zvibuu3zpza",
  "history_input" : "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\nimport requests\nimport os\nimport tensorflow as tf\n\nhomedir = os.path.expanduser('~')\ndatadir = f\"{homedir}/Documents/data/\"\n\nexampledataurl = \"https://raw.githubusercontent.com/snowex-hackweek/website/main/book/tutorials/machine-learning/data/snow_depth_data.csv\"\n\nr = requests.get(exampledataurl, allow_redirects=True)\n\nopen(f\"{datadir}snow_depth_data.csv\", 'wb').write(r.content)\n\n# retrieve the example training data first\ndataset = pd.read_csv(\"data/snow_depth_data.csv\")\n\nprint(dataset.info())\n\nprint(dataset.head())\n\ntrain_dataset = dataset.sample(frac=0.8, random_state=0)\ntest_dataset = dataset.drop(train_dataset.index)\n\ntrain_dataset.describe().transpose()\n\nscaler = MinMaxScaler()\nscaled_train = scaler.fit_transform(train_dataset)\nscaled_test = scaler.transform(test_dataset) ## fit_transform != transform. \n## transform uses the parameters of fit_transform\n\n  \n# Sepatare Features from Labels\ntrain_X, train_y = scaled_train[:, :-1], scaled_train[:, -1]\ntest_X, test_y = scaled_test[:, :-1], scaled_test[:, -1]\n\nprint(\"TensorFlow version ==>\", tf.__version__) \nprint(\"Keras version ==>\",tf.keras.__version__)\n\ntf.random.set_seed(0) ## For reproducible results\nlinear_regression = tf.keras.models.Sequential() # Specify layers in their sequential order\n# inputs are 4 dimensions (4 dimensions = 4 features)\n# Dense = Fully Connected.  \nlinear_regression.add(tf.keras.layers.Dense(1, activation=None ,input_shape=(train_X.shape[1],)))\n# Output layer has no activation with just 1 node\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nlinear_regression.compile(optimizer = opt, loss='mean_squared_error')\n\nprint(linear_regression.summary())\n\n# NOTE: can changed from epochs=150 to run faster, change to verbose=1 for per-epoch output\nhistory =linear_regression.fit(train_X, train_y, epochs=100, validation_split = 0.2, verbose=0)\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.grid(True)\n\n# linear regression\n\nlinear_regression.get_weights()\n\ndef Sigmoid(z):\n    \"\"\"\n    A function that performs the sigmoid transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* logistic: the transformed/activated version of the array\n    \"\"\"\n    \n    logistic = 1/(1+ np.exp(-z))\n    return logistic\n    \n\ndef Tanh(z):\n    \"\"\"\n    A function that performs the hyperbolic tangent transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* hyp: the transformed/activated version of the array\n    \"\"\"\n    \n    hyp = np.tanh(z)\n    return hyp\n\n\ndef ReLu(z):\n    \"\"\"\n    A function that performs the hyperbolic tangent transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* points: the transformed/activated version of the array\n    \"\"\"\n    \n    points = np.where(z < 0, 0, z)\n    return points\n\nz = np.linspace(-10,10)\nfa = plt.figure(figsize=(16,5))\n\nplt.subplot(1,3,1)\nplt.plot(z,Sigmoid(z),color=\"red\", label=r'$\\frac{1}{1 + e^{-z}}$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Sigmoid Activation Function\")\nplt.legend(loc='best',fontsize = 22)\n\n\nplt.subplot(1,3,2)\nplt.plot(z,Tanh(z),color=\"red\", label=r'$\\tanh (z)$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Hyperbolic Tangent Activation Function\")\nplt.legend(loc='best',fontsize = 18)\n\nplt.subplot(1,3,3)\nplt.plot(z,ReLu(z),color=\"red\", label=r'$\\max(0,z)$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Rectified Linear Unit Activation Function\")\nplt.legend(loc='best', fontsize = 18)\n\ntf.random.set_seed(1000)  ## For reproducible results\nnetwork = tf.keras.models.Sequential() # Specify layers in their sequential order\n# inputs are 4 dimensions (4 dimensions = 4 features)\n# Dense = Fully Connected.   \n# First hidden layer has 1000 neurons with relu activations.\n# Second hidden layer has 512 neurons with relu activations\n# Third hidden layer has 256 neurons with Sigmoid activations\nnetwork.add(tf.keras.layers.Dense(1000, activation='relu' ,input_shape=(train_X.shape[1],)))\nnetwork.add(tf.keras.layers.Dense(512, activation='relu')) # sigmoid, tanh\nnetwork.add(tf.keras.layers.Dense(256, activation='sigmoid'))\n# Output layer uses no activation with 1 output neurons\nnetwork.add(tf.keras.layers.Dense(1)) # Output layer\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nnetwork.compile(optimizer = opt, loss='mean_squared_error')\n\nnetwork.summary()\n\n# NOTE: if you have time, consider upping epochs -> 150\nhistory =network.fit(train_X, train_y, epochs=20, validation_split = 0.2, verbose=0)\n\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('Error')\nplt.legend()\nplt.grid(True)\n\n# Prediction\n\n## Linear Regression\n\nyhat_linReg = linear_regression.predict(test_X)\ninv_yhat_linReg = np.concatenate((test_X, yhat_linReg), axis=1)\ninv_yhat_linReg = scaler.inverse_transform(inv_yhat_linReg)\ninv_yhat_linReg = inv_yhat_linReg[:,-1]\n\n## DNN\nyhat_dnn = network.predict(test_X) \ninv_yhat_dnn = np.concatenate((test_X, yhat_dnn), axis=1)\ninv_yhat_dnn = scaler.inverse_transform(inv_yhat_dnn)\ninv_yhat_dnn = inv_yhat_dnn[:,-1]\n\n## True Snow Depth (Test Set)\ninv_y = test_dataset[\"snow_depth\"]\n\n\n## Put Observed and Predicted (Linear Regression and DNN) in a Dataframe\nprediction_df = pd.DataFrame({\"Observed\": inv_y,\n                    \"LR\":inv_yhat_linReg, \"DNN\":inv_yhat_dnn})\n\n# check performance\n\ndef metrics_print(test_data,test_predict):\n    print('Test RMSE: ', round(np.sqrt(mean_squared_error(test_data, test_predict)), 2))\n    print('Test R^2 : ', round((r2_score(test_data, test_predict)*100), 2) ,\"%\")\n    print('Test MAPE: ', round(mean_absolute_percentage_error(test_data, test_predict)*100,2), '%')\n    \nprint(\"##************** Linear Regression Results **************##\")\nmetrics_print(prediction_df['Observed'], prediction_df['LR'])\nprint(\" \")\nprint(\" \")\n\nprint(\"##************** Deep Learning Results **************##\")\nmetrics_print(prediction_df['Observed'], prediction_df['DNN'])\nprint(\" \")\nprint(\" \")\n\n# visualize performance\n\nfa = plt.figure(figsize=(16,5))\nplt.subplot(1,2,1)\nplt.scatter(prediction_df['Observed'],prediction_df['LR'])\nplt.xlabel('True Values [snow_depth]', fontsize=15)\nplt.ylabel('Predictions [snow_depth]', fontsize=15)\nplt.title(\"Linear Regression\")\n\n\nplt.subplot(1,2,2)\nplt.scatter(prediction_df['Observed'],prediction_df['DNN'])\nplt.xlabel('True Values [snow_depth]', fontsize=15)\nplt.ylabel('Predictions [snow_depth]', fontsize=15)\nplt.title(\"Deep Neural Network\")\n\n# visualize error\n\nLR_error = prediction_df['Observed'] - prediction_df['LR']\nDNN_error = prediction_df['Observed'] - prediction_df['DNN']\n\nfa = plt.figure(figsize=(16,5))\n\nplt.subplot(1,2,1)\nLR_error.hist()\nplt.xlabel('Error', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.title(\"Linear Regression\")\n\nplt.subplot(1,2,2)\nDNN_error.hist()\nplt.xlabel('Error', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.title(\"Deep Neural Network\")\n\n# save best model\n\nnetwork.save('DNN')\n\n## To load model, use;\nmodel = tf.keras.models.load_model('DNN')\n\n\n\n\n",
  "history_output" : "  File \"snowex_ml_experiment.py\", line 12\n    datadir = f\"{homedir}/Documents/data/\"\n                                         ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1642607500471,
  "history_end_time" : 1642607500633,
  "history_notes" : null,
  "history_process" : "7nyo77",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "46rtqkig5ws",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642607465555,
  "history_end_time" : 1642607465612,
  "history_notes" : null,
  "history_process" : "7nyo77",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "hjm1ust6awa",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642606603061,
  "history_end_time" : 1642606603321,
  "history_notes" : null,
  "history_process" : "7nyo77",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "nlf04oq8ad4",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642606580439,
  "history_end_time" : 1642606580522,
  "history_notes" : null,
  "history_process" : "7nyo77",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "chvsquw9cfg",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642606525791,
  "history_end_time" : 1642606525897,
  "history_notes" : null,
  "history_process" : "7nyo77",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "dseaznw7k5u",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642606503154,
  "history_end_time" : 1642606503311,
  "history_notes" : null,
  "history_process" : "7nyo77",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "38airecag9x",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642606455753,
  "history_end_time" : 1642606455800,
  "history_notes" : null,
  "history_process" : "7nyo77",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "nkcq9ubzupz",
  "history_input" : "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\nimport requests\nimport os\nimport tensorflow as tf\n\nhomedir = os.path.expanduser('~')\ndatadir = f\"{homedir}/Documents/data/\"\n\nexampledataurl = \"https://raw.githubusercontent.com/snowex-hackweek/website/main/book/tutorials/machine-learning/data/snow_depth_data.csv\"\n\nr = requests.get(exampledataurl, allow_redirects=True)\n\nopen(f\"{datadir}snow_depth_data.csv\", 'wb').write(r.content)\n\n# retrieve the example training data first\ndataset = pd.read_csv(\"data/snow_depth_data.csv\")\n\nprint(dataset.info())\n\nprint(dataset.head())\n\ntrain_dataset = dataset.sample(frac=0.8, random_state=0)\ntest_dataset = dataset.drop(train_dataset.index)\n\ntrain_dataset.describe().transpose()\n\nscaler = MinMaxScaler()\nscaled_train = scaler.fit_transform(train_dataset)\nscaled_test = scaler.transform(test_dataset) ## fit_transform != transform. \n## transform uses the parameters of fit_transform\n\n  \n# Sepatare Features from Labels\ntrain_X, train_y = scaled_train[:, :-1], scaled_train[:, -1]\ntest_X, test_y = scaled_test[:, :-1], scaled_test[:, -1]\n\nprint(\"TensorFlow version ==>\", tf.__version__) \nprint(\"Keras version ==>\",tf.keras.__version__)\n\ntf.random.set_seed(0) ## For reproducible results\nlinear_regression = tf.keras.models.Sequential() # Specify layers in their sequential order\n# inputs are 4 dimensions (4 dimensions = 4 features)\n# Dense = Fully Connected.  \nlinear_regression.add(tf.keras.layers.Dense(1, activation=None ,input_shape=(train_X.shape[1],)))\n# Output layer has no activation with just 1 node\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nlinear_regression.compile(optimizer = opt, loss='mean_squared_error')\n\nprint(linear_regression.summary())\n\n# NOTE: can changed from epochs=150 to run faster, change to verbose=1 for per-epoch output\nhistory =linear_regression.fit(train_X, train_y, epochs=100, validation_split = 0.2, verbose=0)\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.grid(True)\n\n# linear regression\n\nlinear_regression.get_weights()\n\ndef Sigmoid(z):\n    \"\"\"\n    A function that performs the sigmoid transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* logistic: the transformed/activated version of the array\n    \"\"\"\n    \n    logistic = 1/(1+ np.exp(-z))\n    return logistic\n    \n\ndef Tanh(z):\n    \"\"\"\n    A function that performs the hyperbolic tangent transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* hyp: the transformed/activated version of the array\n    \"\"\"\n    \n    hyp = np.tanh(z)\n    return hyp\n\n\ndef ReLu(z):\n    \"\"\"\n    A function that performs the hyperbolic tangent transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* points: the transformed/activated version of the array\n    \"\"\"\n    \n    points = np.where(z < 0, 0, z)\n    return points\n\nz = np.linspace(-10,10)\nfa = plt.figure(figsize=(16,5))\n\nplt.subplot(1,3,1)\nplt.plot(z,Sigmoid(z),color=\"red\", label=r'$\\frac{1}{1 + e^{-z}}$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Sigmoid Activation Function\")\nplt.legend(loc='best',fontsize = 22)\n\n\nplt.subplot(1,3,2)\nplt.plot(z,Tanh(z),color=\"red\", label=r'$\\tanh (z)$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Hyperbolic Tangent Activation Function\")\nplt.legend(loc='best',fontsize = 18)\n\nplt.subplot(1,3,3)\nplt.plot(z,ReLu(z),color=\"red\", label=r'$\\max(0,z)$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Rectified Linear Unit Activation Function\")\nplt.legend(loc='best', fontsize = 18)\n\ntf.random.set_seed(1000)  ## For reproducible results\nnetwork = tf.keras.models.Sequential() # Specify layers in their sequential order\n# inputs are 4 dimensions (4 dimensions = 4 features)\n# Dense = Fully Connected.   \n# First hidden layer has 1000 neurons with relu activations.\n# Second hidden layer has 512 neurons with relu activations\n# Third hidden layer has 256 neurons with Sigmoid activations\nnetwork.add(tf.keras.layers.Dense(1000, activation='relu' ,input_shape=(train_X.shape[1],)))\nnetwork.add(tf.keras.layers.Dense(512, activation='relu')) # sigmoid, tanh\nnetwork.add(tf.keras.layers.Dense(256, activation='sigmoid'))\n# Output layer uses no activation with 1 output neurons\nnetwork.add(tf.keras.layers.Dense(1)) # Output layer\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nnetwork.compile(optimizer = opt, loss='mean_squared_error')\n\nnetwork.summary()\n\n# NOTE: if you have time, consider upping epochs -> 150\nhistory =network.fit(train_X, train_y, epochs=20, validation_split = 0.2, verbose=0)\n\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('Error')\nplt.legend()\nplt.grid(True)\n\n# Prediction\n\n## Linear Regression\n\nyhat_linReg = linear_regression.predict(test_X)\ninv_yhat_linReg = np.concatenate((test_X, yhat_linReg), axis=1)\ninv_yhat_linReg = scaler.inverse_transform(inv_yhat_linReg)\ninv_yhat_linReg = inv_yhat_linReg[:,-1]\n\n## DNN\nyhat_dnn = network.predict(test_X) \ninv_yhat_dnn = np.concatenate((test_X, yhat_dnn), axis=1)\ninv_yhat_dnn = scaler.inverse_transform(inv_yhat_dnn)\ninv_yhat_dnn = inv_yhat_dnn[:,-1]\n\n## True Snow Depth (Test Set)\ninv_y = test_dataset[\"snow_depth\"]\n\n\n## Put Observed and Predicted (Linear Regression and DNN) in a Dataframe\nprediction_df = pd.DataFrame({\"Observed\": inv_y,\n                    \"LR\":inv_yhat_linReg, \"DNN\":inv_yhat_dnn})\n\n# check performance\n\ndef metrics_print(test_data,test_predict):\n    print('Test RMSE: ', round(np.sqrt(mean_squared_error(test_data, test_predict)), 2))\n    print('Test R^2 : ', round((r2_score(test_data, test_predict)*100), 2) ,\"%\")\n    print('Test MAPE: ', round(mean_absolute_percentage_error(test_data, test_predict)*100,2), '%')\n    \nprint(\"##************** Linear Regression Results **************##\")\nmetrics_print(prediction_df['Observed'], prediction_df['LR'])\nprint(\" \")\nprint(\" \")\n\nprint(\"##************** Deep Learning Results **************##\")\nmetrics_print(prediction_df['Observed'], prediction_df['DNN'])\nprint(\" \")\nprint(\" \")\n\n# visualize performance\n\nfa = plt.figure(figsize=(16,5))\nplt.subplot(1,2,1)\nplt.scatter(prediction_df['Observed'],prediction_df['LR'])\nplt.xlabel('True Values [snow_depth]', fontsize=15)\nplt.ylabel('Predictions [snow_depth]', fontsize=15)\nplt.title(\"Linear Regression\")\n\n\nplt.subplot(1,2,2)\nplt.scatter(prediction_df['Observed'],prediction_df['DNN'])\nplt.xlabel('True Values [snow_depth]', fontsize=15)\nplt.ylabel('Predictions [snow_depth]', fontsize=15)\nplt.title(\"Deep Neural Network\")\n\n# visualize error\n\nLR_error = prediction_df['Observed'] - prediction_df['LR']\nDNN_error = prediction_df['Observed'] - prediction_df['DNN']\n\nfa = plt.figure(figsize=(16,5))\n\nplt.subplot(1,2,1)\nLR_error.hist()\nplt.xlabel('Error', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.title(\"Linear Regression\")\n\nplt.subplot(1,2,2)\nDNN_error.hist()\nplt.xlabel('Error', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.title(\"Deep Neural Network\")\n\n# save best model\n\nnetwork.save('DNN')\n\n## To load model, use;\nmodel = tf.keras.models.load_model('DNN')\n\n\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1642584180102,
  "history_end_time" : 1642584182124,
  "history_notes" : "not working, tensorflow doesn't work on m1",
  "history_process" : "7nyo77",
  "host_id" : "100001",
  "indicator" : "Done"
},]
