[{
  "history_id" : "gC0cOYRCZooJ",
  "history_input" : "#!/usr/bin/env python\n# ----------------------------------------------------------------------------\n# NSIDC Data Download Script\n#\n# Copyright (c) 2021 Regents of the University of Colorado\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# Tested in Python 2.7 and Python 3.4, 3.6, 3.7\n#\n# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n#   $ python nsidc-data-download.py\n#\n# On Windows, open Start menu -> Run and type cmd. Then type:\n#     python nsidc-data-download.py\n#\n# The script will first search Earthdata for all matching files.\n# You will then be prompted for your Earthdata username/password\n# and the script will download the matching files.\n#\n# If you wish, you may store your Earthdata username/password in a .netrc\n# file in your $HOME directory and the script will automatically attempt to\n# read this file. The .netrc file should have the following format:\n#    machine urs.earthdata.nasa.gov login myusername password mypassword\n# where 'myusername' and 'mypassword' are your Earthdata credentials.\n#\nfrom __future__ import print_function\n\nimport base64\nimport getopt\nimport itertools\nimport json\nimport math\nimport netrc\nimport os.path\nimport ssl\nimport sys\nimport time\nfrom getpass import getpass\nimport os\nimport re\nimport shutil\n\ntry:\n    from urllib.parse import urlparse\n    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n    from urllib.error import HTTPError, URLError\nexcept ImportError:\n    from urlparse import urlparse\n    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n\nshort_name = 'SNEX20_SD'\nversion = '1'\ntime_start = '2020-01-28T00:00:00Z'\ntime_end = '2020-02-12T23:59:59Z'\nbounding_box = ''\npolygon = ''\nfilename_filter = ''\nurl_list = []\n\nCMR_URL = 'https://cmr.earthdata.nasa.gov'\nURS_URL = 'https://urs.earthdata.nasa.gov'\nCMR_PAGE_SIZE = 2000\nCMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n\nhomedir = os.path.expanduser('~') \ndatadir = f\"{homedir}/snowx/\"\n\n\ndef get_username():\n    username = ''\n\n    # For Python 2/3 compatibility:\n    try:\n        do_input = raw_input  # noqa\n    except NameError:\n        do_input = input\n\n    while not username:\n        username = \"uhhmed\"\n    return username\n\n\ndef get_password():\n    password = ''\n    while not password:\n        password = \"Ahmad1583-%\"\n    return password\n\n\ndef get_credentials(url):\n    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n    credentials = None\n    errprefix = ''\n    try:\n        info = netrc.netrc()\n        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n        errprefix = 'netrc error: '\n    except Exception as e:\n        if (not ('No such file' in str(e))):\n            print('netrc error: {0}'.format(str(e)))\n        username = None\n        password = None\n\n    while not credentials:\n        if not username:\n            username = get_username()\n            password = get_password()\n        credentials = '{0}:{1}'.format(username, password)\n        credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n\n        if url:\n            try:\n                req = Request(url)\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n                opener = build_opener(HTTPCookieProcessor())\n                opener.open(req)\n            except HTTPError:\n                print(errprefix + 'Incorrect username or password')\n                errprefix = ''\n                credentials = None\n                username = None\n                password = None\n\n    return credentials\n\n\ndef build_version_query_params(version):\n    desired_pad_length = 3\n    if len(version) > desired_pad_length:\n        print('Version string too long: \"{0}\"'.format(version))\n        quit()\n\n    version = str(int(version))  # Strip off any leading zeros\n    query_params = ''\n\n    while len(version) <= desired_pad_length:\n        padded_version = version.zfill(desired_pad_length)\n        query_params += '&version={0}'.format(padded_version)\n        desired_pad_length -= 1\n    return query_params\n\n\ndef filter_add_wildcards(filter):\n    if not filter.startswith('*'):\n        filter = '*' + filter\n    if not filter.endswith('*'):\n        filter = filter + '*'\n    return filter\n\n\ndef build_filename_filter(filename_filter):\n    filters = filename_filter.split(',')\n    result = '&options[producer_granule_id][pattern]=true'\n    for filter in filters:\n        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n    return result\n\n\ndef build_cmr_query_url(short_name, version, time_start, time_end,\n                        bounding_box=None, polygon=None,\n                        filename_filter=None):\n    params = '&short_name={0}'.format(short_name)\n    params += build_version_query_params(version)\n    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n    if polygon:\n        params += '&polygon={0}'.format(polygon)\n    elif bounding_box:\n        params += '&bounding_box={0}'.format(bounding_box)\n    if filename_filter:\n        params += build_filename_filter(filename_filter)\n    return CMR_FILE_URL + params\n\n\ndef get_speed(time_elapsed, chunk_size):\n    if time_elapsed <= 0:\n        return ''\n    speed = chunk_size / time_elapsed\n    if speed <= 0:\n        speed = 1\n    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    i = int(math.floor(math.log(speed, 1000)))\n    p = math.pow(1000, i)\n    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n\n\ndef output_progress(count, total, status='', bar_len=60):\n    if total <= 0:\n        return\n    fraction = min(max(count / float(total), 0), 1)\n    filled_len = int(round(bar_len * fraction))\n    percents = int(round(100.0 * fraction))\n    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n    sys.stdout.write(fmt)\n    sys.stdout.flush()\n\n\ndef cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n    while True:\n        data = file_object.read(chunk_size)\n        if not data:\n            break\n        yield data\n\n\ndef cmr_download(urls, force=False, quiet=False):\n    \"\"\"Download files from list of urls.\"\"\"\n    if not urls:\n        return\n\n    url_count = len(urls)\n    if not quiet:\n        print('Downloading {0} files...'.format(url_count))\n    credentials = None\n\n    for index, url in enumerate(urls, start=1):\n        if not credentials and urlparse(url).scheme == 'https':\n            credentials = get_credentials(url)\n\n        filename = url.split('/')[-1]\n        if not quiet:\n            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n                                        url_count, filename))\n\n        try:\n            req = Request(url)\n            if credentials:\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n            opener = build_opener(HTTPCookieProcessor())\n            response = opener.open(req)\n            length = int(response.headers['content-length'])\n            try:\n                if not force and length == os.path.getsize(filename):\n                    if not quiet:\n                        print('  File exists, skipping')\n                    continue\n            except OSError:\n                pass\n            count = 0\n            chunk_size = min(max(length, 1), 1024 * 1024)\n            max_chunks = int(math.ceil(length / chunk_size))\n            time_initial = time.time()\n            with open(filename, 'wb') as out_file:\n                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n                    out_file.write(data)\n                    if not quiet:\n                        count = count + 1\n                        time_elapsed = time.time() - time_initial\n                        download_speed = get_speed(time_elapsed, count * chunk_size)\n                        output_progress(count, max_chunks, status=download_speed)\n            if not quiet:\n                print()\n        except HTTPError as e:\n            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n        except URLError as e:\n            print('URL error: {0}'.format(e.reason))\n        except IOError:\n            raise\n\n\ndef cmr_filter_urls(search_results):\n    \"\"\"Select only the desired data files from CMR response.\"\"\"\n    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n        return []\n\n    entries = [e['links']\n               for e in search_results['feed']['entry']\n               if 'links' in e]\n    # Flatten \"entries\" to a simple list of links\n    links = list(itertools.chain(*entries))\n\n    urls = []\n    unique_filenames = set()\n    for link in links:\n        if 'href' not in link:\n            # Exclude links with nothing to download\n            continue\n        if 'inherited' in link and link['inherited'] is True:\n            # Why are we excluding these links?\n            continue\n        if 'rel' in link and 'data#' not in link['rel']:\n            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n            continue\n\n        if 'title' in link and 'opendap' in link['title'].lower():\n            # Exclude OPeNDAP links--they are responsible for many duplicates\n            # This is a hack; when the metadata is updated to properly identify\n            # non-datapool links, we should be able to do this in a non-hack way\n            continue\n\n        filename = link['href'].split('/')[-1]\n        if filename in unique_filenames:\n            # Exclude links with duplicate filenames (they would overwrite)\n            continue\n        unique_filenames.add(filename)\n\n        urls.append(link['href'])\n\n    return urls\n\n\ndef cmr_search(short_name, version, time_start, time_end,\n               bounding_box='', polygon='', filename_filter='', quiet=False):\n    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n                                        time_start=time_start, time_end=time_end,\n                                        bounding_box=bounding_box,\n                                        polygon=polygon, filename_filter=filename_filter)\n    if not quiet:\n        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n\n    cmr_scroll_id = None\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n\n    urls = []\n    hits = 0\n    while True:\n        req = Request(cmr_query_url)\n        if cmr_scroll_id:\n            req.add_header('cmr-scroll-id', cmr_scroll_id)\n        response = urlopen(req, context=ctx)\n        if not cmr_scroll_id:\n            # Python 2 and 3 have different case for the http headers\n            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n            cmr_scroll_id = headers['cmr-scroll-id']\n            hits = int(headers['cmr-hits'])\n            if not quiet:\n                if hits > 0:\n                    print('Found {0} matches.'.format(hits))\n                else:\n                    print('Found no matches.')\n        search_page = response.read()\n        search_page = json.loads(search_page.decode('utf-8'))\n        url_scroll_results = cmr_filter_urls(search_page)\n        if not url_scroll_results:\n            break\n        if not quiet and hits > CMR_PAGE_SIZE:\n            print('.', end='')\n            sys.stdout.flush()\n        urls += url_scroll_results\n\n    if not quiet and hits > CMR_PAGE_SIZE:\n        print()\n    return urls\n\n\ndef main(argv=None):\n    global short_name, version, time_start, time_end, bounding_box, \\\n        polygon, filename_filter, url_list\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    force = False\n    quiet = False\n    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n\n    try:\n        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n        for opt, _arg in opts:\n            if opt in ('-f', '--force'):\n                force = True\n            elif opt in ('-q', '--quiet'):\n                quiet = True\n            elif opt in ('-h', '--help'):\n                print(usage)\n                sys.exit(0)\n    except getopt.GetoptError as e:\n        print(e.args[0])\n        print(usage)\n        sys.exit(1)\n\n    # Supply some default search parameters, just for testing purposes.\n    # These are only used if the parameters aren't filled in up above.\n    if 'short_name' in short_name:\n        short_name = 'ATL06'\n        version = '003'\n        time_start = '2018-10-14T00:00:00Z'\n        time_end = '2021-01-08T21:48:13Z'\n        bounding_box = ''\n        polygon = ''\n        filename_filter = '*ATL06_2020111121*'\n        url_list = []\n\n    try:\n        if not url_list:\n            url_list = cmr_search(short_name, version, time_start, time_end,\n                                  bounding_box=bounding_box, polygon=polygon,\n                                  filename_filter=filename_filter, quiet=quiet)\n\n        cmr_download(url_list, force=force, quiet=quiet)\n    except KeyboardInterrupt:\n        quit()\n        \n    try:\n        # move all the downloaded csv files to $datadir\n        for filename in os.listdir('./'):\n            if re.match(r'.*csv', filename) or re.match(r'.*csv', filename):\n                shutil.move(os.path.join('./', filename), f'{datadir}')\n    except ValueError:\n        quit()\n            \n    # and so forth\n\n\nif __name__ == '__main__':\n    main()",
  "history_output" : "Querying for data:\n\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=SNEX20_SD&version=001&version=01&version=1&temporal[]=2020-01-28T00:00:00Z,2020-02-12T23:59:59Z\nFound 1 matches.\nDownloading 2 files...\nnetrc error: ~/.netrc access too permissive: access permissions must restrict access to only the owner (/Users/uhhmed/.netrc, line 1)\n1/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [===============                                             ]  25%  3.6MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [==============================                              ]  50%  4.4MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [=============================================               ]  75%  4.0MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [============================================================] 100%  5.3MB/s   \n2/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv.xml\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [============================================================] 100%  2.7MB/s   \n",
  "history_begin_time" : 1657492874192,
  "history_end_time" : 1657492879873,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "HAH5UPTphfBE",
  "history_input" : "#!/usr/bin/env python\n# ----------------------------------------------------------------------------\n# NSIDC Data Download Script\n#\n# Copyright (c) 2021 Regents of the University of Colorado\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# Tested in Python 2.7 and Python 3.4, 3.6, 3.7\n#\n# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n#   $ python nsidc-data-download.py\n#\n# On Windows, open Start menu -> Run and type cmd. Then type:\n#     python nsidc-data-download.py\n#\n# The script will first search Earthdata for all matching files.\n# You will then be prompted for your Earthdata username/password\n# and the script will download the matching files.\n#\n# If you wish, you may store your Earthdata username/password in a .netrc\n# file in your $HOME directory and the script will automatically attempt to\n# read this file. The .netrc file should have the following format:\n#    machine urs.earthdata.nasa.gov login myusername password mypassword\n# where 'myusername' and 'mypassword' are your Earthdata credentials.\n#\nfrom __future__ import print_function\n\nimport base64\nimport getopt\nimport itertools\nimport json\nimport math\nimport netrc\nimport os.path\nimport ssl\nimport sys\nimport time\nfrom getpass import getpass\nimport os\nimport re\nimport shutil\n\ntry:\n    from urllib.parse import urlparse\n    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n    from urllib.error import HTTPError, URLError\nexcept ImportError:\n    from urlparse import urlparse\n    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n\nshort_name = 'SNEX20_SD'\nversion = '1'\ntime_start = '2020-01-28T00:00:00Z'\ntime_end = '2020-02-12T23:59:59Z'\nbounding_box = ''\npolygon = ''\nfilename_filter = ''\nurl_list = []\n\nCMR_URL = 'https://cmr.earthdata.nasa.gov'\nURS_URL = 'https://urs.earthdata.nasa.gov'\nCMR_PAGE_SIZE = 2000\nCMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n\nhomedir = os.path.expanduser('~') \ndatadir = f\"{homedir}/Documents/data/\"\n\n\ndef get_username():\n    username = ''\n\n    # For Python 2/3 compatibility:\n    try:\n        do_input = raw_input  # noqa\n    except NameError:\n        do_input = input\n\n    while not username:\n        username = \"uhhmed\"\n    return username\n\n\ndef get_password():\n    password = ''\n    while not password:\n        password = \"Ahmad1583-%\"\n    return password\n\n\ndef get_credentials(url):\n    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n    credentials = None\n    errprefix = ''\n    try:\n        info = netrc.netrc()\n        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n        errprefix = 'netrc error: '\n    except Exception as e:\n        if (not ('No such file' in str(e))):\n            print('netrc error: {0}'.format(str(e)))\n        username = None\n        password = None\n\n    while not credentials:\n        if not username:\n            username = get_username()\n            password = get_password()\n        credentials = '{0}:{1}'.format(username, password)\n        credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n\n        if url:\n            try:\n                req = Request(url)\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n                opener = build_opener(HTTPCookieProcessor())\n                opener.open(req)\n            except HTTPError:\n                print(errprefix + 'Incorrect username or password')\n                errprefix = ''\n                credentials = None\n                username = None\n                password = None\n\n    return credentials\n\n\ndef build_version_query_params(version):\n    desired_pad_length = 3\n    if len(version) > desired_pad_length:\n        print('Version string too long: \"{0}\"'.format(version))\n        quit()\n\n    version = str(int(version))  # Strip off any leading zeros\n    query_params = ''\n\n    while len(version) <= desired_pad_length:\n        padded_version = version.zfill(desired_pad_length)\n        query_params += '&version={0}'.format(padded_version)\n        desired_pad_length -= 1\n    return query_params\n\n\ndef filter_add_wildcards(filter):\n    if not filter.startswith('*'):\n        filter = '*' + filter\n    if not filter.endswith('*'):\n        filter = filter + '*'\n    return filter\n\n\ndef build_filename_filter(filename_filter):\n    filters = filename_filter.split(',')\n    result = '&options[producer_granule_id][pattern]=true'\n    for filter in filters:\n        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n    return result\n\n\ndef build_cmr_query_url(short_name, version, time_start, time_end,\n                        bounding_box=None, polygon=None,\n                        filename_filter=None):\n    params = '&short_name={0}'.format(short_name)\n    params += build_version_query_params(version)\n    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n    if polygon:\n        params += '&polygon={0}'.format(polygon)\n    elif bounding_box:\n        params += '&bounding_box={0}'.format(bounding_box)\n    if filename_filter:\n        params += build_filename_filter(filename_filter)\n    return CMR_FILE_URL + params\n\n\ndef get_speed(time_elapsed, chunk_size):\n    if time_elapsed <= 0:\n        return ''\n    speed = chunk_size / time_elapsed\n    if speed <= 0:\n        speed = 1\n    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    i = int(math.floor(math.log(speed, 1000)))\n    p = math.pow(1000, i)\n    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n\n\ndef output_progress(count, total, status='', bar_len=60):\n    if total <= 0:\n        return\n    fraction = min(max(count / float(total), 0), 1)\n    filled_len = int(round(bar_len * fraction))\n    percents = int(round(100.0 * fraction))\n    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n    sys.stdout.write(fmt)\n    sys.stdout.flush()\n\n\ndef cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n    while True:\n        data = file_object.read(chunk_size)\n        if not data:\n            break\n        yield data\n\n\ndef cmr_download(urls, force=False, quiet=False):\n    \"\"\"Download files from list of urls.\"\"\"\n    if not urls:\n        return\n\n    url_count = len(urls)\n    if not quiet:\n        print('Downloading {0} files...'.format(url_count))\n    credentials = None\n\n    for index, url in enumerate(urls, start=1):\n        if not credentials and urlparse(url).scheme == 'https':\n            credentials = get_credentials(url)\n\n        filename = url.split('/')[-1]\n        if not quiet:\n            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n                                        url_count, filename))\n\n        try:\n            req = Request(url)\n            if credentials:\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n            opener = build_opener(HTTPCookieProcessor())\n            response = opener.open(req)\n            length = int(response.headers['content-length'])\n            try:\n                if not force and length == os.path.getsize(filename):\n                    if not quiet:\n                        print('  File exists, skipping')\n                    continue\n            except OSError:\n                pass\n            count = 0\n            chunk_size = min(max(length, 1), 1024 * 1024)\n            max_chunks = int(math.ceil(length / chunk_size))\n            time_initial = time.time()\n            with open(filename, 'wb') as out_file:\n                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n                    out_file.write(data)\n                    if not quiet:\n                        count = count + 1\n                        time_elapsed = time.time() - time_initial\n                        download_speed = get_speed(time_elapsed, count * chunk_size)\n                        output_progress(count, max_chunks, status=download_speed)\n            if not quiet:\n                print()\n        except HTTPError as e:\n            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n        except URLError as e:\n            print('URL error: {0}'.format(e.reason))\n        except IOError:\n            raise\n\n\ndef cmr_filter_urls(search_results):\n    \"\"\"Select only the desired data files from CMR response.\"\"\"\n    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n        return []\n\n    entries = [e['links']\n               for e in search_results['feed']['entry']\n               if 'links' in e]\n    # Flatten \"entries\" to a simple list of links\n    links = list(itertools.chain(*entries))\n\n    urls = []\n    unique_filenames = set()\n    for link in links:\n        if 'href' not in link:\n            # Exclude links with nothing to download\n            continue\n        if 'inherited' in link and link['inherited'] is True:\n            # Why are we excluding these links?\n            continue\n        if 'rel' in link and 'data#' not in link['rel']:\n            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n            continue\n\n        if 'title' in link and 'opendap' in link['title'].lower():\n            # Exclude OPeNDAP links--they are responsible for many duplicates\n            # This is a hack; when the metadata is updated to properly identify\n            # non-datapool links, we should be able to do this in a non-hack way\n            continue\n\n        filename = link['href'].split('/')[-1]\n        if filename in unique_filenames:\n            # Exclude links with duplicate filenames (they would overwrite)\n            continue\n        unique_filenames.add(filename)\n\n        urls.append(link['href'])\n\n    return urls\n\n\ndef cmr_search(short_name, version, time_start, time_end,\n               bounding_box='', polygon='', filename_filter='', quiet=False):\n    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n                                        time_start=time_start, time_end=time_end,\n                                        bounding_box=bounding_box,\n                                        polygon=polygon, filename_filter=filename_filter)\n    if not quiet:\n        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n\n    cmr_scroll_id = None\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n\n    urls = []\n    hits = 0\n    while True:\n        req = Request(cmr_query_url)\n        if cmr_scroll_id:\n            req.add_header('cmr-scroll-id', cmr_scroll_id)\n        response = urlopen(req, context=ctx)\n        if not cmr_scroll_id:\n            # Python 2 and 3 have different case for the http headers\n            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n            cmr_scroll_id = headers['cmr-scroll-id']\n            hits = int(headers['cmr-hits'])\n            if not quiet:\n                if hits > 0:\n                    print('Found {0} matches.'.format(hits))\n                else:\n                    print('Found no matches.')\n        search_page = response.read()\n        search_page = json.loads(search_page.decode('utf-8'))\n        url_scroll_results = cmr_filter_urls(search_page)\n        if not url_scroll_results:\n            break\n        if not quiet and hits > CMR_PAGE_SIZE:\n            print('.', end='')\n            sys.stdout.flush()\n        urls += url_scroll_results\n\n    if not quiet and hits > CMR_PAGE_SIZE:\n        print()\n    return urls\n\n\ndef main(argv=None):\n    global short_name, version, time_start, time_end, bounding_box, \\\n        polygon, filename_filter, url_list\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    force = False\n    quiet = False\n    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n\n    try:\n        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n        for opt, _arg in opts:\n            if opt in ('-f', '--force'):\n                force = True\n            elif opt in ('-q', '--quiet'):\n                quiet = True\n            elif opt in ('-h', '--help'):\n                print(usage)\n                sys.exit(0)\n    except getopt.GetoptError as e:\n        print(e.args[0])\n        print(usage)\n        sys.exit(1)\n\n    # Supply some default search parameters, just for testing purposes.\n    # These are only used if the parameters aren't filled in up above.\n    if 'short_name' in short_name:\n        short_name = 'ATL06'\n        version = '003'\n        time_start = '2018-10-14T00:00:00Z'\n        time_end = '2021-01-08T21:48:13Z'\n        bounding_box = ''\n        polygon = ''\n        filename_filter = '*ATL06_2020111121*'\n        url_list = []\n\n    try:\n        if not url_list:\n            url_list = cmr_search(short_name, version, time_start, time_end,\n                                  bounding_box=bounding_box, polygon=polygon,\n                                  filename_filter=filename_filter, quiet=quiet)\n\n        cmr_download(url_list, force=force, quiet=quiet)\n    except KeyboardInterrupt:\n        quit()\n        \n    try:\n        # move all the downloaded csv files to $datadir\n        for filename in os.listdir('./'):\n            if re.match(r'.*csv', filename) or re.match(r'.*csv', filename):\n                shutil.move(os.path.join('./', filename), f'{datadir}')\n    except ValueError:\n        quit()\n            \n    # and so forth\n\n\nif __name__ == '__main__':\n    main()",
  "history_output" : "Querying for data:\n\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=SNEX20_SD&version=001&version=01&version=1&temporal[]=2020-01-28T00:00:00Z,2020-02-12T23:59:59Z\nFound 1 matches.\nDownloading 2 files...\nnetrc error: ~/.netrc access too permissive: access permissions must restrict access to only the owner (/Users/uhhmed/.netrc, line 1)\n1/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [===============                                             ]  25%  4.5MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [==============================                              ]  50%  5.1MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [=============================================               ]  75%  4.8MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [============================================================] 100%  6.4MB/s   \n2/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv.xml\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [============================================================] 100%  1.6MB/s   \nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.8/shutil.py\", line 791, in move\n    os.rename(src, real_dst)\nFileNotFoundError: [Errno 2] No such file or directory: './text_to_csv_daily.py' -> '/Users/uhhmed/Documents/data/'\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"snowex_nsidc_download_script.py\", line 420, in <module>\n    main()\n  File \"snowex_nsidc_download_script.py\", line 412, in main\n    shutil.move(os.path.join('./', filename), f'{datadir}')\n  File \"/opt/anaconda3/lib/python3.8/shutil.py\", line 805, in move\n    copy_function(src, real_dst)\n  File \"/opt/anaconda3/lib/python3.8/shutil.py\", line 435, in copy2\n    copyfile(src, dst, follow_symlinks=follow_symlinks)\n  File \"/opt/anaconda3/lib/python3.8/shutil.py\", line 264, in copyfile\n    with open(src, 'rb') as fsrc, open(dst, 'wb') as fdst:\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/uhhmed/Documents/data/'\n",
  "history_begin_time" : 1657492708687,
  "history_end_time" : 1657492714331,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "SpTxtOJJVp0w",
  "history_input" : "#!/usr/bin/env python\n# ----------------------------------------------------------------------------\n# NSIDC Data Download Script\n#\n# Copyright (c) 2021 Regents of the University of Colorado\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# Tested in Python 2.7 and Python 3.4, 3.6, 3.7\n#\n# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n#   $ python nsidc-data-download.py\n#\n# On Windows, open Start menu -> Run and type cmd. Then type:\n#     python nsidc-data-download.py\n#\n# The script will first search Earthdata for all matching files.\n# You will then be prompted for your Earthdata username/password\n# and the script will download the matching files.\n#\n# If you wish, you may store your Earthdata username/password in a .netrc\n# file in your $HOME directory and the script will automatically attempt to\n# read this file. The .netrc file should have the following format:\n#    machine urs.earthdata.nasa.gov login myusername password mypassword\n# where 'myusername' and 'mypassword' are your Earthdata credentials.\n#\nfrom __future__ import print_function\n\nimport base64\nimport getopt\nimport itertools\nimport json\nimport math\nimport netrc\nimport os.path\nimport ssl\nimport sys\nimport time\nfrom getpass import getpass\nimport os\nimport re\nimport shutil\n\ntry:\n    from urllib.parse import urlparse\n    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n    from urllib.error import HTTPError, URLError\nexcept ImportError:\n    from urlparse import urlparse\n    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n\nshort_name = 'SNEX20_SD'\nversion = '1'\ntime_start = '2020-01-28T00:00:00Z'\ntime_end = '2020-02-12T23:59:59Z'\nbounding_box = ''\npolygon = ''\nfilename_filter = ''\nurl_list = []\n\nCMR_URL = 'https://cmr.earthdata.nasa.gov'\nURS_URL = 'https://urs.earthdata.nasa.gov'\nCMR_PAGE_SIZE = 2000\nCMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n\nhomedir = os.path.expanduser('~') \ndatadir = f\"{homedir}/Documents/data/\"\n\n\ndef get_username():\n    username = ''\n\n    # For Python 2/3 compatibility:\n    try:\n        do_input = raw_input  # noqa\n    except NameError:\n        do_input = input\n\n    while not username:\n        username = do_input('Earthdata username: ')\n    return username\n\n\ndef get_password():\n    password = ''\n    while not password:\n        password = getpass('password: ')\n    return password\n\n\ndef get_credentials(url):\n    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n    credentials = None\n    errprefix = ''\n    try:\n        info = netrc.netrc()\n        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n        errprefix = 'netrc error: '\n    except Exception as e:\n        if (not ('No such file' in str(e))):\n            print('netrc error: {0}'.format(str(e)))\n        username = None\n        password = None\n\n    while not credentials:\n        if not username:\n            username = get_username()\n            password = get_password()\n        credentials = '{0}:{1}'.format(username, password)\n        credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n\n        if url:\n            try:\n                req = Request(url)\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n                opener = build_opener(HTTPCookieProcessor())\n                opener.open(req)\n            except HTTPError:\n                print(errprefix + 'Incorrect username or password')\n                errprefix = ''\n                credentials = None\n                username = None\n                password = None\n\n    return credentials\n\n\ndef build_version_query_params(version):\n    desired_pad_length = 3\n    if len(version) > desired_pad_length:\n        print('Version string too long: \"{0}\"'.format(version))\n        quit()\n\n    version = str(int(version))  # Strip off any leading zeros\n    query_params = ''\n\n    while len(version) <= desired_pad_length:\n        padded_version = version.zfill(desired_pad_length)\n        query_params += '&version={0}'.format(padded_version)\n        desired_pad_length -= 1\n    return query_params\n\n\ndef filter_add_wildcards(filter):\n    if not filter.startswith('*'):\n        filter = '*' + filter\n    if not filter.endswith('*'):\n        filter = filter + '*'\n    return filter\n\n\ndef build_filename_filter(filename_filter):\n    filters = filename_filter.split(',')\n    result = '&options[producer_granule_id][pattern]=true'\n    for filter in filters:\n        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n    return result\n\n\ndef build_cmr_query_url(short_name, version, time_start, time_end,\n                        bounding_box=None, polygon=None,\n                        filename_filter=None):\n    params = '&short_name={0}'.format(short_name)\n    params += build_version_query_params(version)\n    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n    if polygon:\n        params += '&polygon={0}'.format(polygon)\n    elif bounding_box:\n        params += '&bounding_box={0}'.format(bounding_box)\n    if filename_filter:\n        params += build_filename_filter(filename_filter)\n    return CMR_FILE_URL + params\n\n\ndef get_speed(time_elapsed, chunk_size):\n    if time_elapsed <= 0:\n        return ''\n    speed = chunk_size / time_elapsed\n    if speed <= 0:\n        speed = 1\n    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    i = int(math.floor(math.log(speed, 1000)))\n    p = math.pow(1000, i)\n    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n\n\ndef output_progress(count, total, status='', bar_len=60):\n    if total <= 0:\n        return\n    fraction = min(max(count / float(total), 0), 1)\n    filled_len = int(round(bar_len * fraction))\n    percents = int(round(100.0 * fraction))\n    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n    sys.stdout.write(fmt)\n    sys.stdout.flush()\n\n\ndef cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n    while True:\n        data = file_object.read(chunk_size)\n        if not data:\n            break\n        yield data\n\n\ndef cmr_download(urls, force=False, quiet=False):\n    \"\"\"Download files from list of urls.\"\"\"\n    if not urls:\n        return\n\n    url_count = len(urls)\n    if not quiet:\n        print('Downloading {0} files...'.format(url_count))\n    credentials = None\n\n    for index, url in enumerate(urls, start=1):\n        if not credentials and urlparse(url).scheme == 'https':\n            credentials = get_credentials(url)\n\n        filename = url.split('/')[-1]\n        if not quiet:\n            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n                                        url_count, filename))\n\n        try:\n            req = Request(url)\n            if credentials:\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n            opener = build_opener(HTTPCookieProcessor())\n            response = opener.open(req)\n            length = int(response.headers['content-length'])\n            try:\n                if not force and length == os.path.getsize(filename):\n                    if not quiet:\n                        print('  File exists, skipping')\n                    continue\n            except OSError:\n                pass\n            count = 0\n            chunk_size = min(max(length, 1), 1024 * 1024)\n            max_chunks = int(math.ceil(length / chunk_size))\n            time_initial = time.time()\n            with open(filename, 'wb') as out_file:\n                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n                    out_file.write(data)\n                    if not quiet:\n                        count = count + 1\n                        time_elapsed = time.time() - time_initial\n                        download_speed = get_speed(time_elapsed, count * chunk_size)\n                        output_progress(count, max_chunks, status=download_speed)\n            if not quiet:\n                print()\n        except HTTPError as e:\n            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n        except URLError as e:\n            print('URL error: {0}'.format(e.reason))\n        except IOError:\n            raise\n\n\ndef cmr_filter_urls(search_results):\n    \"\"\"Select only the desired data files from CMR response.\"\"\"\n    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n        return []\n\n    entries = [e['links']\n               for e in search_results['feed']['entry']\n               if 'links' in e]\n    # Flatten \"entries\" to a simple list of links\n    links = list(itertools.chain(*entries))\n\n    urls = []\n    unique_filenames = set()\n    for link in links:\n        if 'href' not in link:\n            # Exclude links with nothing to download\n            continue\n        if 'inherited' in link and link['inherited'] is True:\n            # Why are we excluding these links?\n            continue\n        if 'rel' in link and 'data#' not in link['rel']:\n            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n            continue\n\n        if 'title' in link and 'opendap' in link['title'].lower():\n            # Exclude OPeNDAP links--they are responsible for many duplicates\n            # This is a hack; when the metadata is updated to properly identify\n            # non-datapool links, we should be able to do this in a non-hack way\n            continue\n\n        filename = link['href'].split('/')[-1]\n        if filename in unique_filenames:\n            # Exclude links with duplicate filenames (they would overwrite)\n            continue\n        unique_filenames.add(filename)\n\n        urls.append(link['href'])\n\n    return urls\n\n\ndef cmr_search(short_name, version, time_start, time_end,\n               bounding_box='', polygon='', filename_filter='', quiet=False):\n    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n                                        time_start=time_start, time_end=time_end,\n                                        bounding_box=bounding_box,\n                                        polygon=polygon, filename_filter=filename_filter)\n    if not quiet:\n        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n\n    cmr_scroll_id = None\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n\n    urls = []\n    hits = 0\n    while True:\n        req = Request(cmr_query_url)\n        if cmr_scroll_id:\n            req.add_header('cmr-scroll-id', cmr_scroll_id)\n        response = urlopen(req, context=ctx)\n        if not cmr_scroll_id:\n            # Python 2 and 3 have different case for the http headers\n            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n            cmr_scroll_id = headers['cmr-scroll-id']\n            hits = int(headers['cmr-hits'])\n            if not quiet:\n                if hits > 0:\n                    print('Found {0} matches.'.format(hits))\n                else:\n                    print('Found no matches.')\n        search_page = response.read()\n        search_page = json.loads(search_page.decode('utf-8'))\n        url_scroll_results = cmr_filter_urls(search_page)\n        if not url_scroll_results:\n            break\n        if not quiet and hits > CMR_PAGE_SIZE:\n            print('.', end='')\n            sys.stdout.flush()\n        urls += url_scroll_results\n\n    if not quiet and hits > CMR_PAGE_SIZE:\n        print()\n    return urls\n\n\ndef main(argv=None):\n    global short_name, version, time_start, time_end, bounding_box, \\\n        polygon, filename_filter, url_list\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    force = False\n    quiet = False\n    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n\n    try:\n        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n        for opt, _arg in opts:\n            if opt in ('-f', '--force'):\n                force = True\n            elif opt in ('-q', '--quiet'):\n                quiet = True\n            elif opt in ('-h', '--help'):\n                print(usage)\n                sys.exit(0)\n    except getopt.GetoptError as e:\n        print(e.args[0])\n        print(usage)\n        sys.exit(1)\n\n    # Supply some default search parameters, just for testing purposes.\n    # These are only used if the parameters aren't filled in up above.\n    if 'short_name' in short_name:\n        short_name = 'ATL06'\n        version = '003'\n        time_start = '2018-10-14T00:00:00Z'\n        time_end = '2021-01-08T21:48:13Z'\n        bounding_box = ''\n        polygon = ''\n        filename_filter = '*ATL06_2020111121*'\n        url_list = []\n\n    try:\n        if not url_list:\n            url_list = cmr_search(short_name, version, time_start, time_end,\n                                  bounding_box=bounding_box, polygon=polygon,\n                                  filename_filter=filename_filter, quiet=quiet)\n\n        cmr_download(url_list, force=force, quiet=quiet)\n    except KeyboardInterrupt:\n        quit()\n        \n    try:\n        # move all the downloaded csv files to $datadir\n        for filename in os.listdir('./'):\n            if re.match(r'.*csv', filename) or re.match(r'.*csv', filename):\n                shutil.move(os.path.join('./', filename), f'{datadir}')\n    except ValueError:\n        quit()\n            \n    # and so forth\n\n\nif __name__ == '__main__':\n    main()",
  "history_output" : "Querying for data:\n\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=SNEX20_SD&version=001&version=01&version=1&temporal[]=2020-01-28T00:00:00Z,2020-02-12T23:59:59Z\nFound 1 matches.\nDownloading 2 files...\nnetrc error: ~/.netrc access too permissive: access permissions must restrict access to only the owner (/Users/uhhmed/.netrc, line 1)\nEarthdata username: \n\nStream closed",
  "history_begin_time" : 1657490938575,
  "history_end_time" : 1657492667084,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "clgqf5wm2vz",
  "history_input" : "#!/usr/bin/env python\n# ----------------------------------------------------------------------------\n# NSIDC Data Download Script\n#\n# Copyright (c) 2021 Regents of the University of Colorado\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# Tested in Python 2.7 and Python 3.4, 3.6, 3.7\n#\n# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n#   $ python nsidc-data-download.py\n#\n# On Windows, open Start menu -> Run and type cmd. Then type:\n#     python nsidc-data-download.py\n#\n# The script will first search Earthdata for all matching files.\n# You will then be prompted for your Earthdata username/password\n# and the script will download the matching files.\n#\n# If you wish, you may store your Earthdata username/password in a .netrc\n# file in your $HOME directory and the script will automatically attempt to\n# read this file. The .netrc file should have the following format:\n#    machine urs.earthdata.nasa.gov login myusername password mypassword\n# where 'myusername' and 'mypassword' are your Earthdata credentials.\n#\nfrom __future__ import print_function\n\nimport base64\nimport getopt\nimport itertools\nimport json\nimport math\nimport netrc\nimport os.path\nimport ssl\nimport sys\nimport time\nfrom getpass import getpass\nimport os\nimport re\nimport shutil\n\ntry:\n    from urllib.parse import urlparse\n    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n    from urllib.error import HTTPError, URLError\nexcept ImportError:\n    from urlparse import urlparse\n    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n\nshort_name = 'SNEX20_SD'\nversion = '1'\ntime_start = '2020-01-28T00:00:00Z'\ntime_end = '2020-02-12T23:59:59Z'\nbounding_box = ''\npolygon = ''\nfilename_filter = ''\nurl_list = []\n\nCMR_URL = 'https://cmr.earthdata.nasa.gov'\nURS_URL = 'https://urs.earthdata.nasa.gov'\nCMR_PAGE_SIZE = 2000\nCMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n\nhomedir = os.path.expanduser('~') \ndatadir = f\"{homedir}/Documents/data/\"\n\n\ndef get_username():\n    username = ''\n\n    # For Python 2/3 compatibility:\n    try:\n        do_input = raw_input  # noqa\n    except NameError:\n        do_input = input\n\n    while not username:\n        username = do_input('Earthdata username: ')\n    return username\n\n\ndef get_password():\n    password = ''\n    while not password:\n        password = getpass('password: ')\n    return password\n\n\ndef get_credentials(url):\n    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n    credentials = None\n    errprefix = ''\n    try:\n        info = netrc.netrc()\n        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n        errprefix = 'netrc error: '\n    except Exception as e:\n        if (not ('No such file' in str(e))):\n            print('netrc error: {0}'.format(str(e)))\n        username = None\n        password = None\n\n    while not credentials:\n        if not username:\n            username = get_username()\n            password = get_password()\n        credentials = '{0}:{1}'.format(username, password)\n        credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n\n        if url:\n            try:\n                req = Request(url)\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n                opener = build_opener(HTTPCookieProcessor())\n                opener.open(req)\n            except HTTPError:\n                print(errprefix + 'Incorrect username or password')\n                errprefix = ''\n                credentials = None\n                username = None\n                password = None\n\n    return credentials\n\n\ndef build_version_query_params(version):\n    desired_pad_length = 3\n    if len(version) > desired_pad_length:\n        print('Version string too long: \"{0}\"'.format(version))\n        quit()\n\n    version = str(int(version))  # Strip off any leading zeros\n    query_params = ''\n\n    while len(version) <= desired_pad_length:\n        padded_version = version.zfill(desired_pad_length)\n        query_params += '&version={0}'.format(padded_version)\n        desired_pad_length -= 1\n    return query_params\n\n\ndef filter_add_wildcards(filter):\n    if not filter.startswith('*'):\n        filter = '*' + filter\n    if not filter.endswith('*'):\n        filter = filter + '*'\n    return filter\n\n\ndef build_filename_filter(filename_filter):\n    filters = filename_filter.split(',')\n    result = '&options[producer_granule_id][pattern]=true'\n    for filter in filters:\n        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n    return result\n\n\ndef build_cmr_query_url(short_name, version, time_start, time_end,\n                        bounding_box=None, polygon=None,\n                        filename_filter=None):\n    params = '&short_name={0}'.format(short_name)\n    params += build_version_query_params(version)\n    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n    if polygon:\n        params += '&polygon={0}'.format(polygon)\n    elif bounding_box:\n        params += '&bounding_box={0}'.format(bounding_box)\n    if filename_filter:\n        params += build_filename_filter(filename_filter)\n    return CMR_FILE_URL + params\n\n\ndef get_speed(time_elapsed, chunk_size):\n    if time_elapsed <= 0:\n        return ''\n    speed = chunk_size / time_elapsed\n    if speed <= 0:\n        speed = 1\n    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    i = int(math.floor(math.log(speed, 1000)))\n    p = math.pow(1000, i)\n    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n\n\ndef output_progress(count, total, status='', bar_len=60):\n    if total <= 0:\n        return\n    fraction = min(max(count / float(total), 0), 1)\n    filled_len = int(round(bar_len * fraction))\n    percents = int(round(100.0 * fraction))\n    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n    sys.stdout.write(fmt)\n    sys.stdout.flush()\n\n\ndef cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n    while True:\n        data = file_object.read(chunk_size)\n        if not data:\n            break\n        yield data\n\n\ndef cmr_download(urls, force=False, quiet=False):\n    \"\"\"Download files from list of urls.\"\"\"\n    if not urls:\n        return\n\n    url_count = len(urls)\n    if not quiet:\n        print('Downloading {0} files...'.format(url_count))\n    credentials = None\n\n    for index, url in enumerate(urls, start=1):\n        if not credentials and urlparse(url).scheme == 'https':\n            credentials = get_credentials(url)\n\n        filename = url.split('/')[-1]\n        if not quiet:\n            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n                                        url_count, filename))\n\n        try:\n            req = Request(url)\n            if credentials:\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n            opener = build_opener(HTTPCookieProcessor())\n            response = opener.open(req)\n            length = int(response.headers['content-length'])\n            try:\n                if not force and length == os.path.getsize(filename):\n                    if not quiet:\n                        print('  File exists, skipping')\n                    continue\n            except OSError:\n                pass\n            count = 0\n            chunk_size = min(max(length, 1), 1024 * 1024)\n            max_chunks = int(math.ceil(length / chunk_size))\n            time_initial = time.time()\n            with open(filename, 'wb') as out_file:\n                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n                    out_file.write(data)\n                    if not quiet:\n                        count = count + 1\n                        time_elapsed = time.time() - time_initial\n                        download_speed = get_speed(time_elapsed, count * chunk_size)\n                        output_progress(count, max_chunks, status=download_speed)\n            if not quiet:\n                print()\n        except HTTPError as e:\n            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n        except URLError as e:\n            print('URL error: {0}'.format(e.reason))\n        except IOError:\n            raise\n\n\ndef cmr_filter_urls(search_results):\n    \"\"\"Select only the desired data files from CMR response.\"\"\"\n    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n        return []\n\n    entries = [e['links']\n               for e in search_results['feed']['entry']\n               if 'links' in e]\n    # Flatten \"entries\" to a simple list of links\n    links = list(itertools.chain(*entries))\n\n    urls = []\n    unique_filenames = set()\n    for link in links:\n        if 'href' not in link:\n            # Exclude links with nothing to download\n            continue\n        if 'inherited' in link and link['inherited'] is True:\n            # Why are we excluding these links?\n            continue\n        if 'rel' in link and 'data#' not in link['rel']:\n            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n            continue\n\n        if 'title' in link and 'opendap' in link['title'].lower():\n            # Exclude OPeNDAP links--they are responsible for many duplicates\n            # This is a hack; when the metadata is updated to properly identify\n            # non-datapool links, we should be able to do this in a non-hack way\n            continue\n\n        filename = link['href'].split('/')[-1]\n        if filename in unique_filenames:\n            # Exclude links with duplicate filenames (they would overwrite)\n            continue\n        unique_filenames.add(filename)\n\n        urls.append(link['href'])\n\n    return urls\n\n\ndef cmr_search(short_name, version, time_start, time_end,\n               bounding_box='', polygon='', filename_filter='', quiet=False):\n    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n                                        time_start=time_start, time_end=time_end,\n                                        bounding_box=bounding_box,\n                                        polygon=polygon, filename_filter=filename_filter)\n    if not quiet:\n        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n\n    cmr_scroll_id = None\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n\n    urls = []\n    hits = 0\n    while True:\n        req = Request(cmr_query_url)\n        if cmr_scroll_id:\n            req.add_header('cmr-scroll-id', cmr_scroll_id)\n        response = urlopen(req, context=ctx)\n        if not cmr_scroll_id:\n            # Python 2 and 3 have different case for the http headers\n            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n            cmr_scroll_id = headers['cmr-scroll-id']\n            hits = int(headers['cmr-hits'])\n            if not quiet:\n                if hits > 0:\n                    print('Found {0} matches.'.format(hits))\n                else:\n                    print('Found no matches.')\n        search_page = response.read()\n        search_page = json.loads(search_page.decode('utf-8'))\n        url_scroll_results = cmr_filter_urls(search_page)\n        if not url_scroll_results:\n            break\n        if not quiet and hits > CMR_PAGE_SIZE:\n            print('.', end='')\n            sys.stdout.flush()\n        urls += url_scroll_results\n\n    if not quiet and hits > CMR_PAGE_SIZE:\n        print()\n    return urls\n\n\ndef main(argv=None):\n    global short_name, version, time_start, time_end, bounding_box, \\\n        polygon, filename_filter, url_list\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    force = False\n    quiet = False\n    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n\n    try:\n        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n        for opt, _arg in opts:\n            if opt in ('-f', '--force'):\n                force = True\n            elif opt in ('-q', '--quiet'):\n                quiet = True\n            elif opt in ('-h', '--help'):\n                print(usage)\n                sys.exit(0)\n    except getopt.GetoptError as e:\n        print(e.args[0])\n        print(usage)\n        sys.exit(1)\n\n    # Supply some default search parameters, just for testing purposes.\n    # These are only used if the parameters aren't filled in up above.\n    if 'short_name' in short_name:\n        short_name = 'ATL06'\n        version = '003'\n        time_start = '2018-10-14T00:00:00Z'\n        time_end = '2021-01-08T21:48:13Z'\n        bounding_box = ''\n        polygon = ''\n        filename_filter = '*ATL06_2020111121*'\n        url_list = []\n\n    try:\n        if not url_list:\n            url_list = cmr_search(short_name, version, time_start, time_end,\n                                  bounding_box=bounding_box, polygon=polygon,\n                                  filename_filter=filename_filter, quiet=quiet)\n\n        cmr_download(url_list, force=force, quiet=quiet)\n    except KeyboardInterrupt:\n        quit()\n        \n    try:\n        # move all the downloaded csv files to $datadir\n        for filename in os.listdir('./'):\n            if re.match(r'.*csv', filename) or re.match(r'.*csv', filename):\n                shutil.move(os.path.join('./', filename), f'{datadir}')\n    except ValueError:\n        quit()\n            \n    # and so forth\n\n\nif __name__ == '__main__':\n    main()",
  "history_output" : "Running",
  "history_begin_time" : 1642613645923,
  "history_end_time" : 1656425770254,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lwgvcigfntr",
  "history_input" : "#!/usr/bin/env python\n# ----------------------------------------------------------------------------\n# NSIDC Data Download Script\n#\n# Copyright (c) 2021 Regents of the University of Colorado\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# Tested in Python 2.7 and Python 3.4, 3.6, 3.7\n#\n# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n#   $ python nsidc-data-download.py\n#\n# On Windows, open Start menu -> Run and type cmd. Then type:\n#     python nsidc-data-download.py\n#\n# The script will first search Earthdata for all matching files.\n# You will then be prompted for your Earthdata username/password\n# and the script will download the matching files.\n#\n# If you wish, you may store your Earthdata username/password in a .netrc\n# file in your $HOME directory and the script will automatically attempt to\n# read this file. The .netrc file should have the following format:\n#    machine urs.earthdata.nasa.gov login myusername password mypassword\n# where 'myusername' and 'mypassword' are your Earthdata credentials.\n#\nfrom __future__ import print_function\n\nimport base64\nimport getopt\nimport itertools\nimport json\nimport math\nimport netrc\nimport os.path\nimport ssl\nimport sys\nimport time\nfrom getpass import getpass\nimport os\nimport re\nimport shutil\n\ntry:\n    from urllib.parse import urlparse\n    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n    from urllib.error import HTTPError, URLError\nexcept ImportError:\n    from urlparse import urlparse\n    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n\nshort_name = 'SNEX20_SD'\nversion = '1'\ntime_start = '2020-01-28T00:00:00Z'\ntime_end = '2020-02-12T23:59:59Z'\nbounding_box = ''\npolygon = ''\nfilename_filter = ''\nurl_list = []\n\nCMR_URL = 'https://cmr.earthdata.nasa.gov'\nURS_URL = 'https://urs.earthdata.nasa.gov'\nCMR_PAGE_SIZE = 2000\nCMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n\nhomedir = os.path.expanduser('~') \ndatadir = f\"{homedir}/Documents/data/\"\n\n\ndef get_username():\n    username = ''\n\n    # For Python 2/3 compatibility:\n    try:\n        do_input = raw_input  # noqa\n    except NameError:\n        do_input = input\n\n    while not username:\n        username = do_input('Earthdata username: ')\n    return username\n\n\ndef get_password():\n    password = ''\n    while not password:\n        password = getpass('password: ')\n    return password\n\n\ndef get_credentials(url):\n    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n    credentials = None\n    errprefix = ''\n    try:\n        info = netrc.netrc()\n        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n        errprefix = 'netrc error: '\n    except Exception as e:\n        if (not ('No such file' in str(e))):\n            print('netrc error: {0}'.format(str(e)))\n        username = None\n        password = None\n\n    while not credentials:\n        if not username:\n            username = get_username()\n            password = get_password()\n        credentials = '{0}:{1}'.format(username, password)\n        credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n\n        if url:\n            try:\n                req = Request(url)\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n                opener = build_opener(HTTPCookieProcessor())\n                opener.open(req)\n            except HTTPError:\n                print(errprefix + 'Incorrect username or password')\n                errprefix = ''\n                credentials = None\n                username = None\n                password = None\n\n    return credentials\n\n\ndef build_version_query_params(version):\n    desired_pad_length = 3\n    if len(version) > desired_pad_length:\n        print('Version string too long: \"{0}\"'.format(version))\n        quit()\n\n    version = str(int(version))  # Strip off any leading zeros\n    query_params = ''\n\n    while len(version) <= desired_pad_length:\n        padded_version = version.zfill(desired_pad_length)\n        query_params += '&version={0}'.format(padded_version)\n        desired_pad_length -= 1\n    return query_params\n\n\ndef filter_add_wildcards(filter):\n    if not filter.startswith('*'):\n        filter = '*' + filter\n    if not filter.endswith('*'):\n        filter = filter + '*'\n    return filter\n\n\ndef build_filename_filter(filename_filter):\n    filters = filename_filter.split(',')\n    result = '&options[producer_granule_id][pattern]=true'\n    for filter in filters:\n        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n    return result\n\n\ndef build_cmr_query_url(short_name, version, time_start, time_end,\n                        bounding_box=None, polygon=None,\n                        filename_filter=None):\n    params = '&short_name={0}'.format(short_name)\n    params += build_version_query_params(version)\n    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n    if polygon:\n        params += '&polygon={0}'.format(polygon)\n    elif bounding_box:\n        params += '&bounding_box={0}'.format(bounding_box)\n    if filename_filter:\n        params += build_filename_filter(filename_filter)\n    return CMR_FILE_URL + params\n\n\ndef get_speed(time_elapsed, chunk_size):\n    if time_elapsed <= 0:\n        return ''\n    speed = chunk_size / time_elapsed\n    if speed <= 0:\n        speed = 1\n    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    i = int(math.floor(math.log(speed, 1000)))\n    p = math.pow(1000, i)\n    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n\n\ndef output_progress(count, total, status='', bar_len=60):\n    if total <= 0:\n        return\n    fraction = min(max(count / float(total), 0), 1)\n    filled_len = int(round(bar_len * fraction))\n    percents = int(round(100.0 * fraction))\n    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n    sys.stdout.write(fmt)\n    sys.stdout.flush()\n\n\ndef cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n    while True:\n        data = file_object.read(chunk_size)\n        if not data:\n            break\n        yield data\n\n\ndef cmr_download(urls, force=False, quiet=False):\n    \"\"\"Download files from list of urls.\"\"\"\n    if not urls:\n        return\n\n    url_count = len(urls)\n    if not quiet:\n        print('Downloading {0} files...'.format(url_count))\n    credentials = None\n\n    for index, url in enumerate(urls, start=1):\n        if not credentials and urlparse(url).scheme == 'https':\n            credentials = get_credentials(url)\n\n        filename = url.split('/')[-1]\n        if not quiet:\n            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n                                        url_count, filename))\n\n        try:\n            req = Request(url)\n            if credentials:\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n            opener = build_opener(HTTPCookieProcessor())\n            response = opener.open(req)\n            length = int(response.headers['content-length'])\n            try:\n                if not force and length == os.path.getsize(filename):\n                    if not quiet:\n                        print('  File exists, skipping')\n                    continue\n            except OSError:\n                pass\n            count = 0\n            chunk_size = min(max(length, 1), 1024 * 1024)\n            max_chunks = int(math.ceil(length / chunk_size))\n            time_initial = time.time()\n            with open(filename, 'wb') as out_file:\n                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n                    out_file.write(data)\n                    if not quiet:\n                        count = count + 1\n                        time_elapsed = time.time() - time_initial\n                        download_speed = get_speed(time_elapsed, count * chunk_size)\n                        output_progress(count, max_chunks, status=download_speed)\n            if not quiet:\n                print()\n        except HTTPError as e:\n            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n        except URLError as e:\n            print('URL error: {0}'.format(e.reason))\n        except IOError:\n            raise\n\n\ndef cmr_filter_urls(search_results):\n    \"\"\"Select only the desired data files from CMR response.\"\"\"\n    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n        return []\n\n    entries = [e['links']\n               for e in search_results['feed']['entry']\n               if 'links' in e]\n    # Flatten \"entries\" to a simple list of links\n    links = list(itertools.chain(*entries))\n\n    urls = []\n    unique_filenames = set()\n    for link in links:\n        if 'href' not in link:\n            # Exclude links with nothing to download\n            continue\n        if 'inherited' in link and link['inherited'] is True:\n            # Why are we excluding these links?\n            continue\n        if 'rel' in link and 'data#' not in link['rel']:\n            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n            continue\n\n        if 'title' in link and 'opendap' in link['title'].lower():\n            # Exclude OPeNDAP links--they are responsible for many duplicates\n            # This is a hack; when the metadata is updated to properly identify\n            # non-datapool links, we should be able to do this in a non-hack way\n            continue\n\n        filename = link['href'].split('/')[-1]\n        if filename in unique_filenames:\n            # Exclude links with duplicate filenames (they would overwrite)\n            continue\n        unique_filenames.add(filename)\n\n        urls.append(link['href'])\n\n    return urls\n\n\ndef cmr_search(short_name, version, time_start, time_end,\n               bounding_box='', polygon='', filename_filter='', quiet=False):\n    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n                                        time_start=time_start, time_end=time_end,\n                                        bounding_box=bounding_box,\n                                        polygon=polygon, filename_filter=filename_filter)\n    if not quiet:\n        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n\n    cmr_scroll_id = None\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n\n    urls = []\n    hits = 0\n    while True:\n        req = Request(cmr_query_url)\n        if cmr_scroll_id:\n            req.add_header('cmr-scroll-id', cmr_scroll_id)\n        response = urlopen(req, context=ctx)\n        if not cmr_scroll_id:\n            # Python 2 and 3 have different case for the http headers\n            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n            cmr_scroll_id = headers['cmr-scroll-id']\n            hits = int(headers['cmr-hits'])\n            if not quiet:\n                if hits > 0:\n                    print('Found {0} matches.'.format(hits))\n                else:\n                    print('Found no matches.')\n        search_page = response.read()\n        search_page = json.loads(search_page.decode('utf-8'))\n        url_scroll_results = cmr_filter_urls(search_page)\n        if not url_scroll_results:\n            break\n        if not quiet and hits > CMR_PAGE_SIZE:\n            print('.', end='')\n            sys.stdout.flush()\n        urls += url_scroll_results\n\n    if not quiet and hits > CMR_PAGE_SIZE:\n        print()\n    return urls\n\n\ndef main(argv=None):\n    global short_name, version, time_start, time_end, bounding_box, \\\n        polygon, filename_filter, url_list\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    force = False\n    quiet = False\n    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n\n    try:\n        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n        for opt, _arg in opts:\n            if opt in ('-f', '--force'):\n                force = True\n            elif opt in ('-q', '--quiet'):\n                quiet = True\n            elif opt in ('-h', '--help'):\n                print(usage)\n                sys.exit(0)\n    except getopt.GetoptError as e:\n        print(e.args[0])\n        print(usage)\n        sys.exit(1)\n\n    # Supply some default search parameters, just for testing purposes.\n    # These are only used if the parameters aren't filled in up above.\n    if 'short_name' in short_name:\n        short_name = 'ATL06'\n        version = '003'\n        time_start = '2018-10-14T00:00:00Z'\n        time_end = '2021-01-08T21:48:13Z'\n        bounding_box = ''\n        polygon = ''\n        filename_filter = '*ATL06_2020111121*'\n        url_list = []\n\n    try:\n        if not url_list:\n            url_list = cmr_search(short_name, version, time_start, time_end,\n                                  bounding_box=bounding_box, polygon=polygon,\n                                  filename_filter=filename_filter, quiet=quiet)\n\n        cmr_download(url_list, force=force, quiet=quiet)\n    except KeyboardInterrupt:\n        quit()\n        \n    try:\n        # move all the downloaded csv files to $datadir\n        for filename in os.listdir('./'):\n            if re.match(r'.*csv', filename) or re.match(r'.*csv', filename):\n                shutil.move(os.path.join('./', filename), f'{datadir}')\n    except ValueError:\n        quit()\n            \n    # and so forth\n\n\nif __name__ == '__main__':\n    main()",
  "history_output" : "Querying for data:\n\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=SNEX20_SD&version=001&version=01&version=1&temporal[]=2020-01-28T00:00:00Z,2020-02-12T23:59:59Z\nFound 1 matches.\nDownloading 2 files...\n1/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [===============                                             ]  25%  5.9MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [==============================                              ]  50%  6.8MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [=============================================               ]  75%  7.3MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [============================================================] 100%  9.1MB/s   \n2/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv.xml\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [============================================================] 100%  673.0kB/s   \nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/pXH9sGbNV4JrcX6ZLAZSIb2mfG/snowex_nsidc_download_script.py\", line 420, in <module>\n    main()\n  File \"/Users/joe/gw-workspace/pXH9sGbNV4JrcX6ZLAZSIb2mfG/snowex_nsidc_download_script.py\", line 412, in main\n    shutil.move(os.path.join('./', filename), f'{datadir}')\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/shutil.py\", line 812, in move\n    raise Error(\"Destination path '%s' already exists\" % real_dst)\nshutil.Error: Destination path '/Users/joe/Documents/data/SnowEx2020_SnowDepths_COGM_alldepths_v01.csv.xml' already exists\n",
  "history_begin_time" : 1642609008813,
  "history_end_time" : 1642609014117,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "c58l7v0ap9v",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642608941356,
  "history_end_time" : 1642608941404,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ica7gim517f",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642608917121,
  "history_end_time" : 1642608917162,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "nchtt4l5l21",
  "history_input" : "#!/usr/bin/env python\n# ----------------------------------------------------------------------------\n# NSIDC Data Download Script\n#\n# Copyright (c) 2021 Regents of the University of Colorado\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# Tested in Python 2.7 and Python 3.4, 3.6, 3.7\n#\n# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n#   $ python nsidc-data-download.py\n#\n# On Windows, open Start menu -> Run and type cmd. Then type:\n#     python nsidc-data-download.py\n#\n# The script will first search Earthdata for all matching files.\n# You will then be prompted for your Earthdata username/password\n# and the script will download the matching files.\n#\n# If you wish, you may store your Earthdata username/password in a .netrc\n# file in your $HOME directory and the script will automatically attempt to\n# read this file. The .netrc file should have the following format:\n#    machine urs.earthdata.nasa.gov login myusername password mypassword\n# where 'myusername' and 'mypassword' are your Earthdata credentials.\n#\nfrom __future__ import print_function\n\nimport base64\nimport getopt\nimport itertools\nimport json\nimport math\nimport netrc\nimport os.path\nimport ssl\nimport sys\nimport time\nfrom getpass import getpass\nimport os\nimport re\nimport shutil\n\ntry:\n    from urllib.parse import urlparse\n    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n    from urllib.error import HTTPError, URLError\nexcept ImportError:\n    from urlparse import urlparse\n    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n\nshort_name = 'SNEX20_SD'\nversion = '1'\ntime_start = '2020-01-28T00:00:00Z'\ntime_end = '2020-02-12T23:59:59Z'\nbounding_box = ''\npolygon = ''\nfilename_filter = ''\nurl_list = []\n\nCMR_URL = 'https://cmr.earthdata.nasa.gov'\nURS_URL = 'https://urs.earthdata.nasa.gov'\nCMR_PAGE_SIZE = 2000\nCMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n\nhomedir = os.path.expanduser('~') \ndatadir = f\"{homedir}/Documents/data/\"\n\n\ndef get_username():\n    username = ''\n\n    # For Python 2/3 compatibility:\n    try:\n        do_input = raw_input  # noqa\n    except NameError:\n        do_input = input\n\n    while not username:\n        username = do_input('Earthdata username: ')\n    return username\n\n\ndef get_password():\n    password = ''\n    while not password:\n        password = getpass('password: ')\n    return password\n\n\ndef get_credentials(url):\n    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n    credentials = None\n    errprefix = ''\n    try:\n        info = netrc.netrc()\n        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n        errprefix = 'netrc error: '\n    except Exception as e:\n        if (not ('No such file' in str(e))):\n            print('netrc error: {0}'.format(str(e)))\n        username = None\n        password = None\n\n    while not credentials:\n        if not username:\n            username = get_username()\n            password = get_password()\n        credentials = '{0}:{1}'.format(username, password)\n        credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n\n        if url:\n            try:\n                req = Request(url)\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n                opener = build_opener(HTTPCookieProcessor())\n                opener.open(req)\n            except HTTPError:\n                print(errprefix + 'Incorrect username or password')\n                errprefix = ''\n                credentials = None\n                username = None\n                password = None\n\n    return credentials\n\n\ndef build_version_query_params(version):\n    desired_pad_length = 3\n    if len(version) > desired_pad_length:\n        print('Version string too long: \"{0}\"'.format(version))\n        quit()\n\n    version = str(int(version))  # Strip off any leading zeros\n    query_params = ''\n\n    while len(version) <= desired_pad_length:\n        padded_version = version.zfill(desired_pad_length)\n        query_params += '&version={0}'.format(padded_version)\n        desired_pad_length -= 1\n    return query_params\n\n\ndef filter_add_wildcards(filter):\n    if not filter.startswith('*'):\n        filter = '*' + filter\n    if not filter.endswith('*'):\n        filter = filter + '*'\n    return filter\n\n\ndef build_filename_filter(filename_filter):\n    filters = filename_filter.split(',')\n    result = '&options[producer_granule_id][pattern]=true'\n    for filter in filters:\n        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n    return result\n\n\ndef build_cmr_query_url(short_name, version, time_start, time_end,\n                        bounding_box=None, polygon=None,\n                        filename_filter=None):\n    params = '&short_name={0}'.format(short_name)\n    params += build_version_query_params(version)\n    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n    if polygon:\n        params += '&polygon={0}'.format(polygon)\n    elif bounding_box:\n        params += '&bounding_box={0}'.format(bounding_box)\n    if filename_filter:\n        params += build_filename_filter(filename_filter)\n    return CMR_FILE_URL + params\n\n\ndef get_speed(time_elapsed, chunk_size):\n    if time_elapsed <= 0:\n        return ''\n    speed = chunk_size / time_elapsed\n    if speed <= 0:\n        speed = 1\n    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    i = int(math.floor(math.log(speed, 1000)))\n    p = math.pow(1000, i)\n    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n\n\ndef output_progress(count, total, status='', bar_len=60):\n    if total <= 0:\n        return\n    fraction = min(max(count / float(total), 0), 1)\n    filled_len = int(round(bar_len * fraction))\n    percents = int(round(100.0 * fraction))\n    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n    sys.stdout.write(fmt)\n    sys.stdout.flush()\n\n\ndef cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n    while True:\n        data = file_object.read(chunk_size)\n        if not data:\n            break\n        yield data\n\n\ndef cmr_download(urls, force=False, quiet=False):\n    \"\"\"Download files from list of urls.\"\"\"\n    if not urls:\n        return\n\n    url_count = len(urls)\n    if not quiet:\n        print('Downloading {0} files...'.format(url_count))\n    credentials = None\n\n    for index, url in enumerate(urls, start=1):\n        if not credentials and urlparse(url).scheme == 'https':\n            credentials = get_credentials(url)\n\n        filename = url.split('/')[-1]\n        if not quiet:\n            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n                                        url_count, filename))\n\n        try:\n            req = Request(url)\n            if credentials:\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n            opener = build_opener(HTTPCookieProcessor())\n            response = opener.open(req)\n            length = int(response.headers['content-length'])\n            try:\n                if not force and length == os.path.getsize(filename):\n                    if not quiet:\n                        print('  File exists, skipping')\n                    continue\n            except OSError:\n                pass\n            count = 0\n            chunk_size = min(max(length, 1), 1024 * 1024)\n            max_chunks = int(math.ceil(length / chunk_size))\n            time_initial = time.time()\n            with open(filename, 'wb') as out_file:\n                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n                    out_file.write(data)\n                    if not quiet:\n                        count = count + 1\n                        time_elapsed = time.time() - time_initial\n                        download_speed = get_speed(time_elapsed, count * chunk_size)\n                        output_progress(count, max_chunks, status=download_speed)\n            if not quiet:\n                print()\n        except HTTPError as e:\n            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n        except URLError as e:\n            print('URL error: {0}'.format(e.reason))\n        except IOError:\n            raise\n\n\ndef cmr_filter_urls(search_results):\n    \"\"\"Select only the desired data files from CMR response.\"\"\"\n    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n        return []\n\n    entries = [e['links']\n               for e in search_results['feed']['entry']\n               if 'links' in e]\n    # Flatten \"entries\" to a simple list of links\n    links = list(itertools.chain(*entries))\n\n    urls = []\n    unique_filenames = set()\n    for link in links:\n        if 'href' not in link:\n            # Exclude links with nothing to download\n            continue\n        if 'inherited' in link and link['inherited'] is True:\n            # Why are we excluding these links?\n            continue\n        if 'rel' in link and 'data#' not in link['rel']:\n            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n            continue\n\n        if 'title' in link and 'opendap' in link['title'].lower():\n            # Exclude OPeNDAP links--they are responsible for many duplicates\n            # This is a hack; when the metadata is updated to properly identify\n            # non-datapool links, we should be able to do this in a non-hack way\n            continue\n\n        filename = link['href'].split('/')[-1]\n        if filename in unique_filenames:\n            # Exclude links with duplicate filenames (they would overwrite)\n            continue\n        unique_filenames.add(filename)\n\n        urls.append(link['href'])\n\n    return urls\n\n\ndef cmr_search(short_name, version, time_start, time_end,\n               bounding_box='', polygon='', filename_filter='', quiet=False):\n    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n                                        time_start=time_start, time_end=time_end,\n                                        bounding_box=bounding_box,\n                                        polygon=polygon, filename_filter=filename_filter)\n    if not quiet:\n        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n\n    cmr_scroll_id = None\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n\n    urls = []\n    hits = 0\n    while True:\n        req = Request(cmr_query_url)\n        if cmr_scroll_id:\n            req.add_header('cmr-scroll-id', cmr_scroll_id)\n        response = urlopen(req, context=ctx)\n        if not cmr_scroll_id:\n            # Python 2 and 3 have different case for the http headers\n            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n            cmr_scroll_id = headers['cmr-scroll-id']\n            hits = int(headers['cmr-hits'])\n            if not quiet:\n                if hits > 0:\n                    print('Found {0} matches.'.format(hits))\n                else:\n                    print('Found no matches.')\n        search_page = response.read()\n        search_page = json.loads(search_page.decode('utf-8'))\n        url_scroll_results = cmr_filter_urls(search_page)\n        if not url_scroll_results:\n            break\n        if not quiet and hits > CMR_PAGE_SIZE:\n            print('.', end='')\n            sys.stdout.flush()\n        urls += url_scroll_results\n\n    if not quiet and hits > CMR_PAGE_SIZE:\n        print()\n    return urls\n\n\ndef main(argv=None):\n    global short_name, version, time_start, time_end, bounding_box, \\\n        polygon, filename_filter, url_list\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    force = False\n    quiet = False\n    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n\n    try:\n        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n        for opt, _arg in opts:\n            if opt in ('-f', '--force'):\n                force = True\n            elif opt in ('-q', '--quiet'):\n                quiet = True\n            elif opt in ('-h', '--help'):\n                print(usage)\n                sys.exit(0)\n    except getopt.GetoptError as e:\n        print(e.args[0])\n        print(usage)\n        sys.exit(1)\n\n    # Supply some default search parameters, just for testing purposes.\n    # These are only used if the parameters aren't filled in up above.\n    if 'short_name' in short_name:\n        short_name = 'ATL06'\n        version = '003'\n        time_start = '2018-10-14T00:00:00Z'\n        time_end = '2021-01-08T21:48:13Z'\n        bounding_box = ''\n        polygon = ''\n        filename_filter = '*ATL06_2020111121*'\n        url_list = []\n\n    try:\n        if not url_list:\n            url_list = cmr_search(short_name, version, time_start, time_end,\n                                  bounding_box=bounding_box, polygon=polygon,\n                                  filename_filter=filename_filter, quiet=quiet)\n\n        cmr_download(url_list, force=force, quiet=quiet)\n    except KeyboardInterrupt:\n        quit()\n        \n    try:\n        # move all the downloaded csv files to $datadir\n        for filename in os.listdir('./'):\n            if re.match(r'.*csv', filename) or re.match(r'.*csv', filename):\n                shutil.move(os.path.join('./', filename), f'{datadir}')\n    except ValueError:\n        quit()\n            \n    # and so forth\n\n\nif __name__ == '__main__':\n    main()",
  "history_output" : "Querying for data:\n\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=SNEX20_SD&version=001&version=01&version=1&temporal[]=2020-01-28T00:00:00Z,2020-02-12T23:59:59Z\nFound 1 matches.\nDownloading 2 files...\n1/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [===============                                             ]  25%  6.6MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [==============================                              ]  50%  7.1MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [=============================================               ]  75%  7.6MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [============================================================] 100%  9.2MB/s   \n2/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv.xml\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [============================================================] 100%  1.4MB/s   \nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/yBJFsMRVjtT5C47gkJdsALU85n/snowex_nsidc_download_script.py\", line 420, in <module>\n    main()\n  File \"/Users/joe/gw-workspace/yBJFsMRVjtT5C47gkJdsALU85n/snowex_nsidc_download_script.py\", line 412, in main\n    shutil.move(os.path.join('./', filename), f'{datadir}')\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/shutil.py\", line 812, in move\n    raise Error(\"Destination path '%s' already exists\" % real_dst)\nshutil.Error: Destination path '/Users/joe/Documents/data/SnowEx2020_SnowDepths_COGM_alldepths_v01.csv.xml' already exists\n",
  "history_begin_time" : 1642608597946,
  "history_end_time" : 1642608603692,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "be43rjxed6l",
  "history_input" : "#!/usr/bin/env python\n# ----------------------------------------------------------------------------\n# NSIDC Data Download Script\n#\n# Copyright (c) 2021 Regents of the University of Colorado\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# Tested in Python 2.7 and Python 3.4, 3.6, 3.7\n#\n# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n#   $ python nsidc-data-download.py\n#\n# On Windows, open Start menu -> Run and type cmd. Then type:\n#     python nsidc-data-download.py\n#\n# The script will first search Earthdata for all matching files.\n# You will then be prompted for your Earthdata username/password\n# and the script will download the matching files.\n#\n# If you wish, you may store your Earthdata username/password in a .netrc\n# file in your $HOME directory and the script will automatically attempt to\n# read this file. The .netrc file should have the following format:\n#    machine urs.earthdata.nasa.gov login myusername password mypassword\n# where 'myusername' and 'mypassword' are your Earthdata credentials.\n#\nfrom __future__ import print_function\n\nimport base64\nimport getopt\nimport itertools\nimport json\nimport math\nimport netrc\nimport os.path\nimport ssl\nimport sys\nimport time\nfrom getpass import getpass\nimport os\nimport re\nimport shutil\n\ntry:\n    from urllib.parse import urlparse\n    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n    from urllib.error import HTTPError, URLError\nexcept ImportError:\n    from urlparse import urlparse\n    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n\nshort_name = 'SNEX20_SD'\nversion = '1'\ntime_start = '2020-01-28T00:00:00Z'\ntime_end = '2020-02-12T23:59:59Z'\nbounding_box = ''\npolygon = ''\nfilename_filter = ''\nurl_list = []\n\nCMR_URL = 'https://cmr.earthdata.nasa.gov'\nURS_URL = 'https://urs.earthdata.nasa.gov'\nCMR_PAGE_SIZE = 2000\nCMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n\nhomedir = os.path.expanduser('~') \ndatadir = f\"{homedir}/Documents/data/\"\n\n\ndef get_username():\n    username = ''\n\n    # For Python 2/3 compatibility:\n    try:\n        do_input = raw_input  # noqa\n    except NameError:\n        do_input = input\n\n    while not username:\n        username = do_input('Earthdata username: ')\n    return username\n\n\ndef get_password():\n    password = ''\n    while not password:\n        password = getpass('password: ')\n    return password\n\n\ndef get_credentials(url):\n    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n    credentials = None\n    errprefix = ''\n    try:\n        info = netrc.netrc()\n        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n        errprefix = 'netrc error: '\n    except Exception as e:\n        if (not ('No such file' in str(e))):\n            print('netrc error: {0}'.format(str(e)))\n        username = None\n        password = None\n\n    while not credentials:\n        if not username:\n            username = get_username()\n            password = get_password()\n        credentials = '{0}:{1}'.format(username, password)\n        credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n\n        if url:\n            try:\n                req = Request(url)\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n                opener = build_opener(HTTPCookieProcessor())\n                opener.open(req)\n            except HTTPError:\n                print(errprefix + 'Incorrect username or password')\n                errprefix = ''\n                credentials = None\n                username = None\n                password = None\n\n    return credentials\n\n\ndef build_version_query_params(version):\n    desired_pad_length = 3\n    if len(version) > desired_pad_length:\n        print('Version string too long: \"{0}\"'.format(version))\n        quit()\n\n    version = str(int(version))  # Strip off any leading zeros\n    query_params = ''\n\n    while len(version) <= desired_pad_length:\n        padded_version = version.zfill(desired_pad_length)\n        query_params += '&version={0}'.format(padded_version)\n        desired_pad_length -= 1\n    return query_params\n\n\ndef filter_add_wildcards(filter):\n    if not filter.startswith('*'):\n        filter = '*' + filter\n    if not filter.endswith('*'):\n        filter = filter + '*'\n    return filter\n\n\ndef build_filename_filter(filename_filter):\n    filters = filename_filter.split(',')\n    result = '&options[producer_granule_id][pattern]=true'\n    for filter in filters:\n        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n    return result\n\n\ndef build_cmr_query_url(short_name, version, time_start, time_end,\n                        bounding_box=None, polygon=None,\n                        filename_filter=None):\n    params = '&short_name={0}'.format(short_name)\n    params += build_version_query_params(version)\n    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n    if polygon:\n        params += '&polygon={0}'.format(polygon)\n    elif bounding_box:\n        params += '&bounding_box={0}'.format(bounding_box)\n    if filename_filter:\n        params += build_filename_filter(filename_filter)\n    return CMR_FILE_URL + params\n\n\ndef get_speed(time_elapsed, chunk_size):\n    if time_elapsed <= 0:\n        return ''\n    speed = chunk_size / time_elapsed\n    if speed <= 0:\n        speed = 1\n    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    i = int(math.floor(math.log(speed, 1000)))\n    p = math.pow(1000, i)\n    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n\n\ndef output_progress(count, total, status='', bar_len=60):\n    if total <= 0:\n        return\n    fraction = min(max(count / float(total), 0), 1)\n    filled_len = int(round(bar_len * fraction))\n    percents = int(round(100.0 * fraction))\n    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n    sys.stdout.write(fmt)\n    sys.stdout.flush()\n\n\ndef cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n    while True:\n        data = file_object.read(chunk_size)\n        if not data:\n            break\n        yield data\n\n\ndef cmr_download(urls, force=False, quiet=False):\n    \"\"\"Download files from list of urls.\"\"\"\n    if not urls:\n        return\n\n    url_count = len(urls)\n    if not quiet:\n        print('Downloading {0} files...'.format(url_count))\n    credentials = None\n\n    for index, url in enumerate(urls, start=1):\n        if not credentials and urlparse(url).scheme == 'https':\n            credentials = get_credentials(url)\n\n        filename = url.split('/')[-1]\n        if not quiet:\n            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n                                        url_count, filename))\n\n        try:\n            req = Request(url)\n            if credentials:\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n            opener = build_opener(HTTPCookieProcessor())\n            response = opener.open(req)\n            length = int(response.headers['content-length'])\n            try:\n                if not force and length == os.path.getsize(filename):\n                    if not quiet:\n                        print('  File exists, skipping')\n                    continue\n            except OSError:\n                pass\n            count = 0\n            chunk_size = min(max(length, 1), 1024 * 1024)\n            max_chunks = int(math.ceil(length / chunk_size))\n            time_initial = time.time()\n            with open(filename, 'wb') as out_file:\n                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n                    out_file.write(data)\n                    if not quiet:\n                        count = count + 1\n                        time_elapsed = time.time() - time_initial\n                        download_speed = get_speed(time_elapsed, count * chunk_size)\n                        output_progress(count, max_chunks, status=download_speed)\n            if not quiet:\n                print()\n        except HTTPError as e:\n            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n        except URLError as e:\n            print('URL error: {0}'.format(e.reason))\n        except IOError:\n            raise\n\n\ndef cmr_filter_urls(search_results):\n    \"\"\"Select only the desired data files from CMR response.\"\"\"\n    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n        return []\n\n    entries = [e['links']\n               for e in search_results['feed']['entry']\n               if 'links' in e]\n    # Flatten \"entries\" to a simple list of links\n    links = list(itertools.chain(*entries))\n\n    urls = []\n    unique_filenames = set()\n    for link in links:\n        if 'href' not in link:\n            # Exclude links with nothing to download\n            continue\n        if 'inherited' in link and link['inherited'] is True:\n            # Why are we excluding these links?\n            continue\n        if 'rel' in link and 'data#' not in link['rel']:\n            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n            continue\n\n        if 'title' in link and 'opendap' in link['title'].lower():\n            # Exclude OPeNDAP links--they are responsible for many duplicates\n            # This is a hack; when the metadata is updated to properly identify\n            # non-datapool links, we should be able to do this in a non-hack way\n            continue\n\n        filename = link['href'].split('/')[-1]\n        if filename in unique_filenames:\n            # Exclude links with duplicate filenames (they would overwrite)\n            continue\n        unique_filenames.add(filename)\n\n        urls.append(link['href'])\n\n    return urls\n\n\ndef cmr_search(short_name, version, time_start, time_end,\n               bounding_box='', polygon='', filename_filter='', quiet=False):\n    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n                                        time_start=time_start, time_end=time_end,\n                                        bounding_box=bounding_box,\n                                        polygon=polygon, filename_filter=filename_filter)\n    if not quiet:\n        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n\n    cmr_scroll_id = None\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n\n    urls = []\n    hits = 0\n    while True:\n        req = Request(cmr_query_url)\n        if cmr_scroll_id:\n            req.add_header('cmr-scroll-id', cmr_scroll_id)\n        response = urlopen(req, context=ctx)\n        if not cmr_scroll_id:\n            # Python 2 and 3 have different case for the http headers\n            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n            cmr_scroll_id = headers['cmr-scroll-id']\n            hits = int(headers['cmr-hits'])\n            if not quiet:\n                if hits > 0:\n                    print('Found {0} matches.'.format(hits))\n                else:\n                    print('Found no matches.')\n        search_page = response.read()\n        search_page = json.loads(search_page.decode('utf-8'))\n        url_scroll_results = cmr_filter_urls(search_page)\n        if not url_scroll_results:\n            break\n        if not quiet and hits > CMR_PAGE_SIZE:\n            print('.', end='')\n            sys.stdout.flush()\n        urls += url_scroll_results\n\n    if not quiet and hits > CMR_PAGE_SIZE:\n        print()\n    return urls\n\n\ndef main(argv=None):\n    global short_name, version, time_start, time_end, bounding_box, \\\n        polygon, filename_filter, url_list\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    force = False\n    quiet = False\n    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n\n    try:\n        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n        for opt, _arg in opts:\n            if opt in ('-f', '--force'):\n                force = True\n            elif opt in ('-q', '--quiet'):\n                quiet = True\n            elif opt in ('-h', '--help'):\n                print(usage)\n                sys.exit(0)\n    except getopt.GetoptError as e:\n        print(e.args[0])\n        print(usage)\n        sys.exit(1)\n\n    # Supply some default search parameters, just for testing purposes.\n    # These are only used if the parameters aren't filled in up above.\n    if 'short_name' in short_name:\n        short_name = 'ATL06'\n        version = '003'\n        time_start = '2018-10-14T00:00:00Z'\n        time_end = '2021-01-08T21:48:13Z'\n        bounding_box = ''\n        polygon = ''\n        filename_filter = '*ATL06_2020111121*'\n        url_list = []\n\n    try:\n        if not url_list:\n            url_list = cmr_search(short_name, version, time_start, time_end,\n                                  bounding_box=bounding_box, polygon=polygon,\n                                  filename_filter=filename_filter, quiet=quiet)\n\n        cmr_download(url_list, force=force, quiet=quiet)\n    except KeyboardInterrupt:\n        quit()\n        \n    try:\n        # move all the downloaded csv files to $datadir\n        for filename in os.listdir('./'):\n            if re.match(r'.*csv', filename) or re.match(r'.*csv', filename):\n                shutil.move(os.path.join('./', filename), f'{datadir}')\n    except ValueError:\n        quit()\n            \n    # and so forth\n\n\nif __name__ == '__main__':\n    main()",
  "history_output" : "  File \"snowex_nsidc_download_script.py\", line 75\n    datadir = f\"{homedir}/Documents/data/\"\n                                         ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1642607495688,
  "history_end_time" : 1642607495863,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "xyi11necp2p",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642607462367,
  "history_end_time" : 1642607462460,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "xsybf6ktl9z",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642606600971,
  "history_end_time" : 1642606601344,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "x4p2n75avz5",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642606578625,
  "history_end_time" : 1642606578753,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "g0gvv9t0egz",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642606524121,
  "history_end_time" : 1642606524251,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "oqjozzsdkp7",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642606500743,
  "history_end_time" : 1642606500874,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "d86fzjlaayd",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1642606453406,
  "history_end_time" : 1642606453536,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "hzzqca5dc7r",
  "history_input" : "#!/usr/bin/env python\n# ----------------------------------------------------------------------------\n# NSIDC Data Download Script\n#\n# Copyright (c) 2021 Regents of the University of Colorado\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# Tested in Python 2.7 and Python 3.4, 3.6, 3.7\n#\n# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n#   $ python nsidc-data-download.py\n#\n# On Windows, open Start menu -> Run and type cmd. Then type:\n#     python nsidc-data-download.py\n#\n# The script will first search Earthdata for all matching files.\n# You will then be prompted for your Earthdata username/password\n# and the script will download the matching files.\n#\n# If you wish, you may store your Earthdata username/password in a .netrc\n# file in your $HOME directory and the script will automatically attempt to\n# read this file. The .netrc file should have the following format:\n#    machine urs.earthdata.nasa.gov login myusername password mypassword\n# where 'myusername' and 'mypassword' are your Earthdata credentials.\n#\nfrom __future__ import print_function\n\nimport base64\nimport getopt\nimport itertools\nimport json\nimport math\nimport netrc\nimport os.path\nimport ssl\nimport sys\nimport time\nfrom getpass import getpass\nimport os\nimport re\nimport shutil\n\ntry:\n    from urllib.parse import urlparse\n    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n    from urllib.error import HTTPError, URLError\nexcept ImportError:\n    from urlparse import urlparse\n    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n\nshort_name = 'SNEX20_SD'\nversion = '1'\ntime_start = '2020-01-28T00:00:00Z'\ntime_end = '2020-02-12T23:59:59Z'\nbounding_box = ''\npolygon = ''\nfilename_filter = ''\nurl_list = []\n\nCMR_URL = 'https://cmr.earthdata.nasa.gov'\nURS_URL = 'https://urs.earthdata.nasa.gov'\nCMR_PAGE_SIZE = 2000\nCMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n\nhomedir = os.path.expanduser('~') \ndatadir = f\"{homedir}/Documents/data/\"\n\n\ndef get_username():\n    username = ''\n\n    # For Python 2/3 compatibility:\n    try:\n        do_input = raw_input  # noqa\n    except NameError:\n        do_input = input\n\n    while not username:\n        username = do_input('Earthdata username: ')\n    return username\n\n\ndef get_password():\n    password = ''\n    while not password:\n        password = getpass('password: ')\n    return password\n\n\ndef get_credentials(url):\n    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n    credentials = None\n    errprefix = ''\n    try:\n        info = netrc.netrc()\n        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n        errprefix = 'netrc error: '\n    except Exception as e:\n        if (not ('No such file' in str(e))):\n            print('netrc error: {0}'.format(str(e)))\n        username = None\n        password = None\n\n    while not credentials:\n        if not username:\n            username = get_username()\n            password = get_password()\n        credentials = '{0}:{1}'.format(username, password)\n        credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n\n        if url:\n            try:\n                req = Request(url)\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n                opener = build_opener(HTTPCookieProcessor())\n                opener.open(req)\n            except HTTPError:\n                print(errprefix + 'Incorrect username or password')\n                errprefix = ''\n                credentials = None\n                username = None\n                password = None\n\n    return credentials\n\n\ndef build_version_query_params(version):\n    desired_pad_length = 3\n    if len(version) > desired_pad_length:\n        print('Version string too long: \"{0}\"'.format(version))\n        quit()\n\n    version = str(int(version))  # Strip off any leading zeros\n    query_params = ''\n\n    while len(version) <= desired_pad_length:\n        padded_version = version.zfill(desired_pad_length)\n        query_params += '&version={0}'.format(padded_version)\n        desired_pad_length -= 1\n    return query_params\n\n\ndef filter_add_wildcards(filter):\n    if not filter.startswith('*'):\n        filter = '*' + filter\n    if not filter.endswith('*'):\n        filter = filter + '*'\n    return filter\n\n\ndef build_filename_filter(filename_filter):\n    filters = filename_filter.split(',')\n    result = '&options[producer_granule_id][pattern]=true'\n    for filter in filters:\n        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n    return result\n\n\ndef build_cmr_query_url(short_name, version, time_start, time_end,\n                        bounding_box=None, polygon=None,\n                        filename_filter=None):\n    params = '&short_name={0}'.format(short_name)\n    params += build_version_query_params(version)\n    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n    if polygon:\n        params += '&polygon={0}'.format(polygon)\n    elif bounding_box:\n        params += '&bounding_box={0}'.format(bounding_box)\n    if filename_filter:\n        params += build_filename_filter(filename_filter)\n    return CMR_FILE_URL + params\n\n\ndef get_speed(time_elapsed, chunk_size):\n    if time_elapsed <= 0:\n        return ''\n    speed = chunk_size / time_elapsed\n    if speed <= 0:\n        speed = 1\n    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    i = int(math.floor(math.log(speed, 1000)))\n    p = math.pow(1000, i)\n    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n\n\ndef output_progress(count, total, status='', bar_len=60):\n    if total <= 0:\n        return\n    fraction = min(max(count / float(total), 0), 1)\n    filled_len = int(round(bar_len * fraction))\n    percents = int(round(100.0 * fraction))\n    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n    sys.stdout.write(fmt)\n    sys.stdout.flush()\n\n\ndef cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n    while True:\n        data = file_object.read(chunk_size)\n        if not data:\n            break\n        yield data\n\n\ndef cmr_download(urls, force=False, quiet=False):\n    \"\"\"Download files from list of urls.\"\"\"\n    if not urls:\n        return\n\n    url_count = len(urls)\n    if not quiet:\n        print('Downloading {0} files...'.format(url_count))\n    credentials = None\n\n    for index, url in enumerate(urls, start=1):\n        if not credentials and urlparse(url).scheme == 'https':\n            credentials = get_credentials(url)\n\n        filename = url.split('/')[-1]\n        if not quiet:\n            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n                                        url_count, filename))\n\n        try:\n            req = Request(url)\n            if credentials:\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n            opener = build_opener(HTTPCookieProcessor())\n            response = opener.open(req)\n            length = int(response.headers['content-length'])\n            try:\n                if not force and length == os.path.getsize(filename):\n                    if not quiet:\n                        print('  File exists, skipping')\n                    continue\n            except OSError:\n                pass\n            count = 0\n            chunk_size = min(max(length, 1), 1024 * 1024)\n            max_chunks = int(math.ceil(length / chunk_size))\n            time_initial = time.time()\n            with open(filename, 'wb') as out_file:\n                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n                    out_file.write(data)\n                    if not quiet:\n                        count = count + 1\n                        time_elapsed = time.time() - time_initial\n                        download_speed = get_speed(time_elapsed, count * chunk_size)\n                        output_progress(count, max_chunks, status=download_speed)\n            if not quiet:\n                print()\n        except HTTPError as e:\n            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n        except URLError as e:\n            print('URL error: {0}'.format(e.reason))\n        except IOError:\n            raise\n\n\ndef cmr_filter_urls(search_results):\n    \"\"\"Select only the desired data files from CMR response.\"\"\"\n    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n        return []\n\n    entries = [e['links']\n               for e in search_results['feed']['entry']\n               if 'links' in e]\n    # Flatten \"entries\" to a simple list of links\n    links = list(itertools.chain(*entries))\n\n    urls = []\n    unique_filenames = set()\n    for link in links:\n        if 'href' not in link:\n            # Exclude links with nothing to download\n            continue\n        if 'inherited' in link and link['inherited'] is True:\n            # Why are we excluding these links?\n            continue\n        if 'rel' in link and 'data#' not in link['rel']:\n            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n            continue\n\n        if 'title' in link and 'opendap' in link['title'].lower():\n            # Exclude OPeNDAP links--they are responsible for many duplicates\n            # This is a hack; when the metadata is updated to properly identify\n            # non-datapool links, we should be able to do this in a non-hack way\n            continue\n\n        filename = link['href'].split('/')[-1]\n        if filename in unique_filenames:\n            # Exclude links with duplicate filenames (they would overwrite)\n            continue\n        unique_filenames.add(filename)\n\n        urls.append(link['href'])\n\n    return urls\n\n\ndef cmr_search(short_name, version, time_start, time_end,\n               bounding_box='', polygon='', filename_filter='', quiet=False):\n    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n                                        time_start=time_start, time_end=time_end,\n                                        bounding_box=bounding_box,\n                                        polygon=polygon, filename_filter=filename_filter)\n    if not quiet:\n        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n\n    cmr_scroll_id = None\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n\n    urls = []\n    hits = 0\n    while True:\n        req = Request(cmr_query_url)\n        if cmr_scroll_id:\n            req.add_header('cmr-scroll-id', cmr_scroll_id)\n        response = urlopen(req, context=ctx)\n        if not cmr_scroll_id:\n            # Python 2 and 3 have different case for the http headers\n            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n            cmr_scroll_id = headers['cmr-scroll-id']\n            hits = int(headers['cmr-hits'])\n            if not quiet:\n                if hits > 0:\n                    print('Found {0} matches.'.format(hits))\n                else:\n                    print('Found no matches.')\n        search_page = response.read()\n        search_page = json.loads(search_page.decode('utf-8'))\n        url_scroll_results = cmr_filter_urls(search_page)\n        if not url_scroll_results:\n            break\n        if not quiet and hits > CMR_PAGE_SIZE:\n            print('.', end='')\n            sys.stdout.flush()\n        urls += url_scroll_results\n\n    if not quiet and hits > CMR_PAGE_SIZE:\n        print()\n    return urls\n\n\ndef main(argv=None):\n    global short_name, version, time_start, time_end, bounding_box, \\\n        polygon, filename_filter, url_list\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    force = False\n    quiet = False\n    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n\n    try:\n        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n        for opt, _arg in opts:\n            if opt in ('-f', '--force'):\n                force = True\n            elif opt in ('-q', '--quiet'):\n                quiet = True\n            elif opt in ('-h', '--help'):\n                print(usage)\n                sys.exit(0)\n    except getopt.GetoptError as e:\n        print(e.args[0])\n        print(usage)\n        sys.exit(1)\n\n    # Supply some default search parameters, just for testing purposes.\n    # These are only used if the parameters aren't filled in up above.\n    if 'short_name' in short_name:\n        short_name = 'ATL06'\n        version = '003'\n        time_start = '2018-10-14T00:00:00Z'\n        time_end = '2021-01-08T21:48:13Z'\n        bounding_box = ''\n        polygon = ''\n        filename_filter = '*ATL06_2020111121*'\n        url_list = []\n\n    try:\n        if not url_list:\n            url_list = cmr_search(short_name, version, time_start, time_end,\n                                  bounding_box=bounding_box, polygon=polygon,\n                                  filename_filter=filename_filter, quiet=quiet)\n\n        cmr_download(url_list, force=force, quiet=quiet)\n    except KeyboardInterrupt:\n        quit()\n        \n    try:\n        # move all the downloaded csv files to $datadir\n        for filename in os.listdir('./'):\n            if re.match(r'.*csv', filename) or re.match(r'.*csv', filename):\n                shutil.move(os.path.join('./', filename), f'{datadir}')\n    except ValueError:\n        quit()\n            \n    # and so forth\n\n\nif __name__ == '__main__':\n    main()",
  "history_output" : "Querying for data:\n\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=SNEX20_SD&version=001&version=01&version=1&temporal[]=2020-01-28T00:00:00Z,2020-02-12T23:59:59Z\nFound 1 matches.\nDownloading 2 files...\n1/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [===============                                             ]  25%  6.2MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [==============================                              ]  50%  6.9MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [=============================================               ]  75%  7.6MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [============================================================] 100%  9.2MB/s   \n2/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv.xml\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [============================================================] 100%  1.1MB/s   \nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/5L5i0zLodmJMcbde8UdMijLj1O/snowex_nsidc_download_script.py\", line 420, in <module>\n    main()\n  File \"/Users/joe/gw-workspace/5L5i0zLodmJMcbde8UdMijLj1O/snowex_nsidc_download_script.py\", line 412, in main\n    shutil.move(os.path.join('./', filename), f'{datadir}')\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/shutil.py\", line 812, in move\n    raise Error(\"Destination path '%s' already exists\" % real_dst)\nshutil.Error: Destination path '/Users/joe/Documents/data/SnowEx2020_SnowDepths_COGM_alldepths_v01.csv.xml' already exists\n",
  "history_begin_time" : 1642584141679,
  "history_end_time" : 1642584146743,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "r8q5a24jbgz",
  "history_input" : "#!/usr/bin/env python\n# ----------------------------------------------------------------------------\n# NSIDC Data Download Script\n#\n# Copyright (c) 2021 Regents of the University of Colorado\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# Tested in Python 2.7 and Python 3.4, 3.6, 3.7\n#\n# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n#   $ python nsidc-data-download.py\n#\n# On Windows, open Start menu -> Run and type cmd. Then type:\n#     python nsidc-data-download.py\n#\n# The script will first search Earthdata for all matching files.\n# You will then be prompted for your Earthdata username/password\n# and the script will download the matching files.\n#\n# If you wish, you may store your Earthdata username/password in a .netrc\n# file in your $HOME directory and the script will automatically attempt to\n# read this file. The .netrc file should have the following format:\n#    machine urs.earthdata.nasa.gov login myusername password mypassword\n# where 'myusername' and 'mypassword' are your Earthdata credentials.\n#\nfrom __future__ import print_function\n\nimport base64\nimport getopt\nimport itertools\nimport json\nimport math\nimport netrc\nimport os.path\nimport ssl\nimport sys\nimport time\nfrom getpass import getpass\nimport os\nimport re\nimport shutil\n\ntry:\n    from urllib.parse import urlparse\n    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n    from urllib.error import HTTPError, URLError\nexcept ImportError:\n    from urlparse import urlparse\n    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n\nshort_name = 'SNEX20_SD'\nversion = '1'\ntime_start = '2020-01-28T00:00:00Z'\ntime_end = '2020-02-12T23:59:59Z'\nbounding_box = ''\npolygon = ''\nfilename_filter = ''\nurl_list = []\n\nCMR_URL = 'https://cmr.earthdata.nasa.gov'\nURS_URL = 'https://urs.earthdata.nasa.gov'\nCMR_PAGE_SIZE = 2000\nCMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n\nhomedir = os.path.expanduser('~') \ndatadir = f\"{homedir}/Documents/data/\"\n\n\ndef get_username():\n    username = ''\n\n    # For Python 2/3 compatibility:\n    try:\n        do_input = raw_input  # noqa\n    except NameError:\n        do_input = input\n\n    while not username:\n        username = do_input('Earthdata username: ')\n    return username\n\n\ndef get_password():\n    password = ''\n    while not password:\n        password = getpass('password: ')\n    return password\n\n\ndef get_credentials(url):\n    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n    credentials = None\n    errprefix = ''\n    try:\n        info = netrc.netrc()\n        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n        errprefix = 'netrc error: '\n    except Exception as e:\n        if (not ('No such file' in str(e))):\n            print('netrc error: {0}'.format(str(e)))\n        username = None\n        password = None\n\n    while not credentials:\n        if not username:\n            username = get_username()\n            password = get_password()\n        credentials = '{0}:{1}'.format(username, password)\n        credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n\n        if url:\n            try:\n                req = Request(url)\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n                opener = build_opener(HTTPCookieProcessor())\n                opener.open(req)\n            except HTTPError:\n                print(errprefix + 'Incorrect username or password')\n                errprefix = ''\n                credentials = None\n                username = None\n                password = None\n\n    return credentials\n\n\ndef build_version_query_params(version):\n    desired_pad_length = 3\n    if len(version) > desired_pad_length:\n        print('Version string too long: \"{0}\"'.format(version))\n        quit()\n\n    version = str(int(version))  # Strip off any leading zeros\n    query_params = ''\n\n    while len(version) <= desired_pad_length:\n        padded_version = version.zfill(desired_pad_length)\n        query_params += '&version={0}'.format(padded_version)\n        desired_pad_length -= 1\n    return query_params\n\n\ndef filter_add_wildcards(filter):\n    if not filter.startswith('*'):\n        filter = '*' + filter\n    if not filter.endswith('*'):\n        filter = filter + '*'\n    return filter\n\n\ndef build_filename_filter(filename_filter):\n    filters = filename_filter.split(',')\n    result = '&options[producer_granule_id][pattern]=true'\n    for filter in filters:\n        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n    return result\n\n\ndef build_cmr_query_url(short_name, version, time_start, time_end,\n                        bounding_box=None, polygon=None,\n                        filename_filter=None):\n    params = '&short_name={0}'.format(short_name)\n    params += build_version_query_params(version)\n    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n    if polygon:\n        params += '&polygon={0}'.format(polygon)\n    elif bounding_box:\n        params += '&bounding_box={0}'.format(bounding_box)\n    if filename_filter:\n        params += build_filename_filter(filename_filter)\n    return CMR_FILE_URL + params\n\n\ndef get_speed(time_elapsed, chunk_size):\n    if time_elapsed <= 0:\n        return ''\n    speed = chunk_size / time_elapsed\n    if speed <= 0:\n        speed = 1\n    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    i = int(math.floor(math.log(speed, 1000)))\n    p = math.pow(1000, i)\n    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n\n\ndef output_progress(count, total, status='', bar_len=60):\n    if total <= 0:\n        return\n    fraction = min(max(count / float(total), 0), 1)\n    filled_len = int(round(bar_len * fraction))\n    percents = int(round(100.0 * fraction))\n    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n    sys.stdout.write(fmt)\n    sys.stdout.flush()\n\n\ndef cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n    while True:\n        data = file_object.read(chunk_size)\n        if not data:\n            break\n        yield data\n\n\ndef cmr_download(urls, force=False, quiet=False):\n    \"\"\"Download files from list of urls.\"\"\"\n    if not urls:\n        return\n\n    url_count = len(urls)\n    if not quiet:\n        print('Downloading {0} files...'.format(url_count))\n    credentials = None\n\n    for index, url in enumerate(urls, start=1):\n        if not credentials and urlparse(url).scheme == 'https':\n            credentials = get_credentials(url)\n\n        filename = url.split('/')[-1]\n        if not quiet:\n            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n                                        url_count, filename))\n\n        try:\n            req = Request(url)\n            if credentials:\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n            opener = build_opener(HTTPCookieProcessor())\n            response = opener.open(req)\n            length = int(response.headers['content-length'])\n            try:\n                if not force and length == os.path.getsize(filename):\n                    if not quiet:\n                        print('  File exists, skipping')\n                    continue\n            except OSError:\n                pass\n            count = 0\n            chunk_size = min(max(length, 1), 1024 * 1024)\n            max_chunks = int(math.ceil(length / chunk_size))\n            time_initial = time.time()\n            with open(filename, 'wb') as out_file:\n                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n                    out_file.write(data)\n                    if not quiet:\n                        count = count + 1\n                        time_elapsed = time.time() - time_initial\n                        download_speed = get_speed(time_elapsed, count * chunk_size)\n                        output_progress(count, max_chunks, status=download_speed)\n            if not quiet:\n                print()\n        except HTTPError as e:\n            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n        except URLError as e:\n            print('URL error: {0}'.format(e.reason))\n        except IOError:\n            raise\n\n\ndef cmr_filter_urls(search_results):\n    \"\"\"Select only the desired data files from CMR response.\"\"\"\n    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n        return []\n\n    entries = [e['links']\n               for e in search_results['feed']['entry']\n               if 'links' in e]\n    # Flatten \"entries\" to a simple list of links\n    links = list(itertools.chain(*entries))\n\n    urls = []\n    unique_filenames = set()\n    for link in links:\n        if 'href' not in link:\n            # Exclude links with nothing to download\n            continue\n        if 'inherited' in link and link['inherited'] is True:\n            # Why are we excluding these links?\n            continue\n        if 'rel' in link and 'data#' not in link['rel']:\n            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n            continue\n\n        if 'title' in link and 'opendap' in link['title'].lower():\n            # Exclude OPeNDAP links--they are responsible for many duplicates\n            # This is a hack; when the metadata is updated to properly identify\n            # non-datapool links, we should be able to do this in a non-hack way\n            continue\n\n        filename = link['href'].split('/')[-1]\n        if filename in unique_filenames:\n            # Exclude links with duplicate filenames (they would overwrite)\n            continue\n        unique_filenames.add(filename)\n\n        urls.append(link['href'])\n\n    return urls\n\n\ndef cmr_search(short_name, version, time_start, time_end,\n               bounding_box='', polygon='', filename_filter='', quiet=False):\n    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n                                        time_start=time_start, time_end=time_end,\n                                        bounding_box=bounding_box,\n                                        polygon=polygon, filename_filter=filename_filter)\n    if not quiet:\n        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n\n    cmr_scroll_id = None\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n\n    urls = []\n    hits = 0\n    while True:\n        req = Request(cmr_query_url)\n        if cmr_scroll_id:\n            req.add_header('cmr-scroll-id', cmr_scroll_id)\n        response = urlopen(req, context=ctx)\n        if not cmr_scroll_id:\n            # Python 2 and 3 have different case for the http headers\n            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n            cmr_scroll_id = headers['cmr-scroll-id']\n            hits = int(headers['cmr-hits'])\n            if not quiet:\n                if hits > 0:\n                    print('Found {0} matches.'.format(hits))\n                else:\n                    print('Found no matches.')\n        search_page = response.read()\n        search_page = json.loads(search_page.decode('utf-8'))\n        url_scroll_results = cmr_filter_urls(search_page)\n        if not url_scroll_results:\n            break\n        if not quiet and hits > CMR_PAGE_SIZE:\n            print('.', end='')\n            sys.stdout.flush()\n        urls += url_scroll_results\n\n    if not quiet and hits > CMR_PAGE_SIZE:\n        print()\n    return urls\n\n\ndef main(argv=None):\n    global short_name, version, time_start, time_end, bounding_box, \\\n        polygon, filename_filter, url_list\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    force = False\n    quiet = False\n    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n\n    try:\n        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n        for opt, _arg in opts:\n            if opt in ('-f', '--force'):\n                force = True\n            elif opt in ('-q', '--quiet'):\n                quiet = True\n            elif opt in ('-h', '--help'):\n                print(usage)\n                sys.exit(0)\n    except getopt.GetoptError as e:\n        print(e.args[0])\n        print(usage)\n        sys.exit(1)\n\n    # Supply some default search parameters, just for testing purposes.\n    # These are only used if the parameters aren't filled in up above.\n    if 'short_name' in short_name:\n        short_name = 'ATL06'\n        version = '003'\n        time_start = '2018-10-14T00:00:00Z'\n        time_end = '2021-01-08T21:48:13Z'\n        bounding_box = ''\n        polygon = ''\n        filename_filter = '*ATL06_2020111121*'\n        url_list = []\n\n    try:\n        if not url_list:\n            url_list = cmr_search(short_name, version, time_start, time_end,\n                                  bounding_box=bounding_box, polygon=polygon,\n                                  filename_filter=filename_filter, quiet=quiet)\n\n        cmr_download(url_list, force=force, quiet=quiet)\n    except KeyboardInterrupt:\n        quit()\n        \n    try:\n        # move all the downloaded csv files to $datadir\n        for filename in os.listdir('./'):\n            if re.match(r'.*csv', filename) or re.match(r'.*csv', filename):\n                shutil.move(os.path.join('./', filename), f'{datadir}')\n    except ValueError:\n        quit()\n            \n    # and so forth\n\n\nif __name__ == '__main__':\n    main()",
  "history_output" : "Querying for data:\n\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=SNEX20_SD&version=001&version=01&version=1&temporal[]=2020-01-28T00:00:00Z,2020-02-12T23:59:59Z\nFound 1 matches.\nDownloading 2 files...\n1/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv\n  File exists, skipping\n2/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv.xml\n  File exists, skipping\n",
  "history_begin_time" : 1642579751156,
  "history_end_time" : 1642579756591,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "bdubkory441",
  "history_input" : "#!/usr/bin/env python\n# ----------------------------------------------------------------------------\n# NSIDC Data Download Script\n#\n# Copyright (c) 2021 Regents of the University of Colorado\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# Tested in Python 2.7 and Python 3.4, 3.6, 3.7\n#\n# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n#   $ python nsidc-data-download.py\n#\n# On Windows, open Start menu -> Run and type cmd. Then type:\n#     python nsidc-data-download.py\n#\n# The script will first search Earthdata for all matching files.\n# You will then be prompted for your Earthdata username/password\n# and the script will download the matching files.\n#\n# If you wish, you may store your Earthdata username/password in a .netrc\n# file in your $HOME directory and the script will automatically attempt to\n# read this file. The .netrc file should have the following format:\n#    machine urs.earthdata.nasa.gov login myusername password mypassword\n# where 'myusername' and 'mypassword' are your Earthdata credentials.\n#\nfrom __future__ import print_function\n\nimport base64\nimport getopt\nimport itertools\nimport json\nimport math\nimport netrc\nimport os.path\nimport ssl\nimport sys\nimport time\nfrom getpass import getpass\nimport os\nimport re\nimport shutil\n\ntry:\n    from urllib.parse import urlparse\n    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n    from urllib.error import HTTPError, URLError\nexcept ImportError:\n    from urlparse import urlparse\n    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n\nshort_name = 'SNEX20_SD'\nversion = '1'\ntime_start = '2020-01-28T00:00:00Z'\ntime_end = '2020-02-12T23:59:59Z'\nbounding_box = ''\npolygon = ''\nfilename_filter = ''\nurl_list = []\n\nCMR_URL = 'https://cmr.earthdata.nasa.gov'\nURS_URL = 'https://urs.earthdata.nasa.gov'\nCMR_PAGE_SIZE = 2000\nCMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n\nhomedir = os.path.expanduser('~') \ndatadir = f\"{homedir}/Documents/data/\"\n\n\ndef get_username():\n    username = ''\n\n    # For Python 2/3 compatibility:\n    try:\n        do_input = raw_input  # noqa\n    except NameError:\n        do_input = input\n\n    while not username:\n        username = do_input('Earthdata username: ')\n    return username\n\n\ndef get_password():\n    password = ''\n    while not password:\n        password = getpass('password: ')\n    return password\n\n\ndef get_credentials(url):\n    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n    credentials = None\n    errprefix = ''\n    try:\n        info = netrc.netrc()\n        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n        errprefix = 'netrc error: '\n    except Exception as e:\n        if (not ('No such file' in str(e))):\n            print('netrc error: {0}'.format(str(e)))\n        username = None\n        password = None\n\n    while not credentials:\n        if not username:\n            username = get_username()\n            password = get_password()\n        credentials = '{0}:{1}'.format(username, password)\n        credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n\n        if url:\n            try:\n                req = Request(url)\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n                opener = build_opener(HTTPCookieProcessor())\n                opener.open(req)\n            except HTTPError:\n                print(errprefix + 'Incorrect username or password')\n                errprefix = ''\n                credentials = None\n                username = None\n                password = None\n\n    return credentials\n\n\ndef build_version_query_params(version):\n    desired_pad_length = 3\n    if len(version) > desired_pad_length:\n        print('Version string too long: \"{0}\"'.format(version))\n        quit()\n\n    version = str(int(version))  # Strip off any leading zeros\n    query_params = ''\n\n    while len(version) <= desired_pad_length:\n        padded_version = version.zfill(desired_pad_length)\n        query_params += '&version={0}'.format(padded_version)\n        desired_pad_length -= 1\n    return query_params\n\n\ndef filter_add_wildcards(filter):\n    if not filter.startswith('*'):\n        filter = '*' + filter\n    if not filter.endswith('*'):\n        filter = filter + '*'\n    return filter\n\n\ndef build_filename_filter(filename_filter):\n    filters = filename_filter.split(',')\n    result = '&options[producer_granule_id][pattern]=true'\n    for filter in filters:\n        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n    return result\n\n\ndef build_cmr_query_url(short_name, version, time_start, time_end,\n                        bounding_box=None, polygon=None,\n                        filename_filter=None):\n    params = '&short_name={0}'.format(short_name)\n    params += build_version_query_params(version)\n    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n    if polygon:\n        params += '&polygon={0}'.format(polygon)\n    elif bounding_box:\n        params += '&bounding_box={0}'.format(bounding_box)\n    if filename_filter:\n        params += build_filename_filter(filename_filter)\n    return CMR_FILE_URL + params\n\n\ndef get_speed(time_elapsed, chunk_size):\n    if time_elapsed <= 0:\n        return ''\n    speed = chunk_size / time_elapsed\n    if speed <= 0:\n        speed = 1\n    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    i = int(math.floor(math.log(speed, 1000)))\n    p = math.pow(1000, i)\n    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n\n\ndef output_progress(count, total, status='', bar_len=60):\n    if total <= 0:\n        return\n    fraction = min(max(count / float(total), 0), 1)\n    filled_len = int(round(bar_len * fraction))\n    percents = int(round(100.0 * fraction))\n    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n    sys.stdout.write(fmt)\n    sys.stdout.flush()\n\n\ndef cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n    while True:\n        data = file_object.read(chunk_size)\n        if not data:\n            break\n        yield data\n\n\ndef cmr_download(urls, force=False, quiet=False):\n    \"\"\"Download files from list of urls.\"\"\"\n    if not urls:\n        return\n\n    url_count = len(urls)\n    if not quiet:\n        print('Downloading {0} files...'.format(url_count))\n    credentials = None\n\n    for index, url in enumerate(urls, start=1):\n        if not credentials and urlparse(url).scheme == 'https':\n            credentials = get_credentials(url)\n\n        filename = url.split('/')[-1]\n        if not quiet:\n            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n                                        url_count, filename))\n\n        try:\n            req = Request(url)\n            if credentials:\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n            opener = build_opener(HTTPCookieProcessor())\n            response = opener.open(req)\n            length = int(response.headers['content-length'])\n            try:\n                if not force and length == os.path.getsize(filename):\n                    if not quiet:\n                        print('  File exists, skipping')\n                    continue\n            except OSError:\n                pass\n            count = 0\n            chunk_size = min(max(length, 1), 1024 * 1024)\n            max_chunks = int(math.ceil(length / chunk_size))\n            time_initial = time.time()\n            with open(filename, 'wb') as out_file:\n                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n                    out_file.write(data)\n                    if not quiet:\n                        count = count + 1\n                        time_elapsed = time.time() - time_initial\n                        download_speed = get_speed(time_elapsed, count * chunk_size)\n                        output_progress(count, max_chunks, status=download_speed)\n            if not quiet:\n                print()\n        except HTTPError as e:\n            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n        except URLError as e:\n            print('URL error: {0}'.format(e.reason))\n        except IOError:\n            raise\n\n\ndef cmr_filter_urls(search_results):\n    \"\"\"Select only the desired data files from CMR response.\"\"\"\n    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n        return []\n\n    entries = [e['links']\n               for e in search_results['feed']['entry']\n               if 'links' in e]\n    # Flatten \"entries\" to a simple list of links\n    links = list(itertools.chain(*entries))\n\n    urls = []\n    unique_filenames = set()\n    for link in links:\n        if 'href' not in link:\n            # Exclude links with nothing to download\n            continue\n        if 'inherited' in link and link['inherited'] is True:\n            # Why are we excluding these links?\n            continue\n        if 'rel' in link and 'data#' not in link['rel']:\n            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n            continue\n\n        if 'title' in link and 'opendap' in link['title'].lower():\n            # Exclude OPeNDAP links--they are responsible for many duplicates\n            # This is a hack; when the metadata is updated to properly identify\n            # non-datapool links, we should be able to do this in a non-hack way\n            continue\n\n        filename = link['href'].split('/')[-1]\n        if filename in unique_filenames:\n            # Exclude links with duplicate filenames (they would overwrite)\n            continue\n        unique_filenames.add(filename)\n\n        urls.append(link['href'])\n\n    return urls\n\n\ndef cmr_search(short_name, version, time_start, time_end,\n               bounding_box='', polygon='', filename_filter='', quiet=False):\n    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n                                        time_start=time_start, time_end=time_end,\n                                        bounding_box=bounding_box,\n                                        polygon=polygon, filename_filter=filename_filter)\n    if not quiet:\n        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n\n    cmr_scroll_id = None\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n\n    urls = []\n    hits = 0\n    while True:\n        req = Request(cmr_query_url)\n        if cmr_scroll_id:\n            req.add_header('cmr-scroll-id', cmr_scroll_id)\n        response = urlopen(req, context=ctx)\n        if not cmr_scroll_id:\n            # Python 2 and 3 have different case for the http headers\n            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n            cmr_scroll_id = headers['cmr-scroll-id']\n            hits = int(headers['cmr-hits'])\n            if not quiet:\n                if hits > 0:\n                    print('Found {0} matches.'.format(hits))\n                else:\n                    print('Found no matches.')\n        search_page = response.read()\n        search_page = json.loads(search_page.decode('utf-8'))\n        url_scroll_results = cmr_filter_urls(search_page)\n        if not url_scroll_results:\n            break\n        if not quiet and hits > CMR_PAGE_SIZE:\n            print('.', end='')\n            sys.stdout.flush()\n        urls += url_scroll_results\n\n    if not quiet and hits > CMR_PAGE_SIZE:\n        print()\n    return urls\n\n\ndef main(argv=None):\n    global short_name, version, time_start, time_end, bounding_box, \\\n        polygon, filename_filter, url_list\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    force = False\n    quiet = False\n    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n\n    try:\n        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n        for opt, _arg in opts:\n            if opt in ('-f', '--force'):\n                force = True\n            elif opt in ('-q', '--quiet'):\n                quiet = True\n            elif opt in ('-h', '--help'):\n                print(usage)\n                sys.exit(0)\n    except getopt.GetoptError as e:\n        print(e.args[0])\n        print(usage)\n        sys.exit(1)\n\n    # Supply some default search parameters, just for testing purposes.\n    # These are only used if the parameters aren't filled in up above.\n    if 'short_name' in short_name:\n        short_name = 'ATL06'\n        version = '003'\n        time_start = '2018-10-14T00:00:00Z'\n        time_end = '2021-01-08T21:48:13Z'\n        bounding_box = ''\n        polygon = ''\n        filename_filter = '*ATL06_2020111121*'\n        url_list = []\n\n    try:\n        if not url_list:\n            url_list = cmr_search(short_name, version, time_start, time_end,\n                                  bounding_box=bounding_box, polygon=polygon,\n                                  filename_filter=filename_filter, quiet=quiet)\n\n        cmr_download(url_list, force=force, quiet=quiet)\n    except KeyboardInterrupt:\n        quit()\n        \n    try:\n        # move all the downloaded csv files to $datadir\n        for filename in os.listdir('./'):\n            if re.match(r'.*csv', filename) or re.match(r'.*csv', filename):\n                shutil.move(os.path.join('./', filename), f'{datadir}')\n    except ValueError:\n        quit()\n            \n    # and so forth\n\n\nif __name__ == '__main__':\n    main()",
  "history_output" : "Querying for data:\n\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=SNEX20_SD&version=001&version=01&version=1&temporal[]=2020-01-28T00:00:00Z,2020-02-12T23:59:59Z\nFound 1 matches.\nDownloading 2 files...\n1/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [===============                                             ]  25%  6.6MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [==============================                              ]  50%  7.2MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [=============================================               ]  75%  7.8MB/s   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [============================================================] 100%  9.4MB/s   \n2/2: SnowEx2020_SnowDepths_COGM_alldepths_v01.csv.xml\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  [============================================================] 100%  741.9kB/s   \nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/iBclPGbEFuG34bajrTMSS68l44/snowex_nsidc_download_script.py\", line 420, in <module>\n    main()\n  File \"/Users/joe/gw-workspace/iBclPGbEFuG34bajrTMSS68l44/snowex_nsidc_download_script.py\", line 412, in main\n    shutil.move(os.path.join('./', filename), f'{datadir}')\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/shutil.py\", line 812, in move\n    raise Error(\"Destination path '%s' already exists\" % real_dst)\nshutil.Error: Destination path '/Users/joe/Documents/data/SnowEx2020_SnowDepths_COGM_alldepths_v01.csv.xml' already exists\n",
  "history_begin_time" : 1642579691454,
  "history_end_time" : 1642579698222,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Done"
},]
