[{
  "history_id" : "k3k3sdvj7rk",
  "history_input" : "# Write first python in Geoweaver\nimport os\nimport geopandas as gpd\nfrom shapely.geometry import Polygon, mapping\nfrom shapely.geometry.polygon import orient\nimport pandas as pd \nimport requests\nimport json\nimport pprint\nimport getpass\nimport netrc\nfrom platform import system\nfrom getpass import getpass\nfrom urllib import request\nfrom http.cookiejar import CookieJar\nfrom os.path import join, expanduser\nimport requests\nfrom xml.etree import ElementTree as ET\nimport time\nimport zipfile\nimport io\nimport shutil\n\nhomedir = os.path.expanduser('~') \ndatadir = f\"{homedir}/Documents/data/\"\n\npolygon_filepath = str(f'{homedir}/Documents/data/nsidc-polygon.json') # Note: A shapefile or other vector-based spatial data format could be substituted here.\n\ngdf = gpd.read_file(polygon_filepath) #Return a GeoDataFrame object\n\n# Simplify polygon for complex shapes in order to pass a reasonable request length to CMR. The larger the tolerance value, the more simplified the polygon.\n# Orient counter-clockwise: CMR polygon points need to be provided in counter-clockwise order. The last point should match the first point to close the polygon.\npoly = orient(gdf.simplify(0.05, preserve_topology=False).loc[0],sign=1.0)\n\n#Format dictionary to polygon coordinate pairs for CMR polygon filtering\npolygon = ','.join([str(c) for xy in zip(*poly.exterior.coords.xy) for c in xy])\nprint('Polygon coordinates to be used in search:', polygon)\n\ntemporal = '2017-01-01T00:00:00Z,2017-12-31T23:59:59Z' # Set temporal range\n\nparam_dict = {\n    'short_name': 'SNEX17_GPR',\n    'version': '2',\n    'polygon': polygon,\n#     'bounding_box': bounding_box, #optional alternative to polygon search parameter; if using, remove or comment out polygon search parameter\n    'temporal':temporal,\n}\n\ndef search_granules(search_parameters, geojson=None, output_format=\"json\"):\n    \"\"\"\n    Performs a granule search with token authentication for restricted results\n    \n    :search_parameters: dictionary of CMR search parameters\n    :token: CMR token needed for restricted search\n    :geojson: filepath to GeoJSON file for spatial search\n    :output_format: select format for results https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html#supported-result-formats\n    \n    :returns: if hits is greater than 0, search results are returned in chosen output_format, otherwise returns None.\n    \"\"\"\n    search_url = \"https://cmr.earthdata.nasa.gov/search/granules\"\n\n    \n    if geojson:\n        files = {\"shapefile\": (geojson, open(geojson, \"r\"), \"application/geo+json\")}\n    else:\n        files = None\n    \n    \n    parameters = {\n        \"scroll\": \"true\",\n        \"page_size\": 100,\n    }\n    \n    try:\n        response = requests.post(f\"{search_url}.{output_format}\", params=parameters, data=search_parameters, files=files)\n        response.raise_for_status()\n    except HTTPError as http_err:\n        print(f\"HTTP Error: {http_error}\")\n    except Exception as err:\n        print(f\"Error: {err}\")\n    \n    hits = int(response.headers['CMR-Hits'])\n    if hits > 0:\n        print(f\"Found {hits} files\")\n        results = json.loads(response.content)\n        granules = []\n        granules.extend(results['feed']['entry'])\n        granule_sizes = [float(granule['granule_size']) for granule in granules]\n        print(f\"The total size of all files is {sum(granule_sizes):.2f} MB\")\n        return response.json()\n    else:\n        print(\"Found no hits\")\n        return\n\nprint(search_granules(param_dict))\n",
  "history_output" : "  File \"snowex_nsidc_search.py\", line 25\n    datadir = f\"{homedir}/Documents/data/\"\n                                         ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1642607493758,
  "history_end_time" : 1642607494136,
  "history_notes" : null,
  "history_process" : "4fh28s",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "be43rjxed6l",
  "history_input" : "#!/usr/bin/env python\n# ----------------------------------------------------------------------------\n# NSIDC Data Download Script\n#\n# Copyright (c) 2021 Regents of the University of Colorado\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# Tested in Python 2.7 and Python 3.4, 3.6, 3.7\n#\n# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n#   $ python nsidc-data-download.py\n#\n# On Windows, open Start menu -> Run and type cmd. Then type:\n#     python nsidc-data-download.py\n#\n# The script will first search Earthdata for all matching files.\n# You will then be prompted for your Earthdata username/password\n# and the script will download the matching files.\n#\n# If you wish, you may store your Earthdata username/password in a .netrc\n# file in your $HOME directory and the script will automatically attempt to\n# read this file. The .netrc file should have the following format:\n#    machine urs.earthdata.nasa.gov login myusername password mypassword\n# where 'myusername' and 'mypassword' are your Earthdata credentials.\n#\nfrom __future__ import print_function\n\nimport base64\nimport getopt\nimport itertools\nimport json\nimport math\nimport netrc\nimport os.path\nimport ssl\nimport sys\nimport time\nfrom getpass import getpass\nimport os\nimport re\nimport shutil\n\ntry:\n    from urllib.parse import urlparse\n    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n    from urllib.error import HTTPError, URLError\nexcept ImportError:\n    from urlparse import urlparse\n    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n\nshort_name = 'SNEX20_SD'\nversion = '1'\ntime_start = '2020-01-28T00:00:00Z'\ntime_end = '2020-02-12T23:59:59Z'\nbounding_box = ''\npolygon = ''\nfilename_filter = ''\nurl_list = []\n\nCMR_URL = 'https://cmr.earthdata.nasa.gov'\nURS_URL = 'https://urs.earthdata.nasa.gov'\nCMR_PAGE_SIZE = 2000\nCMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n\nhomedir = os.path.expanduser('~') \ndatadir = f\"{homedir}/Documents/data/\"\n\n\ndef get_username():\n    username = ''\n\n    # For Python 2/3 compatibility:\n    try:\n        do_input = raw_input  # noqa\n    except NameError:\n        do_input = input\n\n    while not username:\n        username = do_input('Earthdata username: ')\n    return username\n\n\ndef get_password():\n    password = ''\n    while not password:\n        password = getpass('password: ')\n    return password\n\n\ndef get_credentials(url):\n    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n    credentials = None\n    errprefix = ''\n    try:\n        info = netrc.netrc()\n        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n        errprefix = 'netrc error: '\n    except Exception as e:\n        if (not ('No such file' in str(e))):\n            print('netrc error: {0}'.format(str(e)))\n        username = None\n        password = None\n\n    while not credentials:\n        if not username:\n            username = get_username()\n            password = get_password()\n        credentials = '{0}:{1}'.format(username, password)\n        credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n\n        if url:\n            try:\n                req = Request(url)\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n                opener = build_opener(HTTPCookieProcessor())\n                opener.open(req)\n            except HTTPError:\n                print(errprefix + 'Incorrect username or password')\n                errprefix = ''\n                credentials = None\n                username = None\n                password = None\n\n    return credentials\n\n\ndef build_version_query_params(version):\n    desired_pad_length = 3\n    if len(version) > desired_pad_length:\n        print('Version string too long: \"{0}\"'.format(version))\n        quit()\n\n    version = str(int(version))  # Strip off any leading zeros\n    query_params = ''\n\n    while len(version) <= desired_pad_length:\n        padded_version = version.zfill(desired_pad_length)\n        query_params += '&version={0}'.format(padded_version)\n        desired_pad_length -= 1\n    return query_params\n\n\ndef filter_add_wildcards(filter):\n    if not filter.startswith('*'):\n        filter = '*' + filter\n    if not filter.endswith('*'):\n        filter = filter + '*'\n    return filter\n\n\ndef build_filename_filter(filename_filter):\n    filters = filename_filter.split(',')\n    result = '&options[producer_granule_id][pattern]=true'\n    for filter in filters:\n        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n    return result\n\n\ndef build_cmr_query_url(short_name, version, time_start, time_end,\n                        bounding_box=None, polygon=None,\n                        filename_filter=None):\n    params = '&short_name={0}'.format(short_name)\n    params += build_version_query_params(version)\n    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n    if polygon:\n        params += '&polygon={0}'.format(polygon)\n    elif bounding_box:\n        params += '&bounding_box={0}'.format(bounding_box)\n    if filename_filter:\n        params += build_filename_filter(filename_filter)\n    return CMR_FILE_URL + params\n\n\ndef get_speed(time_elapsed, chunk_size):\n    if time_elapsed <= 0:\n        return ''\n    speed = chunk_size / time_elapsed\n    if speed <= 0:\n        speed = 1\n    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    i = int(math.floor(math.log(speed, 1000)))\n    p = math.pow(1000, i)\n    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n\n\ndef output_progress(count, total, status='', bar_len=60):\n    if total <= 0:\n        return\n    fraction = min(max(count / float(total), 0), 1)\n    filled_len = int(round(bar_len * fraction))\n    percents = int(round(100.0 * fraction))\n    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n    sys.stdout.write(fmt)\n    sys.stdout.flush()\n\n\ndef cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n    while True:\n        data = file_object.read(chunk_size)\n        if not data:\n            break\n        yield data\n\n\ndef cmr_download(urls, force=False, quiet=False):\n    \"\"\"Download files from list of urls.\"\"\"\n    if not urls:\n        return\n\n    url_count = len(urls)\n    if not quiet:\n        print('Downloading {0} files...'.format(url_count))\n    credentials = None\n\n    for index, url in enumerate(urls, start=1):\n        if not credentials and urlparse(url).scheme == 'https':\n            credentials = get_credentials(url)\n\n        filename = url.split('/')[-1]\n        if not quiet:\n            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n                                        url_count, filename))\n\n        try:\n            req = Request(url)\n            if credentials:\n                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n            opener = build_opener(HTTPCookieProcessor())\n            response = opener.open(req)\n            length = int(response.headers['content-length'])\n            try:\n                if not force and length == os.path.getsize(filename):\n                    if not quiet:\n                        print('  File exists, skipping')\n                    continue\n            except OSError:\n                pass\n            count = 0\n            chunk_size = min(max(length, 1), 1024 * 1024)\n            max_chunks = int(math.ceil(length / chunk_size))\n            time_initial = time.time()\n            with open(filename, 'wb') as out_file:\n                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n                    out_file.write(data)\n                    if not quiet:\n                        count = count + 1\n                        time_elapsed = time.time() - time_initial\n                        download_speed = get_speed(time_elapsed, count * chunk_size)\n                        output_progress(count, max_chunks, status=download_speed)\n            if not quiet:\n                print()\n        except HTTPError as e:\n            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n        except URLError as e:\n            print('URL error: {0}'.format(e.reason))\n        except IOError:\n            raise\n\n\ndef cmr_filter_urls(search_results):\n    \"\"\"Select only the desired data files from CMR response.\"\"\"\n    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n        return []\n\n    entries = [e['links']\n               for e in search_results['feed']['entry']\n               if 'links' in e]\n    # Flatten \"entries\" to a simple list of links\n    links = list(itertools.chain(*entries))\n\n    urls = []\n    unique_filenames = set()\n    for link in links:\n        if 'href' not in link:\n            # Exclude links with nothing to download\n            continue\n        if 'inherited' in link and link['inherited'] is True:\n            # Why are we excluding these links?\n            continue\n        if 'rel' in link and 'data#' not in link['rel']:\n            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n            continue\n\n        if 'title' in link and 'opendap' in link['title'].lower():\n            # Exclude OPeNDAP links--they are responsible for many duplicates\n            # This is a hack; when the metadata is updated to properly identify\n            # non-datapool links, we should be able to do this in a non-hack way\n            continue\n\n        filename = link['href'].split('/')[-1]\n        if filename in unique_filenames:\n            # Exclude links with duplicate filenames (they would overwrite)\n            continue\n        unique_filenames.add(filename)\n\n        urls.append(link['href'])\n\n    return urls\n\n\ndef cmr_search(short_name, version, time_start, time_end,\n               bounding_box='', polygon='', filename_filter='', quiet=False):\n    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n                                        time_start=time_start, time_end=time_end,\n                                        bounding_box=bounding_box,\n                                        polygon=polygon, filename_filter=filename_filter)\n    if not quiet:\n        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n\n    cmr_scroll_id = None\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n\n    urls = []\n    hits = 0\n    while True:\n        req = Request(cmr_query_url)\n        if cmr_scroll_id:\n            req.add_header('cmr-scroll-id', cmr_scroll_id)\n        response = urlopen(req, context=ctx)\n        if not cmr_scroll_id:\n            # Python 2 and 3 have different case for the http headers\n            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n            cmr_scroll_id = headers['cmr-scroll-id']\n            hits = int(headers['cmr-hits'])\n            if not quiet:\n                if hits > 0:\n                    print('Found {0} matches.'.format(hits))\n                else:\n                    print('Found no matches.')\n        search_page = response.read()\n        search_page = json.loads(search_page.decode('utf-8'))\n        url_scroll_results = cmr_filter_urls(search_page)\n        if not url_scroll_results:\n            break\n        if not quiet and hits > CMR_PAGE_SIZE:\n            print('.', end='')\n            sys.stdout.flush()\n        urls += url_scroll_results\n\n    if not quiet and hits > CMR_PAGE_SIZE:\n        print()\n    return urls\n\n\ndef main(argv=None):\n    global short_name, version, time_start, time_end, bounding_box, \\\n        polygon, filename_filter, url_list\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    force = False\n    quiet = False\n    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n\n    try:\n        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n        for opt, _arg in opts:\n            if opt in ('-f', '--force'):\n                force = True\n            elif opt in ('-q', '--quiet'):\n                quiet = True\n            elif opt in ('-h', '--help'):\n                print(usage)\n                sys.exit(0)\n    except getopt.GetoptError as e:\n        print(e.args[0])\n        print(usage)\n        sys.exit(1)\n\n    # Supply some default search parameters, just for testing purposes.\n    # These are only used if the parameters aren't filled in up above.\n    if 'short_name' in short_name:\n        short_name = 'ATL06'\n        version = '003'\n        time_start = '2018-10-14T00:00:00Z'\n        time_end = '2021-01-08T21:48:13Z'\n        bounding_box = ''\n        polygon = ''\n        filename_filter = '*ATL06_2020111121*'\n        url_list = []\n\n    try:\n        if not url_list:\n            url_list = cmr_search(short_name, version, time_start, time_end,\n                                  bounding_box=bounding_box, polygon=polygon,\n                                  filename_filter=filename_filter, quiet=quiet)\n\n        cmr_download(url_list, force=force, quiet=quiet)\n    except KeyboardInterrupt:\n        quit()\n        \n    try:\n        # move all the downloaded csv files to $datadir\n        for filename in os.listdir('./'):\n            if re.match(r'.*csv', filename) or re.match(r'.*csv', filename):\n                shutil.move(os.path.join('./', filename), f'{datadir}')\n    except ValueError:\n        quit()\n            \n    # and so forth\n\n\nif __name__ == '__main__':\n    main()",
  "history_output" : "  File \"snowex_nsidc_download_script.py\", line 75\n    datadir = f\"{homedir}/Documents/data/\"\n                                         ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1642607495688,
  "history_end_time" : 1642607495863,
  "history_notes" : null,
  "history_process" : "6zzyji",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "lt4vbaf285p",
  "history_input" : "from snowex_nsidc_search import * \n\n\n\n#Set NSIDC data access base URL\nbase_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n\n#Set the request mode to asynchronous, \"no\" processing agent (no subsetting or reformatting services available), and optionally removing metadata delivery\n\nparam_dict['request_mode'] = 'async'\nparam_dict['agent'] = 'NO'\nparam_dict['INCLUDE_META'] ='N' #optional if you do not wish to receive the associated metadata files with each science file. \n\nparam_string = '&'.join(\"{!s}={!r}\".format(k,v) for (k,v) in param_dict.items()) # Convert param_dict to string\nparam_string = param_string.replace(\"'\",\"\") # Remove quotes\n\napi_list = [f'{base_url}?{param_string}']\napi_request = api_list[0]\nprint(api_request) # Print API base URL + request parameters\n\n# Start authenticated session with Earthdata Login to allow for data downloads:\ndef setup_earthdata_login_auth(endpoint: str='urs.earthdata.nasa.gov'):\n    netrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n    try:\n        username, _, password = netrc.netrc(file=join(expanduser('~'), netrc_name)).authenticators(endpoint)\n    except (FileNotFoundError, TypeError):\n        print('Please provide your Earthdata Login credentials for access.')\n        print('Your info will only be passed to %s and will not be exposed in Jupyter.' % (endpoint))\n        username = input('Username: ')\n        password = getpass('Password: ')\n    manager = request.HTTPPasswordMgrWithDefaultRealm()\n    manager.add_password(None, endpoint, username, password)\n    auth = request.HTTPBasicAuthHandler(manager)\n    jar = CookieJar()\n    processor = request.HTTPCookieProcessor(jar)\n    opener = request.build_opener(auth, processor)\n    request.install_opener(opener)\n\nsetup_earthdata_login_auth(endpoint=\"urs.earthdata.nasa.gov\")\n\n\ndef request_nsidc_data(API_request):\n    \"\"\"\n    Performs a data customization and access request from NSIDC's API/\n    Creates an output folder in the working directory if one does not already exist.\n    \n    :API_request: NSIDC API endpoint; see https://nsidc.org/support/how/how-do-i-programmatically-request-data-services for more info\n    on how to configure the API request.\n    \n    \"\"\"\n\n    path = str(f'{datadir}') # Create an output folder if the folder does not already exist.\n    if not os.path.exists(path):\n        os.mkdir(path)\n        \n    base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n\n    \n    r = request.urlopen(API_request)\n    esir_root = ET.fromstring(r.read())\n    orderlist = []   # Look up order ID\n    for order in esir_root.findall(\"./order/\"):\n        orderlist.append(order.text)\n    orderID = orderlist[0]\n    statusURL = base_url + '/' + orderID # Create status URL\n    print('Order status URL: ', statusURL)\n    request_response = request.urlopen(statusURL) # Find order status  \n    request_root = ET.fromstring(request_response.read())\n    statuslist = []\n    for status in request_root.findall(\"./requestStatus/\"):\n        statuslist.append(status.text)\n    status = statuslist[0]\n    while status == 'pending' or status == 'processing': #Continue loop while request is still processing\n        print('Job status is ', status,'. Trying again.')\n        time.sleep(10)\n        loop_response = request.urlopen(statusURL)\n        loop_root = ET.fromstring(loop_response.read())\n        statuslist = [] #find status\n        for status in loop_root.findall(\"./requestStatus/\"):\n            statuslist.append(status.text)\n        status = statuslist[0]\n        if status == 'pending' or status == 'processing':\n            continue\n    if status == 'complete_with_errors' or status == 'failed': # Provide complete_with_errors error message:\n        messagelist = []\n        for message in loop_root.findall(\"./processInfo/\"):\n            messagelist.append(message.text)\n        print('Job status is ', status)\n        print('error messages:')\n        pprint(messagelist)\n    if status == 'complete' or status == 'complete_with_errors':# Download zipped order if status is complete or complete_with_errors\n        downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n        print('Job status is ', status)\n        print('Zip download URL: ', downloadURL)\n        print('Beginning download of zipped output...')\n        zip_response = request.urlopen(downloadURL)\n        with zipfile.ZipFile(io.BytesIO(zip_response.read())) as z:\n            z.extractall(path)\n        print('Download is complete.')\n    else: print('Request failed.')\n    \n    # Clean up Outputs folder by removing individual granule folders \n    for root, dirs, files in os.walk(path, topdown=False):\n        for file in files:\n            try:\n                shutil.move(os.path.join(root, file), path)\n            except OSError:\n                pass\n        for name in dirs:\n            os.rmdir(os.path.join(root, name))\n    return  \n\n\n# NOTE: downloads ~ 200MB of CSV files\nrequest_nsidc_data(api_request)",
  "history_output" : "  File \"snowex_nsidc_download_api.py\", line 17\n    api_list = [f'{base_url}?{param_string}']\n                                           ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1642607497216,
  "history_end_time" : 1642607497384,
  "history_notes" : null,
  "history_process" : "li3z29",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "hrg3ppew6kj",
  "history_input" : "import os\nimport geopandas as gpd\nfrom shapely.geometry import Polygon, mapping\nfrom shapely.geometry.polygon import orient\nimport pandas as pd \nimport requests\nimport json\nimport pprint\nimport getpass\nimport netrc\nfrom platform import system\nfrom getpass import getpass\nfrom urllib import request\nfrom http.cookiejar import CookieJar\nfrom os.path import join, expanduser\nimport requests\nfrom xml.etree import ElementTree as ET\nimport time\nimport zipfile\nimport io\nimport shutil\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\nimport requests\nimport os\n\nhomedir = os.path.expanduser('~')\ndatadir = f\"{homedir}/Documents/data/\"\n\nsnowex_path = f'{datadir}/SnowEx17_GPR_Version2_Week1.csv' # Define local filepath\ndf = pd.read_csv(snowex_path, sep='\\t') \nprint(df.head())\n\n# extract date columns\n\ndf['date'] = df.collection.str.rsplit('_').str[-1].astype(str)\ndf.date = pd.to_datetime(df.date, format=\"%m%d%y\")\ndf = df.sort_values(['date'])\nprint(df.head())\n\n# Convert to Geopandas dataframe to provide point geometry\n\ngdf_utm= gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs='EPSG:32612')\nprint(gdf_utm.head())\n\nhomedir = os.path.expanduser('~')\ndatadir = f\"{homedir}/Documents/data/\"\n\nexampledataurl = \"https://raw.githubusercontent.com/snowex-hackweek/website/main/book/tutorials/machine-learning/data/snow_depth_data.csv\"\n\nr = requests.get(exampledataurl, allow_redirects=True)\n\nwith open(f\"{datadir}/snow_depth_data.csv\", 'wb') as datafile:\n  datafile.write(r.content)\n",
  "history_output" : "  File \"snowex_nsidc_read_data.py\", line 33\n    datadir = f\"{homedir}/Documents/data/\"\n                                         ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1642607498936,
  "history_end_time" : 1642607499108,
  "history_notes" : null,
  "history_process" : "pvrvwa",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "zvibuu3zpza",
  "history_input" : "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\nimport requests\nimport os\nimport tensorflow as tf\n\nhomedir = os.path.expanduser('~')\ndatadir = f\"{homedir}/Documents/data/\"\n\nexampledataurl = \"https://raw.githubusercontent.com/snowex-hackweek/website/main/book/tutorials/machine-learning/data/snow_depth_data.csv\"\n\nr = requests.get(exampledataurl, allow_redirects=True)\n\nopen(f\"{datadir}snow_depth_data.csv\", 'wb').write(r.content)\n\n# retrieve the example training data first\ndataset = pd.read_csv(\"data/snow_depth_data.csv\")\n\nprint(dataset.info())\n\nprint(dataset.head())\n\ntrain_dataset = dataset.sample(frac=0.8, random_state=0)\ntest_dataset = dataset.drop(train_dataset.index)\n\ntrain_dataset.describe().transpose()\n\nscaler = MinMaxScaler()\nscaled_train = scaler.fit_transform(train_dataset)\nscaled_test = scaler.transform(test_dataset) ## fit_transform != transform. \n## transform uses the parameters of fit_transform\n\n  \n# Sepatare Features from Labels\ntrain_X, train_y = scaled_train[:, :-1], scaled_train[:, -1]\ntest_X, test_y = scaled_test[:, :-1], scaled_test[:, -1]\n\nprint(\"TensorFlow version ==>\", tf.__version__) \nprint(\"Keras version ==>\",tf.keras.__version__)\n\ntf.random.set_seed(0) ## For reproducible results\nlinear_regression = tf.keras.models.Sequential() # Specify layers in their sequential order\n# inputs are 4 dimensions (4 dimensions = 4 features)\n# Dense = Fully Connected.  \nlinear_regression.add(tf.keras.layers.Dense(1, activation=None ,input_shape=(train_X.shape[1],)))\n# Output layer has no activation with just 1 node\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nlinear_regression.compile(optimizer = opt, loss='mean_squared_error')\n\nprint(linear_regression.summary())\n\n# NOTE: can changed from epochs=150 to run faster, change to verbose=1 for per-epoch output\nhistory =linear_regression.fit(train_X, train_y, epochs=100, validation_split = 0.2, verbose=0)\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.grid(True)\n\n# linear regression\n\nlinear_regression.get_weights()\n\ndef Sigmoid(z):\n    \"\"\"\n    A function that performs the sigmoid transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* logistic: the transformed/activated version of the array\n    \"\"\"\n    \n    logistic = 1/(1+ np.exp(-z))\n    return logistic\n    \n\ndef Tanh(z):\n    \"\"\"\n    A function that performs the hyperbolic tangent transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* hyp: the transformed/activated version of the array\n    \"\"\"\n    \n    hyp = np.tanh(z)\n    return hyp\n\n\ndef ReLu(z):\n    \"\"\"\n    A function that performs the hyperbolic tangent transformation\n    \n    Arguments:\n    ---------\n        -* z: array/list of numbers to activate\n    \n    Returns:\n    --------\n        -* points: the transformed/activated version of the array\n    \"\"\"\n    \n    points = np.where(z < 0, 0, z)\n    return points\n\nz = np.linspace(-10,10)\nfa = plt.figure(figsize=(16,5))\n\nplt.subplot(1,3,1)\nplt.plot(z,Sigmoid(z),color=\"red\", label=r'$\\frac{1}{1 + e^{-z}}$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Sigmoid Activation Function\")\nplt.legend(loc='best',fontsize = 22)\n\n\nplt.subplot(1,3,2)\nplt.plot(z,Tanh(z),color=\"red\", label=r'$\\tanh (z)$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Hyperbolic Tangent Activation Function\")\nplt.legend(loc='best',fontsize = 18)\n\nplt.subplot(1,3,3)\nplt.plot(z,ReLu(z),color=\"red\", label=r'$\\max(0,z)$')\nplt.grid(True, which='both')\nplt.xlabel('z')\nplt.ylabel('g(z)', fontsize=15)\nplt.title(\"Rectified Linear Unit Activation Function\")\nplt.legend(loc='best', fontsize = 18)\n\ntf.random.set_seed(1000)  ## For reproducible results\nnetwork = tf.keras.models.Sequential() # Specify layers in their sequential order\n# inputs are 4 dimensions (4 dimensions = 4 features)\n# Dense = Fully Connected.   \n# First hidden layer has 1000 neurons with relu activations.\n# Second hidden layer has 512 neurons with relu activations\n# Third hidden layer has 256 neurons with Sigmoid activations\nnetwork.add(tf.keras.layers.Dense(1000, activation='relu' ,input_shape=(train_X.shape[1],)))\nnetwork.add(tf.keras.layers.Dense(512, activation='relu')) # sigmoid, tanh\nnetwork.add(tf.keras.layers.Dense(256, activation='sigmoid'))\n# Output layer uses no activation with 1 output neurons\nnetwork.add(tf.keras.layers.Dense(1)) # Output layer\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nnetwork.compile(optimizer = opt, loss='mean_squared_error')\n\nnetwork.summary()\n\n# NOTE: if you have time, consider upping epochs -> 150\nhistory =network.fit(train_X, train_y, epochs=20, validation_split = 0.2, verbose=0)\n\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('Error')\nplt.legend()\nplt.grid(True)\n\n# Prediction\n\n## Linear Regression\n\nyhat_linReg = linear_regression.predict(test_X)\ninv_yhat_linReg = np.concatenate((test_X, yhat_linReg), axis=1)\ninv_yhat_linReg = scaler.inverse_transform(inv_yhat_linReg)\ninv_yhat_linReg = inv_yhat_linReg[:,-1]\n\n## DNN\nyhat_dnn = network.predict(test_X) \ninv_yhat_dnn = np.concatenate((test_X, yhat_dnn), axis=1)\ninv_yhat_dnn = scaler.inverse_transform(inv_yhat_dnn)\ninv_yhat_dnn = inv_yhat_dnn[:,-1]\n\n## True Snow Depth (Test Set)\ninv_y = test_dataset[\"snow_depth\"]\n\n\n## Put Observed and Predicted (Linear Regression and DNN) in a Dataframe\nprediction_df = pd.DataFrame({\"Observed\": inv_y,\n                    \"LR\":inv_yhat_linReg, \"DNN\":inv_yhat_dnn})\n\n# check performance\n\ndef metrics_print(test_data,test_predict):\n    print('Test RMSE: ', round(np.sqrt(mean_squared_error(test_data, test_predict)), 2))\n    print('Test R^2 : ', round((r2_score(test_data, test_predict)*100), 2) ,\"%\")\n    print('Test MAPE: ', round(mean_absolute_percentage_error(test_data, test_predict)*100,2), '%')\n    \nprint(\"##************** Linear Regression Results **************##\")\nmetrics_print(prediction_df['Observed'], prediction_df['LR'])\nprint(\" \")\nprint(\" \")\n\nprint(\"##************** Deep Learning Results **************##\")\nmetrics_print(prediction_df['Observed'], prediction_df['DNN'])\nprint(\" \")\nprint(\" \")\n\n# visualize performance\n\nfa = plt.figure(figsize=(16,5))\nplt.subplot(1,2,1)\nplt.scatter(prediction_df['Observed'],prediction_df['LR'])\nplt.xlabel('True Values [snow_depth]', fontsize=15)\nplt.ylabel('Predictions [snow_depth]', fontsize=15)\nplt.title(\"Linear Regression\")\n\n\nplt.subplot(1,2,2)\nplt.scatter(prediction_df['Observed'],prediction_df['DNN'])\nplt.xlabel('True Values [snow_depth]', fontsize=15)\nplt.ylabel('Predictions [snow_depth]', fontsize=15)\nplt.title(\"Deep Neural Network\")\n\n# visualize error\n\nLR_error = prediction_df['Observed'] - prediction_df['LR']\nDNN_error = prediction_df['Observed'] - prediction_df['DNN']\n\nfa = plt.figure(figsize=(16,5))\n\nplt.subplot(1,2,1)\nLR_error.hist()\nplt.xlabel('Error', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.title(\"Linear Regression\")\n\nplt.subplot(1,2,2)\nDNN_error.hist()\nplt.xlabel('Error', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.title(\"Deep Neural Network\")\n\n# save best model\n\nnetwork.save('DNN')\n\n## To load model, use;\nmodel = tf.keras.models.load_model('DNN')\n\n\n\n\n",
  "history_output" : "  File \"snowex_ml_experiment.py\", line 12\n    datadir = f\"{homedir}/Documents/data/\"\n                                         ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1642607500471,
  "history_end_time" : 1642607500633,
  "history_notes" : null,
  "history_process" : "7nyo77",
  "host_id" : "100001",
  "indicator" : "Done"
}]
